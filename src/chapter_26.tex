\chapter{Probabilidad y estadística}

\section{Topología}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Propiedad distributiva}
    \begin{gather*}
        A \cup (B \cap C) = (A \cup B) \cap (A \cup C)
        \\
        A \cap (B \cup C) = (A \cap B) \cup (A \cap C)
        \\[1ex]
        (A \cap B) \cap (A \cap C) = A \cap B \cap C
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Propiedad asociativa}
    \begin{gather*}
        A \cup B \cup C = A \cup (B \cup C) = (A \cup B) \cup C
        \\
        A \cap B \cap C = A \cap (B \cap C) = (A \cap B) \cap C
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Leyes de Morgan}
    \begin{equation*}
        (A \cap B)^c = A^c \cup B^c
        \\
        (A \cup B)^c = A^c \cap B^c
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para $\nth$ conjuntos disjuntos}
    \begin{equation*}
        P \left( \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n P(A_i)
        \iff A_i \cap A_j = \setO
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para dos conjuntos}
    \begin{equation*}
        P(A \cup B) = P(A) + P(B) - P(A \cap B)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para tres conjuntos}
    \begin{gather*}
        P (A \cup B \cup C) =
        \\
        =P(A) + P(B) + P(C) - P(A \cap B) +
        \\
        - P(B \cap C) - P(A \cap C) + P(A \cap B \cap C)
    \end{gather*}
\end{mdframed}


\section{Métodos de conteo}


\subsection{Factorial}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Factorial}
    \cusTe{Multiplicación de todos los números naturales desde 1 hasta $n$.}
    \begin{equation*}
        n! = n(n-1)! = n(n-1)(n-2) \dots 3 \cdot 2 \cdot 1
    \end{equation*}
\end{mdframed}


\subsection{Combinatorio}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Combinatorio}
    \cusTe{Cantidad de formas de elegir $m$ entre un grupo de $n$ elementos siendo $n>m$.}
    \begin{equation*}
        \comb{n}{m} = \frac{n!}{(n-m)! \, m!}
    \end{equation*}
\end{mdframed}


\subsection{Permutaciones}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Permutaciones}
    \cusTe{Cantidad de formas de ordenar una secuancia de $n$ elementos pudiendo cada una de las $k$ tareas repetirse $n_i$ veces.}
    \begin{equation*}
        \#A = \frac{n!}{n_1!n_2! \dots n_k!}
    \end{equation*}
\end{mdframed}


\section{Probabilidad condicional}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \noTi{Probabilidad de que ocurra el evento $A$ con la condición de que $B$ haya ocurrido.}
    \begin{equation*}
        P(A|B) = \frac{P(A \cap B)}{P(B)}
    \end{equation*}
\end{mdframed}


\subsection{Teorema de multiplicación}

Un individuo tiene una probabilidad $P(B)$ de pertenecer a una partición $B$.
Además, hay una probabilidad $P(A|B)$ de que el individuo cumpla cierta característica $A$ además de pertenecer a $B$.
La probabilidad de que un individuo tenga la característica $A$ y sea parte de la partición $B$ simultaneamente está dada por el teorema de multiplicación.

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de multiplicación}
    \begin{equation*}
        P(A \cap B) = P(A|B) \, P(B)
    \end{equation*}
\end{mdframed}


\subsection{Teorema de probabilidad total}

Calcula la probabilidad de que un individuo tenga cierta característica $A$ sobre el total de las particiones.
Esto es la suma de las partes de cada partición $B_i$ que cumplen la característica.

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de probabilidad total}
    \begin{equation*}
        P(A) = \sum_{i=1}^n P(A|B_i) \, P(B_i)
    \end{equation*}
\end{mdframed}


\subsection{Teorema de Bayes}

La probabilidad de que un individuo pertenezca a cierta partición $(B_i)$ dado que tiene cierta característica $(A)$ es
\begin{equation*}
    P(B_i|A) = \frac{P(A \cap B_i)}{P(A)}
\end{equation*}

Usando la definición de Probabilidad Condicional y el Teorema de Multiplicación, se tiene

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de Bayes}
    \begin{equation*}
        P(B_i|A) = \frac{P(A|B_i) \, P(B_i)}{P(A)}
    \end{equation*}
\end{mdframed}


\section{Independencia}

Si dos eventos $A$ y $B$ son independientes uno del otro, el hecho de que ocurra uno no dice nada sobre la ocurrencia del otro.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Independencia de dos eventos}
    \begin{equation*}
        P(A|B) = P(A)
    \end{equation*}
\end{mdframed}

De manera que, para no contradecir la definición de probabilidad condicional, se tiene

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \noTi{Los eventos $A$ y $B$ son independientes $\iff$}
    \begin{equation*}
        P(A \cap B) = P(A) \, P(B)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \noTi{Si $A_1, A_2 \dots A_n$ son independientes $\implies$}
    \begin{equation*}
        P(A_1 \cap A_2 \cap \dots \cap A_n) = P(A_1) \, P(A_2) \dots P(A_n)
    \end{equation*}
\end{mdframed}


\section{Variables aleatorias discretas}

Sea la variable aleatoria $X$ una función que toma valores del espacio muestral $\Omega$ y entrega una cantidad numerable de valores.
\begin{equation*}
    X: \Omega \longrightarrow \setR \tq X(\omega) = x
\end{equation*}

Se dice que $X$ es una variable aleatoria discreta (o VAD), ya que no tiene intervalos contínuos en su imagen.
\begin{equation*}
    \ran(X) = \im(X) \subseteq \setN_0
\end{equation*}


\section{Función de probabilidad puntual}

La función de probabilidad puntual calcula qué probabilidad hay de que una VAD devuelva un valor $k$.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Función de probabilidad puntual}
    \begin{equation*}
        p_X: \setR \longrightarrow \setR \tq p_X(x) = P(X=k)
    \end{equation*}
\end{mdframed}


\section{Función de distribución}

La función de distribución acumulada es una función partida que calcula qué probabilidad hay de que una VAD devuelva un valor menor o igual a $k$.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Función de distribución}
    \begin{equation*}
        F_X: \setR \longrightarrow \setR \tq F_X(x) = P(X \leq k)
    \end{equation*}
\end{mdframed}


\section{Esperanza, varianza, covarianza y desvío}

La esperanza, media, o valor esperado es un promedio entre los valores de $\ran(X)$ pesado por su probabilidad.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Esperanza}
    \begin{equation*}
        E(X) = \sum_{i=1}^n x_i \, \fx[p_X]{x_i}
    \end{equation*}
\end{mdframed}

La varianza calcula qué tan cerca está el valor de la imagen a su esperanza.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza}
    \begin{equation*}
        V(X) = \sum_{i=1}^n \inParentheses{x_i - E(X)}^2 \, p_X(x_i)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        V(X) = \sum_{i=1}^n x_i^2 \, p_X(x_i) - \inParentheses{E(X)}^2
    \end{equation*}
\end{mdframed}

La covarianza es un valor que indica el grado de variación conjunta de dos variables aleatorias respecto a sus medias.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Covarianza}
    \begin{equation*}
        \fx[C]{X\,Y} = \fx[E]{\inParentheses{X-\fx[E]{X}} \inParentheses{Y-\fx[E]{Y}}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Desvío estandar}
    \begin{equation*}
        \sigma = \sqrt{V(X)}
    \end{equation*}
\end{mdframed}


\section{VAD famosas}


\subsection{Variable aleatoria binomial}

\begin{itemize}
\item Se realizan $n$ pruebas para verificar si se da o no cierto evento $A_i$ en cada una de las muestras.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item La probabilidad $(\rho)$ de que cualquier $A_i$ resulte en éxito es constante ya que las $n$ pruebas son con reposición.
\end{itemize}

Sea la variable aleatoria $X$ la cantidad de éxitos $(S)$ entre las $n$ pruebas.
Se denota $X=\bi(n,\rho)$ y su probabilidad puntual es:
\begin{equation*}
p_X(x) = \comb{n}{x} \rho^x \inParentheses{1-\rho}^{n-x}
\end{equation*}

Notar que el combinatorio es la cantidad de $x$ éxitos entre $n$ pruebas.
El factor $\rho^x$ es la proabilidad de cada éxito multiplicada $x$ veces.
Y el factor $\inParentheses{1-\rho}^{n-x}$ es la probabilidad de cada fracaso multiplicada por la cantidad de fracasos.

La esperanza y la varianza de $\bi(n,\rho)$ están dadas por:
\begin{gather*}
    E \inParentheses{\bi(n,\rho)} = n \, \rho
    \\
    V \inParentheses{\bi(n,\rho)} = n \, \rho \inParentheses{1-\rho}
\end{gather*}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Variable Aleatoria de Bernoulli}
    \begin{equation*}
        \be(\rho) = \bi(1,\rho)
    \end{equation*}
\end{mdframed}


\subsection{Variable aleatoria geométrica}

\begin{itemize}
\item Se realiza una cantidad incierta de pruebas para verificar si se da o no cierto evento $A_i$ en cada muestra.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item La probabilidad $(\rho)$ de que cualquier $A_i$ resulte en éxito es constante ya que las $n$ pruebas son con reposición.
\end{itemize}

Sea la variable aleatoria $X$ la primer prueba que resultó en éxito.
Se denota $X=\geo(\rho)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \rho \inParentheses{1-\rho}^{x-1}
\end{equation*}

La esperanza y la varianza de $\geo(\rho)$ están dadas por:
\begin{gather*}
    E \inParentheses{\geo(\rho)} = \frac{1}{\rho}
    \\[1ex]
    V \inParentheses{\geo(\rho)} = \frac{1-\rho}{\rho^2}
\end{gather*}


\subsection{Variable aleatoria hipergeométrica}

\begin{itemize}
\item Se realizan pruebas de $n$ muestras para verificar si se da o no cierto evento $A_i$ en cada una de las pruebas de un lote de $N$ muestras en total.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item Las pruebas son sin reposición.
\end{itemize}

Sea la variable aleatoria $X$ la cantidad de éxitos ($S$) entre las $(n)$ muestras.
Se denota $X=\hip(n,N,S)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \comb{S}{x} \comb{N-S}{n-x} \comb{N}{n}^{-1}
\end{equation*}

La esperanza y la varianza de $\hip(n,N,S)$ están dadas por:
\begin{gather*}
    E \inParentheses{\hip(n,N,S)} = \frac{n \, S}{N}
    \\[1ex]
    V \inParentheses{\hip(n,N,S)} = \frac{\frac{n \, S}{N} \inParentheses{1- \frac{S}{n}} (N-n)}{N-1}
\end{gather*}


\subsection{Variable aleatoria Poisson}

Sea la variable aleatoria $X$ la cantidad $x \in [0;n]$ de muestras que verifiquen un evento $A$.
Se denota $X=\po(\lambda)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \frac{\lambda^x}{e^\lambda x!}
\end{equation*}

La esperanza y la varianza de $\po(\lambda)$ son
\begin{equation*}
    E \inParentheses{\po(\lambda)} = V \inParentheses{\po(\lambda)} = \lambda
\end{equation*}


\section{Variables aleatorias contínuas}

Sea la variable aleatoria $X$ una función que toma valores del espacio muestral $\Omega$ y entrega valores reales.
\begin{equation*}
    X: \Omega \longrightarrow \setR \tq X(\omega) = x
\end{equation*}

Se dice que $X$ es una variable aleatoria contínua, ya que puede tener intervalos contínuos en su imagen.
\begin{equation*}
    \ran(X) = \im(X) \subseteq \setR
\end{equation*}


\section{Función de densidad}

La función de densidad $f_X(x)$ es una aproximación de la altura $h$ de cada rectángulo cuando el ancho $\Delta x \to 0$ en el histograma de una VAC.
\begin{equation*}
    f_X: \setR \longrightarrow \setR \tq 0 < f_X(x) \approx h(x)
\end{equation*}

En el histograma de un experimento, la altura de cada rectángulo depende de la frecuencia relativa $fr$ que tengan los valores $x$ de la VAC y de qué tantos valores $x \in \Delta x$ contemple dicha frecuencia.
Luego
\begin{equation*}
    h(x) = \frac{f_r}{\Delta x}
\end{equation*}

Además, el área de cada rectángulo se calcula multiplicando la altura $h(x)$ por el ancho $\Delta x$:
\begin{equation*}
    A = h(x) \, \Delta x
\end{equation*}

Si el ancho $\Delta x \to 0$, dejando constante la altura $h(x)$, se obtiene un diferencial de área:
\begin{equation*}
    \dif A = h(x) \, \dif x
\end{equation*}

Pero como $A=f_r$, se tiene que $\dif A$ es la mejor aproximación a la probabilidad de que $x \in \Delta x$ por lo tanto $f(x)$ aproxima a $h(x)$:
\begin{equation*}
    \dif F = f(x) \, \dif x
\end{equation*}


\section{Función de distribución}

El área bajo la gráfica de $f_X(x)$ representa la probabilidad de que una variable aleatoria contínua $x$ tome un valor entre $a$ y $b$:
\begin{gather*}
    A_i = h(x_i) \, \Delta x_i = f_r
    \\
    A_n = \sum_{i=1}^n h(x_i) \, \Delta x_i \approx A
    \\
    A = \lim_{n \to \infty} \sum_{i=1}^n f(x_i) \, \Delta x_i
    \\
    A = \int_a^b f(x) \, \dif x
    \\
    A = F(b) - F(a) = P(x \in [a,b])
\end{gather*}

La Función de Distribución Acumulada $F_X(x)$ es una función partida, contínua y no decreciente:
\begin{equation*}
    F_X: \setR \longrightarrow \setR \tq F_X(x) = \int_{-\infty}^k f(x) \, \dif x = P(x \leq k)
\end{equation*}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \noTi{El área total bajo la curva de la gráfica de $f_X(x)$ es  $P(\Omega)=1$, luego}
    \begin{equation*}
        \lim_{k \to \infty} P(x \leq k)=1
    \end{equation*}
\end{mdframed}


\section{Esperanza, varianza y mediana}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Esperanza}
    \begin{equation*}
        E(X) = \int_{-\infty}^{\infty} x \, \fx{x} \, \dif x
    \end{equation*}
\end{mdframed}

Las siguientes son propiedades de la esperanza:
\begin{gather*}
    E(a \, X + b) = a \, E(X) + b
    \\
    E(X+Y) = E(X) + E(Y)
    \\
    E \big( \fx[g]{x} \big) = \int_{-\infty}^{\infty} \fx[g]{x} \, \fx{x} \, \dif x
    \\
    E(X \, Y) = E(X) - E(Y) \quad \text{con $X,Y$ independientes}
\end{gather*}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza}
    \begin{equation*}
        V(X) = \int_{-\infty}^{\infty} \inParentheses{x - E(X)}^2 \fx{x} \, \dif x
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        V(X) = \int_{-\infty}^{\infty} x^2 \, f(x) \, \dif x - \inParentheses{E(X)}^2
    \end{equation*}
\end{mdframed}

Las siguientes son propiedades de la varianza:
\begin{gather*}
    V(a \, X+b) = a^2 \, V(X)
    \\
    V(X+Y) = V(X) + V(Y) \quad \text{con $X, Y$ independientes}
\end{gather*}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Mediana}
    \begin{equation*}
        M(X) = \inBraces{x \in \ran(X) \tq F_X(x) = 0.5}
    \end{equation*}
\end{mdframed}


\section{VAC famosas}


\subsection{Distribución uniforme}

Sea $X$ una VAC con distribución Uniforme:
\begin{equation*}
    X = \uni(a,b)
\end{equation*}

Su función de densidad está dada por:
\begin{equation*}
    f_X(x) =
    \left\{
    \begin{aligned}
        & \frac{1}{b-a} \quad \text{si } a \leq x < b
        \\[1ex]
        & 0 \quad \text{si } x \notin [a,b)
    \end{aligned}
    \right.
\end{equation*}

Y al integrar la rama $a \leq x < b$:
\begin{equation*}
    \int_a^k f(x) \, \dif x = \barrow{\dfrac{x}{b-a}}{a}{k}
\end{equation*}

Se tiene que la función de distribución $F_X(x)$ es:
\begin{equation*}
    F_X(x) =
    \left\{
    \begin{aligned}
        & \frac{x-a}{b-a} \quad \text{si } a \leq x < b
        \\
        & 0 \quad \text{si } x \notin [a,b)
    \end{aligned}
    \right.
\end{equation*}


\subsection{Distribución exponencial}

Sea $X$ una VAC con distribución exponencial:
\begin{equation*}
    X = \ex(\lambda)
\end{equation*}

Su función de densidad está dada por:
\begin{equation*}
    f_X(x) =
    \left\{
    \begin{aligned}
        & \frac{\lambda}{e^{\lambda x}} \quad \text{si } 0 \leq x
        \\[1ex]
        & 0 \quad \text{si } x < 0
    \end{aligned}
    \right.
\end{equation*}

Y al integrar la rama $a \leq x < b$:
\begin{equation*}
    \int_0^k f(x) \, \dif x = -\barrow{e^{-\lambda x}}{0}{k}
\end{equation*}

Se tiene que la función de distribución $F_X(x)$ es:
\begin{equation*}
    F_X(x) =
    \left\{
    \begin{aligned}
        & 1-e^{-\lambda x} \quad \text{si } 0 \leq x
        \\
        & 0 \quad \text{si } x < 0
    \end{aligned}
    \right.
\end{equation*}


\subsection{Distribución normal}

Sea $X$ una VAC con distribución normal:
\begin{equation}
    X = \nor(\mu,\sigma^2)
\end{equation}

Su la función de densidad está dada por:
\begin{equation*}
    f_X(x) = \frac{e^{\tfrac{(x-\mu)^2}{2 \, \sigma^2}}}{\sqrt{2 \, \pi} \, \sigma}
\end{equation*}

Con lo cual no es posible obtener una fórmula para la distribución mediante el cálculo de la densidad.

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Estandarización}
    \begin{equation*}
        \frac{X-\mu}{\sigma} = \nor(0,1)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        c \, \nor(\mu,\sigma^2) = \nor(c \, \mu, c^2 \, \sigma^2) \quad \text{con } c \in \setR
    \end{equation*}
\end{mdframed}


\section{Teorema central del límite}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
        \label{prop:TCL}
    \end{prop}
    \noTi{La suma de $n$ variables $X_i = \nor(\mu_i,\sigma_i^2)$ independientes está dada por}
    \begin{equation*}
        \sum_i^n X_i = \fx[\nor]{\sub{\mu}{tot},\sub{\sigma^2}{tot}}
    \end{equation*}
    \noTi{Donde}
    \begin{align*}
        \sub{\mu}{tot} &= \sum_i^n \mu_i
        \\[1ex]
        \sub{\sigma^2}{tot} &= \sum_i^n \sigma_i^2
    \end{align*}
\end{mdframed}

Dos o más variables se dicen idénticamente distribuidas si tienen igual distribución.
Esto es
\begin{equation*}
    \inBraces{X_1;X_2 \dots X_n} \text{ son IID} \iff F_{X_i}(x)=F_{X_j}(x) \quad \forall \, i,j \in [1,n]
\end{equation*}

De manera que se cumple que:
\begin{itemize}
    \item $E(X_i)$ es igual para toda $X_i$
    \item $V(X_i)$ es igual para toda $X_i$
    \item $p_{X_i}(x)$ es igual para toda VAD $X_i$
    \item $f_{X_i}(x)$ es igual para toda VAC $X_i$
\end{itemize}

Una muestra aleatoria es un conjunto de variables aleatorias independientes e idénticamente distribuidas (IID).

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema central del límite}
    \cusTe{Dadas $X_i$ VAC IID (no necesariamente normales) se cumple}
    \begin{equation*}
        \sum_i^n X_i \approx \nor(n \, \mu , n \, \sigma^2)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=PropertyFrame]
    \begin{prop}
    \end{prop}
    \cusTi{Corolario del TCL}
    \begin{equation*}
        \ave{X} = \dfrac{\sum_i^n X_i}{n} \approx \fx[\nor]{\mu, \tfrac{\sigma^2}{n}}
    \end{equation*}
    \noTi{Donde}
    \begin{align*}
        \mu &= E(\mu_i) 
        \\
        \sigma^2 &= V(\sigma_i^2)
    \end{align*}
\end{mdframed}


\section{Estimación puntual}

Sean $X_1,X_2 \dots X_n$ muestras aleatorias con esperanza $\mu$ y varianza $\sigma^2$.
Para $n>30$, es posible estimar los \emph{momentos} $\hat{\mu}$ y $\hat{\sigma}^2$ que aproximan a la esperanza y la varianza, respectivamente.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Promedio muestral}
    \begin{equation*}
        \ave{X} = \frac{\sum_{i=1}^n X_i}{n} = \hat{\mu} \approx E(X_i) = \mu
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza muestral}
    \begin{equation*}
        s^2 = \frac{\sum_{i=1}^n \inParentheses{X_i - \ave{X}}^2}{n-1} = \hat{\sigma}^2 \approx V(X_i) = \sigma^2 
    \end{equation*}
\end{mdframed}

Cualquier parámetro $\theta$ de una muestra aleatoria ya sea este $\mu, \sigma, \lambda, \rho$ se puede estimar como aquel que verifique la definición de esperanza:
\begin{equation*}
    E_{\hat{\theta}} (X_i) = \ave{X} \implies \theta \approx \hat{\theta}
\end{equation*}

Al estimar un parámetro, se comete un error, ya que $n$ es finito.
El error cuadrático medio ($\ecm$) está definido como:
\begin{equation*}
    \ecm = E \big( (\hat{\theta} - \theta)^2 \big)
    = \big( E(\hat{\theta})-\theta \big)^2 + V(\hat{\theta})
\end{equation*}

Donde $E(\hat{\theta})-\theta$ se denomina \emph{sesgo}.
Se dice que un estimador es insesgado si el sesgo es nulo.


\section{Intervalos de confianza}

Además de estimar un parámetro $\theta$ de manera puntual, se puede determinar un \emph{intervalo de confianza} $I=[a;b]$ de manera que los valores entre $a$ y $b$ sean los que probablemente tome el parámetro real.
Esto es
\begin{equation*}
    P(a<\theta<b) = 1 - \alpha
\end{equation*}

Donde $\alpha \in [0,001 ; 0,05]$ es el nivel de significación.

Como el intervalo $I$ está centrado en $\hat{\theta}$, el error máximo cometido es
\begin{equation*}
    \varepsilon = \frac{b-a}{2}
\end{equation*}


\subsection{Intervalo para la esperanza}

Según la propiedad \ref{prop:TCL}, se tiene para $X_1, X_2 \dots X_n$ VAC con distribución normal:
\begin{equation*}
    \ave{X} = \fx[\nor]{\mu, \tfrac{\sigma^2}{n}}
\end{equation*}

Obteniendo, al estandarizar:
\begin{equation*}
    \frac{\ave{X}-\mu}{\sqrt{\dfrac{\sigma^2}{n}}} = \fx[\nor]{0,1} = Z
\end{equation*}

Por otro lado, se define el percentil $Z_\alpha$ tal que
\begin{equation*}
    P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha
\end{equation*}

Que es $Z_\alpha=1.96$ si el nivel de confianza fuese $95\%$ o, lo que es lo mismo, el nivel de significación fuese $\alpha=0.5$ pues
\begin{equation*}
    P(-1.96<Z<1.96) = 0.95
\end{equation*}

Reemplazando $Z$ en la probabilidad definida, queda
\begin{gather*}
    P \inParentheses{ -Z_\alpha < \frac{\ave{X}-\mu}{\sqrt{\dfrac{\sigma^2}{n}}} < Z_\alpha } = 1 - \alpha
    \\[1ex]
    P \inParentheses{ - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < \ave{X} - \mu < Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
    \\[1ex]
    P \inParentheses{ - \ave{X} - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < -\mu < - \ave{X} + Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
    \\[1ex]
    P \inParentheses{ \ave{X} - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < \mu < \ave{X} + Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
\end{gather*}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo para la esperanza dado $\sigma^2$}
    \begin{equation*}
        \mu = \ave{X} \pm Z_\alpha \sqrt{\frac{\sigma^2}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}

En caso de no conocer la varianza real se realiza el mismo análisis pero para $\sigma^2 \approx s^2$ considerando que la distribución no va a ser normal estandar sino T-student de grado $n-1$.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo para la esperanza dado $s^2$}
    \begin{equation*}
        \mu = \ave{X} \pm T_\alpha \sqrt{\frac{s^2}{n}}
    \end{equation*}
    \noTi{Donde $T_\alpha$ es el percentil cuya distribución T-student verifica $P(T_{n-1}>T_\alpha) = \dfrac{\alpha}{2}$.}
\end{mdframed}

En caso de no conocer la distribución de las VAC $X_1, X_2 \dots X_n$ se realiza una aproximación.
Sea $\ave{X}$ el promedio de una muestra aleatoria de $n>30$ variables, con cualquier distribución.
Por TCL $s^2 \to \sigma^2$.

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo de nivel asintótico para la esperanza}
    \begin{equation*}
        \mu = \ave{X} \pm Z_\alpha \sqrt{\frac{s^2}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}


\subsection{Intervalo para la proporción}

Sea $\ave{X}$ el promedio de una muestra aleatoria de $n>30$ variables de Bernoulli.
Por TCL se trata de una suma de variables con esperanza $\rho \approx \ave{X}$ y varianza $\rho \inParentheses{1-\rho}$.
\begin{align*}
    \ave{X} & \approx \fx[\nor]{\rho , \frac{\rho \inParentheses{1-\rho}}{n}}
    \\
    \frac{\ave{X}-\rho}{\sqrt{\frac{\ave{X} \inParentheses{1-\ave{X}}}{n}}} & \approx \nor(0,1)
\end{align*}

\begin{mdframed}[style=DefinitionFrame]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo de nivel asintótico para la proporción}
    \begin{equation*}
        \rho = \ave{X} \pm Z_\alpha \sqrt{\frac{\ave{X} \inParentheses{1-\ave{X}}}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}


\subsection{Intervalo para la varianza}

Dada una muestra aleatoria con distribución normal, se tiene:
\begin{equation*}
    P \inParentheses{\frac{(n-1)s^2}{\chi_{n-1,\tfrac{\alpha}{2}}^2} < \sigma^2 < \frac{(n-1)s^2}{\chi_{n-1,1-\tfrac{\alpha}{2}}^2} } = 1 - \alpha
\end{equation*}


\section{Test de hipótesis}

Un test de hipótesis sirve para determinar si una modificación en un experimento es significativa o no.

Sea $\mu$ la esperanza luego de haber aplicado la modificación.
Se definen dos posibles hipótesis.
La hipótesis $h_0$ o ``conservadora'' sugiere que no hay evidencia de que la modificación haya sido eficiente.
La hipótesis $h_1$ o ``progresista'' sugiere que la modificación implicaría un cambio efectivo.
\begin{equation*}
    \left\{
    \begin{aligned}
        h_0: \mu = \mu_0
        \\
        h_1: \mu \neq \mu_0
    \end{aligned}
    \right.
\end{equation*}

Podemos definir los posibles errores cometidos al determinar qué hipótesis es correcta.
El primer error es suponer que la modificación es significativa cuando no lo es.
El segundo es descartarla cuando es significativa.
El primero es más grave ya que implica una inversión en un cambio que no representa una mejora.
\begin{equation*}
    \left\{
    \begin{aligned}
        \text{Error de tipo 1: Rechazar $h_0$ siendo verdadera.}
        \\
        \text{Error de tipo 2: No rechazar $h_0$ siendo falsa.}
    \end{aligned}
    \right.
\end{equation*}

El test de hipótesis consiste en determinar el riesgo que se quiere correr al optar por la modificación.
El riesgo está dado por $\alpha$ o $\beta$ según determine el experimento el tipo de error por el cual guiarse.
Se rechaza o no $h_0$ si y solo si
\begin{equation*}
    \left\{
    \begin{aligned}
        P \inParentheses{\text{Error de tipo 1}} = \alpha
        \\
        P \inParentheses{\text{Error de tipo 2}} = \beta
    \end{aligned}
    \right.
\end{equation*}


\subsection{Test para la esperanza}

Según la propiedad \ref{prop:TCL}, se tiene para $X_1, X_2 \dots X_n$ VAC con distribución normal:
\begin{equation*}
    \ave{X} = \fx[\nor]{\mu_0, \tfrac{\sigma^2}{n}}
\end{equation*}

Obteniendo, estandarizar:
\begin{gather*}
    \frac{\ave{X}-\mu_0}{\sqrt{\dfrac{\sigma^2}{n}}} = \fx[\nor]{0,1} = Z
    \\[1ex]
    \frac{\norm{\ave{X}-\mu_0}}{\sqrt{\dfrac{\sigma^2}{n}}} = \norm{Z}
\end{gather*}

Por otro lado, se define el percentil $Z_\alpha$ tal que:
\begin{equation*}
    P(\norm{Z}>\norm{Z_\alpha}) = \alpha
\end{equation*}

Esta ecuación indica que la probabilidad de que la ``distancia'' entre el promedio muestral ($\ave{X}$) y la media real ($\mu_0$) sea mayor que el percentil $Z_\alpha$ es tan baja como $\alpha$ lo sea.
Por lo tanto, se optará por rechazar $h_0$ si se verifica la siguiente inecuación.
\begin{equation*}
    \norm{\ave{X}-\mu_0} > \norm{Z_\alpha} \sqrt{\dfrac{\sigma^2}{n}}
\end{equation*}

% \subsection{P-valor}

% \section{Regresión lineal}