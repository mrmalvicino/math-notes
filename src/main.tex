\documentclass[a5paper,12pt,twoside]{book}

\input{book_packages}
\input{book_commands}

% ENTORNOS DE NUMERACIÓN
\newtheorem{defn}{{Definición}}[chapter]
\newtheorem{prop}{{Propiedad}}[chapter]
\newtheorem{example}{{Ejemplo}}[chapter]

% ENTORNOS DE FORMATO
\newenvironment{formatI}{\vspace{1ex}\par\small\sffamily} % Consignas de ejemplos


\begin{document}

\pagestyle{fancy}
\fancyhf{}
\chead{\scriptsize \nouppercase\rightmark}
\cfoot{\scriptsize \thepage}
\pagenumbering{gobble}
\renewcommand{\headrulewidth}{0pt}

\frontmatter
%\includepdf{./images/cover/cover-front}

\begin{center}

    \begin{Huge}
    \textbf{Notas de matemática para ingeniería de sonido}
    \end{Huge}

    \vspace{1cm}
    \textbf{Primera edición v0.1}
    \vspace{2cm}

    \begin{Large}
        Malvicino, Maximiliano R.
    \end{Large}

\end{center}

\clearpage
\noindent
\textbf{Prefacio}

Notas de cálculo de varias variables reales, cálculo de una variable compleja y probabilidad.
La versión digital más reciente de este texto es gratis y puede ser descargada de
\begin{center}
    \small
    \url{https://github.com/mrmalvicino/math-notes}
\end{center}

\renewcommand{\spanishappendixname}{Anexo}
\tableofcontents

\mainmatter
\pagenumbering{arabic}


\chapter{Topología}


\section{Norma}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Norma}
    \cusTe{La norma es la distancia entre dos puntos $P$ y $Q$.}
    \begin{equation*}
        \nnorm{P-Q} = \operatorname{dist} (P,Q)
    \end{equation*}
\end{mdframed}

Hay varios tipos de normas, siendo algunas definiciones más precisas que otras con la desventaja de ser computacionalmente menos eficientes.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Norma 1}
    \begin{equation*}
        \nnorm{\Vec{x}}_1 = \sum_{\ith=1}^\nth \norm{x_\ith}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:norm2}
    \end{defn}
    \cusTi{Norma 2}
    \begin{equation*}
        \nnorm{\Vec{x}}_2 = \sqrt{\sum_{\ith=1}^\nth x_\ith^2}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Norma infinito}
    \begin{equation*}
        \nnorm{\Vec{x}}_\infty = \operatorname{max} \big\{ \norm{x_\ith} \big\}
    \end{equation*}
\end{mdframed}


\concept{Propiedades de la norma 2}

\begin{align*}
    \nnorm{\Vec{x}} &= 0 \iff \Vec{x} = 0
    \\[1ex]
    \nnorm{k \, \Vec{x}} &= \norm{k} \cdot \nnorm{\Vec{x}} \quad k\in\setR
    \\[1ex]
    \nnorm{\Vec{x}} &\leq \nnorm{\Vec{x}} + \nnorm{\Vec{y}}
    \\[1ex]
    \nnorm{\Vec{x}+\Vec{y}} &\leq \nnorm{\Vec{x}} + \nnorm{\Vec{y}}
    \\[1ex]
    \norm{\Vec{x} \cdot \Vec{y}} &\leq \nnorm{\Vec{x}} \cdot \nnorm{\Vec{y}}
    \\[1ex]
    \norm{x_\ith} &= \nnorm{\Vec{x}}
\end{align*}


\section{Entornos y bolas}

El entorno o bola de un punto es un conjunto que contiene los puntos más próximos a él.
Se lo llama entorno cuando se trata de un intervalo en una recta, disco cuando se trata de un plano y bola en el espacio o en $\nth$ dimensiones.
Dependiendo de si contiene o no a los puntos del borde o al punto del centro se pueden definir bolas abiertas, cerradas y reducidas.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Bola cerrada}
    \begin{equation*}
        \Bar{B} (\Vec{x}_0;r) = \{ \Vec{x} \in \setR^n :  \nnorm{\Vec{x}-\Vec{x}_0} \leq r \}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Bola abierta}
    \begin{equation*}
        B(\Vec{x}_0;r) = \{ \Vec{x} \in \setR^\nth : \nnorm{\Vec{x}-\Vec{x}_0} <r \}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Bola reducida}
    \begin{equation*}
        B' (\Vec{x}_0;r) = \{ \Vec{x} \in \setR^n :  \nnorm{\Vec{x}-\Vec{x}_0} <r \} - \Vec{x}_0
    \end{equation*}
\end{mdframed}

A continuación se grafican un disco cerrado, uno abierto y uno reducido respectivamente.

\begin{center}
    \def\svgwidth{0.8\linewidth}
    \input{./images/topo-bola.pdf_tex}
\end{center}


\section{Clasificación de puntos}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Punto interior}
    \begin{equation*}
        \Vec{x}_0 \in A^0 \iff \exists \, r>0 \tq B(\Vec{x}_0;r) \subset A
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Punto exterior}
    \begin{equation*}
        \Vec{x}_0 \in A^c \iff \exists \, r>0 \tq B(\Vec{x}_0;r) \subset A^c
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Punto frontera}
    \begin{multline*}
        \Vec{x}_0 \in \partial A \iff \forall \, r>0 : \\ B(\Vec{x}_0;r) \cap A \neq \setO \quad \land \quad B(\Vec{x}_0;r) \cap A^c \neq \setO
    \end{multline*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Punto aislado}
    \begin{equation*}
        \Vec{x}_0 : \exists \, r>0 / \big\{ B(\Vec{x}_0;r) \cap A \big\} = \Vec{x}_0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:limitPoint}
    \end{defn}
    \cusTi{Punto de acumulación}
    \begin{equation*}
        \Vec{x}_0 \in A' \iff \forall r>0 : \big\{ B'(\Vec{x}_0;r) \cap A \big\} \neq \setO
    \end{equation*}
\end{mdframed}


\section{Clasificación de conjuntos}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Complemento de un conjunto}
    \begin{equation*}
        A^c = \setR^\nth - A
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto acotado}
    \begin{equation*}
        A: \exists \, r>0 / A \subset B(0;r)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto abierto}
    \begin{equation*}
        A: A=A^0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto cerrado}
    \begin{equation*}
        A: \big\{ \partial A \subset A \big\} \lor \big\{ A=\operatorname{cl}(A) \big\}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto compacto}
    \cusTe{$A$ es un conjunto compacto si es cerrado y acotado simultáneamente.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto conexo}
    \cusTe{Para todo par de puntos existe al menos una trayectoria que los une.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto simplemente conexo}
    \cusTe{Para todo par de curvas dentro del conjunto que comparten sus extremos existe una deformación tal que es posible convertir una curva en la otra dejando fijos los extremos.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto convexo}
    \cusTe{Para todo par de puntos existe una trayectoria recta que los une.}
\end{mdframed}


\chapter{Propiedades reales}


\section{Identidades pitagóricas}

\begin{gather*}
    \sin^2{(x)} + \cos^2{(x)} = 1
    \\
    \sec^2{(x)} - \tan^2{(x)} = 1
    \\
    \csc^2{(x)} - \cot^2{(x)} = 1
\end{gather*}


\section{Relaciones trigonométricas}

\begin{gather*}
    \sin{(x)}=\frac{O}{H}
    \\[1ex]
    \cos{(x)}=\frac{A}{H}
    \\[1ex]
    \tan{(x)}=\frac{O}{A}
    \\[1ex]
    \cot{(x)}=\frac{A}{O}
    \\[1ex]
    \sec{(x)}=\frac{H}{A}
    \\[1ex]
    \csc{(x)}=\frac{H}{O}
\end{gather*}


\section{Funciones trigonométricas}

\begin{gather*}
    \sin{(x)}=\frac{1}{\csc{(x)}}
    \\[1ex]
    \cos{(x)}=\dfrac{1}{\sec{(x)}}
    \\[1ex]
    \tan{(x)}=\frac{\sin{(x)}}{\cos{(x)}}
    \\[1ex]
    \cot{(x)}=\frac{\cos{(x)}}{\sin{(x)}}
    \\[1ex]
    \sec{(x)}=\frac{1}{\cos{(x)}}
    \\[1ex]
    \csc{(x)}=\frac{1}{\sin{(x)}}
\end{gather*}


\section{Ley de senos y cosenos}

\begin{equation*}
    \frac{A}{\sin{(\beta)}} = \frac{H}{\sin{(\gamma)}} = \frac{O}{\sin{(\alpha)}}
\end{equation*}

\begin{equation*}
    \left\{
    \begin{aligned}
        A^2 &= H^2+O^2-2HO.\cos{(A)}
        \\
        H^2 &= A^2+O^2-2AO.\cos{(H)}
        \\
        O^2 &= A^2+H^2-2AH.\cos{(O)}
    \end{aligned}
    \right.
\end{equation*}


\section{Suma y diferencia de ángulos}

\begin{equation*}
    \sin{(x \pm y)} = \sin{(x)} \, \cos{(y)} \pm \cos{(x)}\sin{(y)}
\end{equation*}

\begin{equation*}
    \cos{(x \pm y)} = \cos{(x)} \, \cos{(y)} \mp \sin{(x)}\sin{(y)}
\end{equation*}

\begin{equation*}
    \tan{(x \pm y)} = \dfrac{\tan{(x)} \pm \tan{(y)}}{1 \pm \tan{(x)} \, \tan{(y)}}
\end{equation*}

\begin{equation*}
    \cot{(x \pm y)} = \dfrac{\cot{(x)} \, \cot{(y)} \pm1}{\cot{(x)} \pm \cot{(y)}}
\end{equation*}

\begin{equation*}
    \sec{(x \pm y)} = \dfrac{1}{\cos{(x \pm y)}}
\end{equation*}

\begin{equation*}
    \csc{(x \pm y)} = \dfrac{1}{\sen{(x \pm y)}}
\end{equation*}


\section{Identidades de ángulo doble}

\begin{equation*}
    \sin{(2x)}=2 \, \sin{(x)} \, \cos{(x)}
\end{equation*}

\begin{equation*}
    \cos{(2x)}=\cos^2{(x)}-\sin^2{(x)}
\end{equation*}

\begin{equation*}
    \tan{(2x)}=\dfrac{2\tan{(x)}}{1-\tan^2{(x)}}
\end{equation*}

\begin{equation*}
    \cot{(2x)}=\dfrac{\cot^2{(x)}-1}{2\cot{(x)}}
\end{equation*}


\section{Propiedades logarítmicas}

\begin{equation*}
    \log_b{(x \, y)} = \log_b{(x)} + \log_b{(y)}
\end{equation*}

\begin{equation*}
    \log_b{\left( \frac{x}{y} \right)} = \log_b{(x)} - \log_b{(y)}
\end{equation*}

\begin{equation*}
    k \, \log_b{(x)} = \log_b{\left( x^k \right)}
\end{equation*}

\begin{equation*}
    \log_b{(x)}=\frac{\log_c{(x)}}{\log_c{(b)}}
\end{equation*}


\chapter{Álgebra elemental}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Matriz diagonal}
    \cusTe{Una matriz se dice diagonal si $a_{ij}=0$ cuando $i \neq j$}
    \begin{equation*}
        A =
        \begin{pmatrix}
            a_{11} & 0 & \dots & 0
            \\
            0 & a_{22} & \dots & 0
            \\
            \vdots & \vdots & \ddots & \vdots
            \\
            0 & 0 & \dots & a_{\mth\nth}
        \end{pmatrix}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El determinante de una matriz triangulada inferior o superiormente es el producto de su diagonal principal.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si el determinante de una matriz equivalente es distinto de cero, entonces el determinante de la matriz original también lo es.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Rango de una matriz}
    \cusTe{Se denota como $\operatorname{Rg}(M)$ la cantidad de filas o columnas LI de una matriz una vez triangulada.}
\end{mdframed}

\begin{center}
    \def\svgwidth{\linewidth}
    \input{./images/alg-sistemas.pdf_tex}
\end{center}


\section{Teo. de Rouché–Frobenius}

Dado un sistema lineal de $\mth$ ecuaciones con $\nth$ incógnitas:
\begin{equation*}
    \left\{
    \begin{matrix}
        a_{1,1} \, x_1 + a_{1,2} \, x_2 + \dots + a_{1,\nth} \, x_\nth & = & b_1
        \\
        a_{2,1} \, x_1 + a_{2,2} \, x_2 + \dots + a_{2,\nth} \, x_\nth & = & b_2
        \\
        \vdots & \vdots & \vdots
        \\
        a_{\mth,1} \, x_1 + a_{\mth,2} \, x_2 + \dots + a_{\mth,\nth} \, x_\nth & = & b_\mth
    \end{matrix}
    \right.
\end{equation*}

Representado de forma matricial por:
\begin{gather*}
    A \cdot \Vec{x} = \Vec{b}
    \\
    \begin{pmatrix}
        a_{1,1} & \dots & a_{1,\nth}
        \\
        \vdots & \ddots & \vdots
        \\
        a_{\mth,1} & \dots & a_{\mth,\nth}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        x_1
        \\
        \vdots
        \\
        x_\nth
    \end{pmatrix}
    =
    \begin{pmatrix}
        b_1
        \\
        \vdots
        \\
        b_\mth
    \end{pmatrix}
\end{gather*}

Entonces el sistema lineal es compatible si el rango de la matriz de coeficientes es igual al rango de la matriz ampliada:
\begin{equation*}
    \operatorname{rg}(A) = \operatorname{rg}\left(A\Big|\Vec{b}\right)
\end{equation*}

Y el conjunto de soluciones $S$ se puede expresar de forma paramétrica por $\nth-\operatorname{rg}(A)$ parámetros.
Particularmente, si $\operatorname{rg}(A)=\nth$ entonces la solución es un único punto.

Más formalmente, se dice que el conjunto de soluciones forman un subespacio afín $S\in\setR^\nth$ que verifica:
\begin{equation*}
    \operatorname{dim}(S) = \nth-\operatorname{rg}(A)
\end{equation*}



\chapter{Espacios vectoriales}

Los espacios vectoriales son estructuras algebráicas denotados como $\setV$ por tuplas cuya primer componente representa un conjunto no vacío y las demás indican operaciones definidas, eventualmente sobre un cuerpo $\setK$.

El espacio vectorial $\setV=(\setR^\nth,+,\cdot)$ es una estructura algebráica dada por el conjunto $\setR^\nth$ dotado de las operaciones suma y producto por un escalar sobre el cuerpo $\setK=\setR$ tal que para todo $ \Vec{u},\Vec{v},\Vec{w},\Vec{0} \in \setV$ y para todo $a,b,1 \in \setK$:

La operación interna $+:\setV \times \setV \longrightarrow \setV $ verifica:

\begin{itemize}
    \item Propiedad conmutativa: $\Vec{v}+\Vec{w}=\Vec{w}+\Vec{v}$
    \item Propiedad asociativa: $\Vec{u} + \left( \Vec{v}+\Vec{w} \right) = \left( \Vec{u} + \Vec{v} \right) + \Vec{w}$
    \item Existencia del elemento neutro: $\exists \, \Vec{0} \tq \Vec{v} + \Vec{0} = \Vec{v}$
    \item Existencia del elem. opuesto: $\exists \, -\Vec{v} \tq \Vec{v} - \Vec{v} = \Vec{0}$
\end{itemize}

Y la operación externa $\cdot:\setK \times \setV \longrightarrow \setV$ verifica:

\begin{itemize}
    \item Propiedad asociativa: $a \cdot \left( b \cdot \Vec{v} \right) = \left( a \cdot b \right) \cdot \Vec{v}$
    \item Prop. distributiva respecto de la suma vectorial: $a \cdot \left( \Vec{v} + \Vec{w} \right) = a \cdot \Vec{v} + a \cdot \Vec{w}$
    \item Prop. distributiva respecto de la suma escalar: $\Vec{v} \cdot \left(a+b\right) = a \cdot \Vec{v} + b \cdot \Vec{v}$
    \item Existencia del elemento neutro: $\exists \, 1 \tq \Vec{v} \cdot 1 = \Vec{v}$
\end{itemize}


\section{Subespacios vectoriales}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Combinación lineal}
    \cusTe{Dados los $\kth$ elementos del conjunto $\braces{\Vec{v}_\ith}$ con $1 \leq \ith \leq \kth$ se dice que cierto vector $\Vec{v} \in \setR^\nth$ es combinación lineal de dichos $\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth$ si existen los escalares $\lambda_1, \lambda_2 \dots \lambda_\kth$ tal que:}
    \begin{equation*}
        \Vec{v} = \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\kth \, \Vec{v}_\kth
        = \sum_{\ith=1}^\kth \lambda_\ith \, \Vec{v}_\ith
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Independencia lineal}
    \cusTe{Un conjunto $S \subseteq \setR^n$ es linealmente independiente si no existe ningún $\Vec{v}_\ith \in S$ que sea combinación lineal de los demás elementos.
    Esto es:}
    \begin{equation*}
        \Vec{0} = \sum_{\ith=1}^\kth \lambda_\ith \, \Vec{v}_\ith
        \iff \lambda_\ith = 0 \enspace \forall \enspace \ith \in [1;\kth]
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Subespacio vectorial}
    \cusTe{Se dice que un subconjunto $S \subseteq \setR^\nth$ es un subespacio si para cualquier escalar $\lambda \in \setR$ y dupla de vectores $\Vec{v}_1, \Vec{v}_2 \in S$ se verifica:}
    \begin{equation*}
        \Vec{0} \in S \enspace \land \enspace
        \lambda \, \Vec{v}_1 + \Vec{v}_2 \in S
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto de generadores}
    \cusTe{Se dice que el conjunto $A=\braces{\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth}$ es un conjunto de generadores de $S \subseteq \setR^\nth$ si es posible obtener cualquier elemento de $S$ mediante combinaciones lineales de los elementos de $A$.}
    \begin{multline*}
        S = \operatorname{gen}\braces{\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth} \iff
        \\
        \Vec{w} = \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\kth \, \Vec{v}_\kth \quad \forall \, \Vec{w} \in S
    \end{multline*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Base}
    \cusTe{Se dice que el conjunto $B=\braces{\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth}$ es base de $S \subseteq \setR^\nth$ si es un conjunto generador linealmente independiente.
    Esto es:}
    \begin{equation*}
        S = \operatorname{gen}\braces{\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth}
        \enspace \land \enspace
        B \hspace{1ex} \textrm{es LI}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Dimensión}
    \cusTe{Dado el conjunto $S\subseteq \setR^\nth$ y dada la base $B=\braces{\Vec{v}_1, \Vec{v}_2 \dots \Vec{v}_\kth}$ tal que $S=\operatorname{gen}(B)$ se define la dimensión de $S$ como la cantidad de elementos de $B$.}
    \begin{equation*}
        \operatorname{dim}(S) = \kth
    \end{equation*}
\end{mdframed}

Observar que la cantidad ($k$) de elementos de un conjunto generador de $S$ puede ser mayor, igual o menor que la dimensión de un espacio que contenga a $S$.
Pero no puede ser menor que la dimensión de $S$.
Se identifican entonces dos casos:
\begin{itemize}
    \item Primer caso: $\operatorname{dim}(S) \leq \nth \leq \kth$
    \item Segundo caso: $\operatorname{dim}(S) \leq \kth \leq \nth$
\end{itemize}

\begin{mdframed}[style=MyFrame2]
    \begin{example}
    \end{example}
    Plano con $\operatorname{dim}(S)=2, \nth=3, \kth=4$:

    \begin{center}
        \def\svgwidth{0.6\linewidth}
        \input{./images/alg-generadores.pdf_tex}
    \end{center}

    Plano con $\operatorname{dim}(S)=2, \nth=3, \kth=2$:

    \begin{center}
        \def\svgwidth{0.6\linewidth}
        \input{./images/alg-base.pdf_tex}
    \end{center}
\end{mdframed}

Observar que la cantidad de elementos de una base de $S$, que es igual a la dimensión de $S$, puede ser menor o igual que la dimensión de un espacio que contenga a $S$ pero no mayor.
\begin{equation*}
    \operatorname{dim}(S)\leq\nth
\end{equation*}

Una base que resulta de particular interés es la llamada \emph{base canónica} denotada como
\begin{equation*}
    E = \braces{\eVer_1,\eVer_2 \dots \eVer_\nth}
\end{equation*}

Siendo
\begin{gather*}
    \eVer_1 = (1,0,0 \dots 0)
    \\
    \eVer_2 = (0,1,0 \dots 0)
    \\
    \eVer_\nth = (0,0 \dots 0,1)
\end{gather*}


\section{Coordenadas y cambio de base}

Las coordenadas de un vector $\Vec{w}=(w_1,w_2 \dots w_\nth)$ expresado en la base canónica son las componentes del vector.
\begin{gather*}
    \Vec{w} = \lambda_1 \, \eVer_1 + \lambda_2 \, \eVer_2 + \dots + \lambda_\nth \, \eVer_\nth
    \\
    \Vec{w} = w_1 \, \eVer_1 + w_2 \, \eVer_2 + \dots + w_\nth \, \eVer_\nth
    \\
    [\Vec{w}]_E = (w_1,w_2 \dots w_\nth)
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Coordenadas en una base dada}
    \cusTe{Dado un espacio vectorial $\setV$ generado por una base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ para $\Vec{w} \in \setV$ hay una única combinación lineal que permite obtener $\Vec{w} = \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth$ de manera que se definen las coordenadas de $\Vec{w}$ en la base $B$ como:}
    \begin{equation*}
        [\Vec{w}]_B = (\lambda_1,\lambda_1 \dots \lambda_\nth) \in \setR^\nth
    \end{equation*}
\end{mdframed}

Observar que $\setV$ puede ser un espacio abstracto, por ejemplo de polinomios o matrices.
Pero las coordenadas de sus elementos van a estar dadas en $\setR^\nth$ porque $\operatorname{dim}(\setV)=\nth$.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Dado el conjunto $\braces{\Vec{w}_\ith} \in \setV$ con $1\leq\ith\leq\mth$ y una base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ se tiene que:
    \begin{gather*}
        \braces{\Vec{w}_1,\Vec{w}_2 \dots \Vec{w}_\mth} \hspace{1ex} \textrm{es LI}
        \\
        \Updownarrow
        \\
        \braces{[\Vec{w}_1]_B , [\Vec{w}_2]_B \dots [\Vec{w}_\mth]_B} \hspace{1ex} \textrm{es LI}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame2]
    \begin{example}
    \end{example}
    Dado el espacio vectorial $\setV=\setR^2$ y dos bases $E=\braces{\eVer_1,\eVer_2}$ y $B=\braces{\Vec{v}_1,\Vec{v}_2}=\braces{(1,1),(1,-1)}$ se quieren calcular, para el vector $\Vec{w}$ ubicado en el punto $(x,y)=(-1,1)$, las coordenadas en cada base.
    
    \begin{center}
        \def\svgwidth{0.6\linewidth}
        \input{./images/alg-base-cambio.pdf_tex}
    \end{center}
    
    Las coordenadas del vector $\Vec{w}$ en la base canónica son:
    \begin{align*}
        [\Vec{w}]_E &= \lambda_1 \, \eVer_1 + \lambda_2 \, \eVer_2
        \\
        (x;y) &= \lambda_1 \left(1,0\right) + \lambda_2 \left(0,1\right)
        \\
        (-1;1) &= (\lambda_1,\lambda_2)
    \end{align*}
    
    Vemos que en base canónica el vector $\Vec{w}$ tiene como coordenadas sus componentes.
    
    Mientras que en la base $B$ las cordenadas de $\Vec{w}$ están dadas por el siguiente sistema lineal:
    \begin{align*}
        [\Vec{w}]_E &= \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2
        \\
        (x;y) &= \lambda_1 \left(1,1\right) + \lambda_2 \left(1,-1\right)
        \\
        (-1;1) &= (\lambda_1+\lambda_2,\lambda_1-\lambda_2)
    \end{align*}
    
    Planteando el sistema de manera tradicional, se obtiene:
    \begin{gather*}
        \left\{
        \begin{aligned}
            -1 &= \lambda_1+\lambda_2 \Rightarrow -\lambda_2=\lambda_1+1 
            \\
            1 &= \lambda_1-\lambda_2 \Rightarrow 1=\lambda_1+(\lambda_1+1)
        \end{aligned}
        \right.
        \\
        \Rightarrow (0,-1) = (\lambda_1,\lambda_2)
    \end{gather*}
    
    O bien, podríamos haber llegado al mismo resultado planteando el sistema de forma matricial:
    \begin{gather*}
        \begin{pmatrix}
            -1
            \\
            1
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 1
            \\
            1 & -1
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            \lambda_1
            \\
            \lambda_2
        \end{pmatrix}
    \end{gather*}
\end{mdframed}

En el ejemplo anterior, la matriz que al ser multiplicada por las coordenadas de $\Vec{w}$ en base $B$ da como resultado las coordenadas del vector en base canónica, se la conoce como matriz de cambio de base.
Observar que tiene por columnas los elementos de la base.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:C_BE}
    \end{defn}
    \cusTi{Matriz de cambio de base de $B$ a $E$}
    \cusTe{Sea $C_{BE}$ una matriz cuyas columnas son las coordenadas en base canónica de los vectores de cierta base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ tal que:}
    \begin{equation*}
        \begin{bmatrix}
            \Vec{w}
        \end{bmatrix}_E
        = C_{BE} \cdot
        \begin{bmatrix}
            \Vec{w}
        \end{bmatrix}_B
    \end{equation*}
    \noTi{Donde:}
    \begin{equation*}
        C_{BE} =
        \begin{pmatrix}
            \trans{\sqb{\Vec{v}_1}_E} & \trans{\sqb{\Vec{v}_2}_E} & \dots & \trans{\sqb{\Vec{v}_\nth}_E}
        \end{pmatrix}
    \end{equation*}
\end{mdframed}

A partir de la definición anterior, multiplicando por la inversa de la matriz se tiene:
\begin{align*}
    C_{BE} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    &=
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
    \\[1ex]
    C_{BE}^{-1} \cdot C_{BE} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    &= C_{BE}^{-1} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
    \\[1ex]
    I \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    &= C_{BE}^{-1} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
    \\[1ex]
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    &= C_{EB} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
\end{align*}

Quedando definida la matriz de cambio de base que al ser multiplicada por un vector en base canónica da como resultado las coordenadas del vector en base $B$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Matriz de cambio de base de $E$ a $B$}
    \cusTe{Sea $C_{EB}$ la inversa de $C_{BE}$ tal que:}
    \begin{equation*}
        \begin{bmatrix}
            \Vec{w}
        \end{bmatrix}_B
        = C_{EB} \cdot
        \begin{bmatrix}
            \Vec{w}
        \end{bmatrix}_E
    \end{equation*}
\end{mdframed}

Las  matrices de cambio de base son útiles no solo para cambiar la base en la que están dadas las coordenadas de un vector, sino que también sirven para expresar sistemas matriciales en diferentes bases.
Esta aplicación es de gran utilidad para cambiar las bases de una transformación lineal, como se verá en el capítulo \ref{cha:TL}.
Definamos entonces una matriz de cambio de base que cambie la base de matrices (o eventualmente vectores) entre dos bases $B$ y $B'$ genéricas (pudiendo o no ser alguna de las bases la canónica).

Sabemos que una matriz de cambio de base $C_{BE}$ está dada por los elementos de $B$ puestos como columna.
Así mismo, una matriz de cambio de base $C_{EB'}$ está dada por la inversa de los elementos de $B'$ puestos como columna.
Estas matrices verifican, por definición, las siguientes ecuaciones respectivamente:
\begin{gather*}
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
    = C_{BE} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    \\[1ex]
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_{B'}
    = C_{EB'} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_E
\end{gather*}

Reemplazando la primera en la segunda se tiene:
\begin{align*}
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_{B'}
    &= C_{EB'} \cdot C_{BE} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
    \\[1ex]
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_{B'}
    &= C_{BB'} \cdot
    \begin{bmatrix}
        \Vec{w}
    \end{bmatrix}_B
\end{align*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:C_BB'}
    \end{defn}
    \cusTi{Matriz de cambio de base de $B$ a $B'$}
    \begin{equation*}
        C_{BB'} = C_{EB'} \cdot C_{BE}
    \end{equation*}
\end{mdframed}

Al ser una multiplicación matricial, el orden de los factores sí altera el producto.
Para $C_{BB'}$ la base de entrada es $B$ y la base de salida es $B'$.
Al desarrollar el producto, a la derecha de la igualdad, la matriz que primero actúa es $C_{BE}$ siendo $B$ su entrada y $E$ su salida.
Y luego actúa $C_{EB'}$ siendo $E$ su entrada (que es la salida de $C_{BE}$) y $B'$ su salida.

\begin{center}
    \def\svgwidth{0.4\linewidth}
    \input{./images/alg-rulo.pdf_tex}
\end{center}

Esto no solo es así en las matrices de cambio de base, sino en cualquier producto matricial donde haya bases involucradas.
La primer letra del subíndice de una matriz es la base de entrada de esa matriz, y la segunda la base de salida.
Y ya sea que se multipliquen dos o más matrices, se cumple que el orden está dado por \emph{el rulo} dibujado.


\section{Operaciones entre subespacios}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Intersección}
    \begin{equation*}
        A \cap B = \braces{\Vec{x} \in V \tq \Vec{x}\in A \land \Vec{x}\in B}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Unión}
    \begin{equation*}
        A \cup B = \braces{\Vec{x} \in V \tq \Vec{x}\in A \lor \Vec{x}\in B}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Suma}
    \cusTe{Dados dos subespacios vectoriales $S_1$ y $S_2$ la suma de estos es el conjunto dado por todas las posibles sumas de los elementos $\Vec{v} \in S_1$ y $\Vec{w}\in S_2$.}
    \begin{equation*}
        S_1 + S_2 = \braces{\Vec{x} \in V \tq \Vec{x}=\Vec{v}+\Vec{w}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Suma directa}
    \cusTe{Sean $B_1$ y $B_2$ bases de los subespacios $S_1$ y $S_2$ respectivamente.
    Si el conunto de los elementos de ambas bases $\braces{B_1 , B_2}$ es LI, se dice que $S_1$ y $S_2$ están en suma directa.}
    \begin{equation*}
        S_1 \oplus S_2 = \operatorname{gen} \braces{B_1 , B_2} \iff S_1 \cap S_2 = 0
    \end{equation*}
\end{mdframed}


\chapter{Transformaciones lineales}
\label{cha:TL}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:TL}
    \end{defn}
    \cusTi{Transformación lineal}
    \cusTe{Sean $\setV$ y $\setW$ dos espacios vectoriales, se dice que una función $f:\setV\longrightarrow\setW$ es una transformación lineal si para todo $\Vec{v}_1,\Vec{v}_2\in\setV$ y $\lambda\in\setR$ se verifica:}
    \begin{equation*}
        f(\lambda \, \Vec{v}_1 + \Vec{v}_2) = \lambda \, f(\Vec{v}_1) + f(\Vec{v}_2)
    \end{equation*}
\end{mdframed}

Observar que si la definición se cumple para todo $\lambda$ y para todo $\Vec{v}$ entonces se cumple particularmente para $\lambda=1$ y para $\Vec{v}_2=\Vec{0}$.
Es decir que si se cumple $f(\lambda \, \Vec{v}_1 + \Vec{v}_2)=\lambda \, f(\Vec{v}_1) + f(\Vec{v}_2)$ entonces se cumplen las siguientes condiciones simultáneamente:
\begin{equation*}
    \left\{
    \begin{aligned}
        f(\Vec{v}_1 + \Vec{v}_2) &= f(\Vec{v}_1) + f(\Vec{v}_2)
        \\
        f(\lambda \, \Vec{v}) &= \lambda \, f(\Vec{v})
    \end{aligned}
    \right.
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Condición necesaria}
    \cusTe{Si $f:\setV\longrightarrow\setW$ es una transformación lineal entonces:}
    \begin{equation*}
        f(\Vec{0}_\setV) = \Vec{0}_\setW
    \end{equation*}
\end{mdframed}

Una transformación lineal puede ser expresada por su fórmula:
\begin{equation*}
    f: \setV \longrightarrow \setW \tq f(\Vec{v}) = \Vec{w}
\end{equation*}

Puede ser expresada mediante \emph{la matriz de la transformación} 
\begin{equation*}
    f: \setV \longrightarrow \setW \tq M_{EE}(f) \cdot \Vec{v} = \Vec{w}
\end{equation*}

Pero además, puede ser expresada por los transformados de una base.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema fundamental de existencia y unicidad de las transformaciones lineales}
    \cusTe{Sean $\setV$ y $\setW$ dos espacios vectoriales, $B=\braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\nth}$ una base de $\setV$ y $\braces{\Vec{w}_1 , \Vec{w}_2 \dots \Vec{w}_\nth} \in \setW$ entonces $\exists ! \enspace f: \setV \longrightarrow \setW$ que es transformación lineal tal que:}
    \begin{gather*}
        f(\Vec{v}_1) = \Vec{w}_1
        \\
        f(\Vec{v}_2) = \Vec{w}_2
        \\
        \vdots
        \\
        f(\Vec{v}_\nth) = \Vec{w}_\nth
    \end{gather*}
\end{mdframed}

\concept{Demostración}

Para cierto $\Vec{v} \in \setV$, como $B$ es base de $\setV$, se tiene:
\begin{equation*}
    \exists ! \enspace \lambda_\ith \hspace{1ex} \textrm{con} \hspace{1ex} 1 \leq \ith \leq \nth \tq
    \Vec{v} = \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth
\end{equation*}

Entonces:
\begin{align*}
    f(\Vec{v}) &= f \left( \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth \right)
    \\
    &= \lambda_1 \, f(\Vec{v}_1) + \lambda_2 \, f(\Vec{v}_2) + \dots + \lambda_\nth \, f(\Vec{v}_\nth)
    \\
    &= \lambda_1 \, \Vec{w}_1 + \lambda_2 \, \Vec{w}_2 + \dots + \lambda_\nth \, \Vec{w}_\nth
    \\
    &=
    \begin{pmatrix}
        \trans{\Vec{w}_1} & \trans{\Vec{w}_2} & \dots & \trans{\Vec{w}_\nth}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \lambda_1
        \\
        \lambda_2
        \\
        \vdots
        \\
        \lambda_\nth
    \end{pmatrix}
    \\[1ex]
    &=
    \begin{pmatrix}
        w_{11} & w_{21} & \dots & w_{\nth 1}
        \\
        w_{12} & w_{22} & \dots & w_{\nth 2}
        \\
        \vdots & \vdots & \ddots & \vdots
        \\
        w_{1 \mth} & w_{2 \mth} & \dots & w_{\nth \mth}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \lambda_1
        \\
        \lambda_2
        \\
        \vdots
        \\
        \lambda_\nth
    \end{pmatrix}
    \\[1ex]
    &= M_{BE} (f) \cdot \trans{[\Vec{v}]_B}
\end{align*}

Donde $\nth=\operatorname{dim}(\setV)$ y $\mth=\operatorname{dim}(\setW)$.

La matriz $M_{BE}$ de la ecuación anterior tiene como columnas a las imágenes de los elementos de la base $B$.
Si esta base fuese la canónica, la matriz obtenida sería $M_{EE}(f)$ y estaría dada por las imágenes de los versores canónicos.

Pero en caso de no tener $M(f)$, sino $M_{BE}(f)$, el inconveniente que surge es que las variables que querramos evaluar en $f(\Vec{v})$ van a estar en base $E$ pero las variables de la fórmula en forma matricial en base $B$.
En tal caso, podemos usar las matrices de cambio de base (Def. \ref{defn:C_BE}) para obtener las coordenadas de $\Vec{v}$ en base canónica.
\begin{align*}
    f(\Vec{v}) &= M_{BE} (f) \cdot \trans{[\Vec{v}]_B}
    \\
    &= M_{BE} (f) \cdot C_{EB} \cdot \trans{[\Vec{v}]_E}
    \\
    &= M_{BE} (f) \cdot C_{EB} \cdot \trans{\Vec{v}}
    \\
    &= M(f) \cdot \trans{\Vec{v}}
\end{align*}

Donde:
\begin{align*}
    C_{EB} &=
    \begin{pmatrix}
        \trans{\Vec{v}_1} & \trans{\Vec{v}_2} & \dots & \trans{\Vec{v}_\nth}
    \end{pmatrix}^{-1}
    \\
    &=
    \begin{pmatrix}
        v_{11} & v_{21} & \dots & v_{\nth 1}
        \\
        v_{12} & v_{22} & \dots & v_{\nth 2}
        \\
        \vdots & \vdots & \ddots & \vdots
        \\
        v_{1 \nth} & v_{2 \nth} & \dots & v_{\nth \nth}
    \end{pmatrix}^{-1}
\end{align*}

Observar que los $\Vec{w}$ anteriores están en base canónica, pero podrían estarlo en otra base.
Podemos definir una transformación matricialmente en bases $A$ y $B$ genéricas de la siguiente forma.

Notar que en tal caso $A$ cumple el papel que antes cumplía $B$, ya que $B$ cumplirá el papel que antes cumplía $E$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Forma matricial de una TL en bases $AB$}
    \cusTe{Sean $A=\braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\nth}$ una base de $\setV$ y $B=\braces{\Vec{w}_1 , \Vec{w}_2 \dots \Vec{w}_\mth}$ una base de $\setW$ se define la transformación lineal}
    \begin{equation*}
        f: \setV \longrightarrow \setW \tq M_{AB}(f) \cdot \sqb{\Vec{v}}_A = \sqb{\Vec{w}}_B
    \end{equation*}
    \noTi{Donde:}
    \begin{equation*}
        M_{AB} (f) =
        \begin{pmatrix}
            \trans{\left[f(\Vec{v}_1)\right]_{B}} &
            \trans{\left[f(\Vec{v}_2)\right]_{B}} &
            \dots &
            \trans{\left[f(\Vec{v}_\nth)\right]_{B}}
        \end{pmatrix}
    \end{equation*}
\end{mdframed}

Observar que la matriz $M_{AB}$ tiene $\nth$ columnas según la dimensión de $\setV$ y $\mth$ filas según la dimensión de $\setW$.

Podemos usar las matrices de cambio de base (Def. \ref{defn:C_BB'}) para expresar $M_{AB}$ en bases $A'$ y $B'$.
\begin{align*}
    M_{A'B'}(f) \cdot \sqb{\Vec{v}}_{A'} &= \sqb{\Vec{w}}_{B'}
    \\
    C_{BB'} \cdot M_{AB}(f) \cdot C_{A'A} \cdot \sqb{\Vec{v}}_{A'} &= \sqb{\Vec{w}}_{B'}
\end{align*}

De manera que:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        M_{A'B'}(f) = C_{BB'} \cdot M_{AB}(f) \cdot C_{A'A}
    \end{equation*}
\end{mdframed}

O análogamente:
\begin{align*}
    M_{AB}(f) \cdot \sqb{\Vec{v}}_A &= \sqb{\Vec{w}}_B
    \\
    C_{B'B} \cdot M_{A'B'}(f) \cdot C_{AA'} \cdot \sqb{\Vec{v}}_A &= \sqb{\Vec{w}}_B
\end{align*}

Obteniendo:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:M_BB}
    \end{prop}
    \begin{equation*}
        M_{AB}(f) = C_{B'B} \cdot M_{A'B'}(f) \cdot C_{AA'}
    \end{equation*}
\end{mdframed}


\section{Imagen y núcleo}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Imagen}
    \cusTe{Es el conjunto de vectores que resultan de aplicar la transformación a todos los elementos del dominio.}
    \begin{equation*}
        \im(f) = \braces{\Vec{w} \in \setW \tq \exists \, \Vec{v} \in \setV \enspace \textrm{con} \enspace f(\Vec{v}) = \Vec{w}}
    \end{equation*}
\end{mdframed}

Para calcular la imagen se aplica la transformación a cada elemento de una base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ de $\setV$ y así otener un conjunto generador de la imagen.
Observar que para cualquier elemento $\Vec{v}\in\setV$ se verifica:
\begin{gather*}
    f(\Vec{v}) = f(\lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth)
    \\
    f(\Vec{v}) = \lambda_1 \, f(\Vec{v}_1) + \lambda_2 \, f(\Vec{v}_2) + \dots + \lambda_\nth \, f(\Vec{v}_\nth)
    \\
    \im(f) = \operatorname{gen}\braces{f(\Vec{v}_1),f(\Vec{v}_2) \dots f(\Vec{v}_\nth)}
\end{gather*}

\begin{mdframed}[style=MyFrame2]
    \begin{example}
    \end{example}
    Sea $f:\operatorname{dn}(f) \subseteq \setR^3 \longrightarrow \im(f) \subseteq \setR^2$ una T.L. tal que $f(x,y,z)=(x+y,y-z)$, se pide calcular la imagen.
    
    Aplicando la transformación a los elementos de la base canónica se tiene:
    \begin{align*}
        f(\eVer_1) &= (1,0)
        \\
        f(\eVer_2) &= (1,1)
        \\
        f(\eVer_3) &= (0,-1)
    \end{align*}
    
    Por lo tanto el conjunto generador de la imagen de $f$ es:
    \begin{equation*}
        \im(f) = \operatorname{gen}\braces{(1,0),(1,1),(0,-1)}
    \end{equation*}
    
    O bien, descartando $(0,-1)=(1,0)-(1,1)$ por ser combinación lineal se tiene:
    \begin{equation*}
        \im(f) = \operatorname{gen}\braces{(1,0),(1,1)} = \setR^2
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Núcleo}
    \cusTe{Es el conjunto de vectores que tienen como imagen el origen.}
    \begin{equation*}
        \Nu(f) = \braces{\Vec{v} \in \setV \tq f(\Vec{v})=\Vec{0}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de la dimensión}
    \cusTe{Sea $f:\setV\longrightarrow\setW$ una transformación lineal tal que la dimensión de $\setV$ es finita entonces:}
    \begin{equation*}
        \operatorname{dim}(\setV) = \operatorname{dim} \big(\Nu(f)\big) + \operatorname{dim} \big(\im(f)\big)
    \end{equation*}
\end{mdframed}


\section{Clasificación de T.L.}

Si $f: \setV \longrightarrow \setW$ es una T.L. basta con que el núcleo esté compuesto solamente por el vector origen para que $f$ sea inyectiva.
Esto implica que no existe T.L. cuyo núcleo sea el origen, y además tenga una misma imagen para dos elementos de $\setV$ distintos.
Esto es:
\begin{gather*}
    \textrm{Sea} \enspace f: \setV \longrightarrow \setW \enspace \textrm{una T.L.}
    \\
    \textrm{Sean} \enspace \Vec{v}_1,\Vec{v}_2 \in \setV \tq \Vec{v}_1 \neq \Vec{v}_2 \land f(\Vec{v}_1) = f(\Vec{v}_2) \neq \Vec{0}
    \\
    \textrm{Observar que} \enspace \Vec{v}_1,\Vec{v}_2 \notin \Nu(f)
    \\
    f(\Vec{v}_1) - f(\Vec{v}_2) = \Vec{0}
    \\
    f(\Vec{v}_1 - \Vec{v}_2) = \Vec{0}
    \\
    \therefore \left( \Vec{v}_1 - \Vec{v}_2 \right) \in \Nu(f) \neq \Vec{0}
\end{gather*}


\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Monomorfismo}
    \begin{multline*}
        f \enspace \textrm{es un monomorfismo} \iff
        \\
        \iff f  \enspace \textrm{es inyectiva}
        \iff \Nu(f) - \Vec{0} = \setO
    \end{multline*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Epimorfismo}
    \begin{multline*}
        f \enspace \textrm{es un epimorfismo} \iff
        \\
        \iff f  \enspace \textrm{es sobreyectiva}
        \iff \im(f) = \setW
    \end{multline*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Isomorfismo}
    \begin{equation*}
        f \enspace \textrm{es un isomorfismo} \iff f \enspace \textrm{es biyectiva}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Endomorfismo}
    \begin{equation*}
        f \enspace \textrm{es un endomorfismo} \iff \setV = \setW
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Automorfismo}
    \begin{multline*}
        f \enspace \textrm{es un automorfismo} \iff
        \\
        \iff f \enspace \textrm{es isomorfismo y endomorfismo}
    \end{multline*}
\end{mdframed}


\chapter{Autovalores}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:autovalor}
    \end{defn}
    \cusTi{Autovalor}
    \cusTe{Sea $A \in \setR^{\nth\times\nth}$ una matriz y $\lambda \in \setR$ un escalar.
    Se dice que $\lambda$ es un autovalor de $A$ si $\exists \enspace \Vec{v} \in \setR^\nth \tq \Vec{v} \neq \Vec{0}$} verificando:
    \begin{equation*}
        A \, \Vec{v} = \lambda \, \Vec{v}
    \end{equation*}
\end{mdframed}

Haciendo algunos movimientos algebráicos:
\begin{equation*}
    A \, \Vec{v} - \lambda \, \Vec{v} = 0
\end{equation*}

Queda un sistema lineal homogéneo que siempre tiene (al menos una) solución.
\begin{equation}
    \left( A - \lambda \, I \right) \Vec{v} = 0
    \label{eqn:autovectores}
\end{equation}

Si el determinante de la matriz asociada al sistema fuese distinto de cero entonces la única solución sería la trivial.
Pero si $\Vec{v}=\Vec{0}$ es la única solución se estaría contradiciendo a la definición de autovalor (Def. \ref{defn:autovalor}) que pide un vector no nulo.
Además, no sería útil como aplicación elevar una matriz a cierta potencia.

Para que el sistema tenga infinitas soluciones, además de la trivial, se tiene que cumplir:
\begin{equation*}
    \operatorname{det} \left( A - \lambda \, I \right) = 0
\end{equation*}

Al desarrollar se obtiene un \emph{polinomio característico} de la matriz cuyas raíces son los \emph{autovalores}.

Reemplazando un autovalor $\lambda$ en la ecuación \ref{eqn:autovectores} se obtiene un subespacio de autovectores $\Vec{v}$ que la verifican.
El subespacio generado por un autovector es llamado \emph{autoespacio} asociado al autovalor evaluado.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Autoespacio de autovectores asociados a un autovalor}
    \begin{equation*}
        E_{\lambda_\ith} = \operatorname{gen} \braces{\Vec{v}} \tq \Vec{v} \in \Nu \braces{A - \lambda_\ith \, I}
    \end{equation*}
\end{mdframed}


\section{Diagonalización}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Matrices semejantes}
    \cusTe{Dos matrices $A, B \in \setR^{\nth\times\nth}$ son semejantes si existe una matriz $C \in \setR^{\nth\times\nth}$ tal que:}
    \begin{equation*}
        A = C^{-1} \cdot B \cdot C
    \end{equation*}
\end{mdframed}

Un ejemplo de matrices semejantes son las matrices $M_{AB}$ y $M_{A'B'}$ de la propiedad \ref{prop:M_BB} cuando $A'=B'=E$ y $A=B$ ya que en tal caso para $C_{EB}=C_{BE}^{-1}$ se tiene:
\begin{equation*}
    M_{BB} = C_{EB} \cdot M_{EE} \cdot C_{BE}
\end{equation*}

Una característica importante de las matrices semejantes es que tienen el mismo polinomio característico.

Una matriz se dice diagonalizable si existe una matriz semejante que sea diagonal.
Notar que dos matrices que no sean cuadradas no van a poder verificar la condición de semejanza, porque en tal caso no es posible definir $C$ y su inversa.

Sea $D$ una matriz diagonal, si $A$ es diagonalizable se tiene:
\begin{equation*}
    A = C^{-1} \cdot D \cdot C
\end{equation*}

En tal caso, las potencias donde se eleva una matriz diagonalizable $A$ a un exponente $\kth$ pueden ser computadas más eficientemente.
A saber:
\begin{align*}
    A^2 &= \left( C^{-1} \cdot D \cdot C \right) \left( C^{-1} \cdot D \cdot C \right)
    \\
    &= C^{-1} \cdot D \cdot I \cdot D \cdot C
    \\
    &= C^{-1} \cdot D^2 \cdot C
    \\[1ex]
    A^3 &= \left( C^{-1} \cdot D^2 \cdot C \right) \left( C^{-1} \cdot D \cdot C \right)
    \\
    &= C^{-1} \cdot D^2 \cdot I \cdot D \cdot C
    \\
    &= C^{-1} \cdot D^3 \cdot C
\end{align*}

Y por extrapolación se infiere:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si $A$ es diagonalizable, entonces es semejante a una matriz diagonal $D$ y se verifica:
    \begin{equation*}
        A^\kth = C^{-1} \cdot D^\kth \cdot C
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame2]
    \begin{example}
    \end{example}
    Determinar si la matriz $M_{EE}(f)$ de la siguiente transformación lineal es diagonalizable.
    \begin{equation*}
        f:\setR^2\longrightarrow\setR^2\tq f(x,y)=(2x-y , y)
    \end{equation*}
    
    Expresando $f(x,y)=(2x-y , y)$ de forma matricial se tiene:
    \begin{gather*}
        M_{EE}(f) \cdot
        \begin{pmatrix}
            x
            \\
            y
        \end{pmatrix}
        =
        \begin{pmatrix}
            2x-y
            \\
            y
        \end{pmatrix}
        =
        \begin{pmatrix}
            2 & -1
            \\
            0 & 1
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            x
            \\
            y
        \end{pmatrix}
        \\[1ex]
        M_{EE}(f) =
        \begin{pmatrix}
            2 & -1
            \\
            0 & 1
        \end{pmatrix}
    \end{gather*}
    
    Si $M_{EE}(f)$ es diagonalizable, entonces:
    \begin{equation*}
        M_{EE}(f) = C_{BE} \cdot M_{BB}(f) \cdot C_{EB}
    \end{equation*}
    
    Tal que:
    \begin{equation*}
        M_{BB}(f) =
        \begin{pmatrix}
            a_{11} & 0
            \\
            0 & a_{22}
        \end{pmatrix}
    \end{equation*}
    
    Y además siendo las columnas de $M_{BB}(f)$ las coordenadas en base $B$ de los transformados de la base $B$.
    Esto es:
    \begin{equation*}
        M_{BB}(f) =
        \begin{pmatrix}
            \trans{\sqb{f(\Vec{v}_1)}_B} & \trans{\sqb{f(\Vec{v}_2)}_B}
        \end{pmatrix}
    \end{equation*}
    
    Entonces, por definición de coordenadas:
    \begin{equation*}
        \left\{
        \begin{aligned}
            \sqb{f(\Vec{v}_1)}_B &= (a_{11} , 0)
            \\[1ex]
            \sqb{f(\Vec{v}_2)}_B &= (0 , a_{22})
        \end{aligned}
        \right.
    \end{equation*}
    
    Pués:
    \begin{equation*}
        \left\{
        \begin{aligned}
            f(\Vec{v}_1) &= a_{11} \, \Vec{v}_1 + 0 \, \Vec{v}_2 = a_{11} \, \Vec{v}_1
            \\
            f(\Vec{v}_2) &= 0 \, \Vec{v}_1 + a_{22} \, \Vec{v}_2 = a_{22} \, \Vec{v}_2
        \end{aligned}
        \right.
    \end{equation*}
    
    Lo cual significa que $a_{11}$ y $a_{22}$ son los autovalores de $M_{EE}(f)$, de ahora en más llamados:
    \begin{equation*}
        \left\{
        \begin{aligned}
        \lambda_1 &= a_{11}
        \\
        \lambda_2 &= a_{22}
        \end{aligned}
        \right.
    \end{equation*}
    
    Para calcularlos:
    \begin{gather*}
        \operatorname{det} \left( M_{EE}(f) - \lambda \, I \right) = 0
        \\[1ex]
        \operatorname{det}
        \begin{pmatrix}
            2-\lambda & -1
            \\
            0 & 1-\lambda
        \end{pmatrix}
        = 0
        \\[1ex]
        \left( 2-\lambda \right) \left( 1-\lambda \right) = 0
        \\[1ex]
        \left\{
        \begin{aligned}
        \lambda_1 &= 2
        \\
        \lambda_2 &= 1
        \end{aligned}
        \right.
    \end{gather*}
    
    Los autovectores son aquellos $\Vec{v}$ que verifican:
    \begin{equation*}
        \left( M_{EE}(f) - \lambda \, I \right) \Vec{v} = 0
    \end{equation*}
    
    Para $\lambda_1=2$ se tiene:
    \begin{equation}
        \begin{pmatrix}
            0 & -1
            \\
            0 & -1
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            x
            \\
            y
        \end{pmatrix}
        =
        \begin{pmatrix}
            0
            \\
            0
        \end{pmatrix}
        \Rightarrow y=0
    \end{equation}
    
    Para $\lambda_1=2$ se tiene:
    \begin{equation}
        \begin{pmatrix}
            1 & -1
            \\
            0 & 0
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            x
            \\
            y
        \end{pmatrix}
        =
        \begin{pmatrix}
            0
            \\
            0
        \end{pmatrix}
        \Rightarrow y=x
    \end{equation}
    
    De manera que la bases de autovectores es:
    \begin{equation*}
        B = \braces{\Vec{v}_1 , \Vec{v}_2} = \braces{(1,0),(1,1)}
    \end{equation*}
    
    Verificándose:
    \begin{equation*}
        \left\{
        \begin{aligned}
            f(\Vec{v}_1) = 2 \, \Vec{v}_1
            \\
            f(\Vec{v}_2) = 1 \, \Vec{v}_2
        \end{aligned}
        \right.
    \end{equation*}
    
    De manera que las matrices semejantes verifican:
    \begin{gather*}
        M_{EE}(f) = C_{BE} \cdot M_{BB}(f) \cdot C_{EB}
        \\
        \begin{pmatrix}
            2 & -1
            \\
            0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 1
            \\
            0 & 1
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            2 & 0
            \\
            0 & 1
        \end{pmatrix}
        \cdot
        \begin{pmatrix}
            1 & 1
            \\
            0 & 1
        \end{pmatrix}^{-1}
    \end{gather*}
\end{mdframed}


\section{Multiplicidad}

Se llama \emph{multiplicidad algebráica} al grado de un autovalor como raíz del polinomio característico.
Mientras que se llama \emph{multiplicidad geométrica} a la dimensión del autoespacio asociado a un autovalor.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \operatorname{MultG} (\lambda_0) \leq \operatorname{MultA} (\lambda_0)
    \end{equation*}
\end{mdframed}

Si $\Vec{v} \neq \Vec{0}$ pertenece a $E_{\lambda_1} \cap E_{\lambda_1}$ sería un autovalor de ambos autovectores.
Esto es:
\begin{equation*}
    \left\{
    \begin{aligned}
        f(\Vec{v}) &= \lambda_1 \Vec{v}
        \\
        f(\Vec{v}) &= \lambda_2 \Vec{v}
    \end{aligned}
    \right.
    \Rightarrow \Vec{0} = f(\Vec{v}) - f(\Vec{v}) = \left( \lambda_1-\lambda_2 \right) \Vec{v}
\end{equation*}

Pero dado que $\Vec{v}\neq\Vec{0}$ se tiene que $\lambda_1=\lambda_2$ lo cual es absurdo.
Por lo tanto, se tiene la siguiente propiedad que se puede demostrar para una cantidad arbitraria de autoespacios.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        E_{\lambda_1} \cap E_{\lambda_1} = \Vec{0}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Sea $\lambda$ un autovalor de la matriz $A\in\setR^{\nth\times\nth}$ entonces $A$ es diagonalizable si y solo si:
    \begin{gather*}
        \operatorname{MultG} (\lambda_\ith) = \operatorname{MultA} (\lambda_\ith) \quad 1\leq\ith\leq\nth
        \\
        \iff
        \\
        \operatorname{dim}(\setV) = \operatorname{dim}(E_{\lambda_1} \oplus E_{\lambda_2} \dots E_{\lambda_\nth})
    \end{gather*}
\end{mdframed}


\chapter{Espacios euclídeos}

Los \emph{espacios euclídeos} son espacios vectoriales dotados de un producto interno.
Se diferencian de los \emph{espacios normados} para los que se define distancia y norma y los \emph{espacios métricos} para los que solo se define distancia, por no tener estos producto interno.

El producto interno para espacios euclídeos permite definir una noción de norma, de ángulo u ortogonalidad y de proyección ortogonal para elementos de un subespacio.


\section{Producto interno}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:intProd}
    \end{defn}
    \cusTi{Producto interno}
    \cusTe{Sea $\setV$ un subespacio vectorial, se define la función $\intProd{\enspace}{\enspace}: \setV \times \setV \longrightarrow \setR$ como un producto interno si y solo si es una forma bilineal, simétrica y definida positiva.
    Esto es, si para todo $\Vec{u},\Vec{v},\Vec{w} \in \setV , \lambda \in \setR$ se verifica:}
    \begin{align*}
        \textrm{Bilinealidad:} &
        \\
        \intProd{\Vec{u}+\Vec{v}}{\Vec{w}} &= \intProd{\Vec{u}}{\Vec{w}} + \intProd{\Vec{v}}{\Vec{w}}
        \\
        \intProd{\lambda \, \Vec{v}}{\Vec{w}} &= \lambda \intProd{\Vec{v}}{\Vec{w}}
        \\
        \textrm{Simetría:} &
        \\
        \intProd{\Vec{v}}{\Vec{w}} &= \intProd{\Vec{w}}{\Vec{v}}
        \\
        \textrm{Definido positivo:} &
        \\
        \intProd{\Vec{v}}{\Vec{v}} &\geq 0
    \end{align*}
\end{mdframed}

Sea $A\in\setR^{\nth\times\nth}$ una matriz cuadrada y simétrica:
\begin{equation*}
    A =
    \begin{pmatrix}
        a_{11} & a_{12} & \dots & a_{1\nth}
        \\
        a_{21} & a_{22} & \dots & a_{2\nth}
        \\
        \vdots & \vdots & \ddots & \vdots
        \\
        a_{\nth 1} & a_{\nth 2} & \dots & a_{\nth \nth}
    \end{pmatrix}
\end{equation*}

Y definida positiva, con los siguientes $\nth$ determinantes positivos:
\begin{gather*}
    \operatorname{det}(a_{11})>0
    \\
    \operatorname{det}
    \begin{pmatrix}
        a_{11} & a_{12}
        \\
        a_{21} & a_{22}
    \end{pmatrix}
    >0
    \\
    \operatorname{det}
    \begin{pmatrix}
        a_{11} & a_{12} & a_{13}
        \\
        a_{21} & a_{22} & a_{23}
        \\
        a_{31} & a_{32} & a_{33}
    \end{pmatrix}
    >0
    \\
    \vdots
    \\
    \operatorname{det}(A)>0
\end{gather*}

De manera general, el producto interno para $\setV = \setR^\nth$ puede estar definido por el siguiente producto matricial.
\begin{equation*}
    \intProd{\Vec{v}}{\Vec{w}} = \Vec{v} \cdot A \cdot \trans{\Vec{w}}
\end{equation*}

De manera que si $A=I$ se obtiene el producto escalar entre vectores:
\begin{align*}
    \intProd{\Vec{v}}{\Vec{w}} &= \Vec{v} \cdot \trans{\Vec{w}}
    \\
    &= \sum_{\ith=1}^\nth v_\ith \, w_\ith
\end{align*}

Que para $\setV = \setR^2$ es:
\begin{align*}
    \intProd{\Vec{v}}{\Vec{w}} &= (v_x,v_y) \cdot (w_x,w_y)
    \\
    &= v_x \, w_x + v_y \, w_y
\end{align*}

Pero para ciertos espacios vectoriales los productos internos pueden ser más abstractos.
Para el espacio vectorial de funciones contínuas el producto interno puede estar definido por una integral.
\begin{equation*}
    \intProd{f}{g} = \int_a^b f(x) \, g(x) \, \dif x
\end{equation*}

Como se mencionó anteriormente, los conceptos de norma, distancia, ángulo y ortogonalidad se definen en función del producto interno.
Esto significa que la norma de un mismo vector puede tener distintos valores para productos internos diferentes.
Y así mismo, la distancia o el ángulo entre dos vectores puede variar en función de cómo se defina el producto interno.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Norma}
    \begin{equation*}
        \nnorm{\Vec{v}} = \sqrt{\intProd{\Vec{v}}{\Vec{v}}}
    \end{equation*}
\end{mdframed}

Observar que para $\setV = \setR^\nth$, si el producto interno es el producto escalar, la fórmula es la definida según la norma 2 (Def. \ref{defn:norm2}).
Particularmente para $\setR^2$ es:
\begin{equation*}
    \nnorm{\Vec{v}} = \sqrt{(x,y) \cdot (x,y)} = \sqrt{x^2+y^2}
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Distancia}
    \begin{equation*}
        \operatorname{dist}(\Vec{v},\Vec{w}) = \nnorm{\Vec{v}-\Vec{w}} = \sqrt{\intProd{\Vec{v}-\Vec{w}}{\Vec{v}-\Vec{w}}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El ángulo $\theta$ que forman dos vectores $\Vec{v}$ y $\Vec{w}$ es:
    \begin{equation*}
        \cos(\theta) = \frac{\intProd{\Vec{v}}{\Vec{w}}}{\nnorm{v} \, \nnorm{w}}
    \end{equation*}
\end{mdframed}

A partir de esta propiedad vemos que si dos vectores forman un ángulo de $\theta=\ang{90}$, la expresión queda igualada a cero y se verifica cuando el numerador es nulo.

Surgen luego las siguientes definiciones:

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Ortogonalidad}
    \cusTe{Dos vectores $\Vec{v}$ y $\Vec{w}$ son ortogonales si}
    \begin{equation*}
        \intProd{\Vec{v}}{\Vec{w}} = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Base ortogonal}
    \cusTe{Una base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ es ortogonal si:}
    \begin{equation*}
        \intProd{\Vec{v}_\ith}{\Vec{v}_\jth} = 0 \quad \forall \enspace  \ith\neq\jth \enspace \textrm{con} \enspace \ith,\jth \in [1,\nth]
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Base ortonormal}
    \cusTe{Una base $B=\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ es ortonormal si es ortogonal y además:}
    \begin{equation*}
        \nnorm{\Vec{v}_\ith} = 1 \quad \forall \enspace  \ith \in [1,\nth]
    \end{equation*}
\end{mdframed}

La proyección de un vector $\Vec{v}$ sobre un director $\Vec{r}$ es un tercer vector proporcional a $\Vec{r}$.
En $\setR^2$ y $\setR^3$ la proporción surge de \emph{proyectar}, desde la punta de $\Vec{v}$, una recta que sea ortogonal a $\Vec{r}$.
A contnuación se tiene un esquema en $\setR^2$.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/alg-orto-1.pdf_tex}
\end{center}

Se plantea entonces la condición de proporción mencionada y se pretende deducir cuál es el factor de proporcionalidad $\lambda$.
\begin{equation}
    \proy_{\Vec{r}}(\Vec{v}) = \lambda \, \Vec{r}
    \label{eqn:orto1}
\end{equation}

Vemos que el vector que va desde la proyección $\proy_{\Vec{r}}(\Vec{v})$ hasta $\Vec{v}$ está dado por la resta de estos y es ortogonal a $\Vec{r}$.
Con lo cual:
\begin{equation}
    \intProd{\Vec{v} - \proy_{\Vec{r}}(\Vec{v})}{\Vec{r}} = 0
    \label{eqn:orto2}
\end{equation}

Reemplazando la ecuación \ref{eqn:orto1} en la ecuación \ref{eqn:orto2} y aplicando las propiedades del producto interno (Def. \ref{defn:intProd}), se despeja $\lambda$.
\begin{align*}
    \intProd{\Vec{v} - \lambda \, \Vec{r}}{\Vec{r}} &= 0
    \\
    \intProd{\Vec{v}}{\Vec{r}} - \lambda \intProd{\Vec{r}}{\Vec{r}} &=
    \\
    \intProd{\Vec{v}}{\Vec{r}} - \lambda \, \nnorm{\Vec{r}}^2 &=
    \\
    \lambda &= \frac{\intProd{\Vec{v}}{\Vec{r}}}{\nnorm{\Vec{r}}^2}
\end{align*}

Por lo tanto, reemplazando $\lambda$ en la ecuacíon \ref{eqn:orto1} se infiere la relación entre $\proy_{\Vec{r}}(\Vec{v})$ y $\Vec{r}$ obteniendo una fórmula para la proyección ortogonal entre 2 vectores en el plano o el espacio.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:proyOrto}
    \end{prop}
    \cusTi{Proyección ortogonal}
    \cusTe{La proyección ortogonal de $\Vec{v}$ sobre $\Vec{r}$ está dada por:}
    \begin{equation*}
        \proy_{\Vec{r}}(\Vec{v}) = \frac{\intProd{\Vec{v}}{\Vec{r}}}{\nnorm{\Vec{r}}^2} \, \Vec{r}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Desigualdad de Cauchy-Schwartz}
    \begin{equation*}
        \intProd{\Vec{v}}{\Vec{w}} \leq \nnorm{\Vec{v}} \, \nnorm{\Vec{w}}
    \end{equation*}
\end{mdframed}


\section{Ortogonalización de Gram Schmidt}

El método de Gram Schmidt es un algoritmo recursivo para obtener, a partir de cierta base dada, una base ortogonal que genere el mismo subespacio que la base original.

Sea $B=\braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\nth}$ una base de cierto subespacio $\setV$.
Llamaremos $B'$ a la nueva base ortogonal de $\setV$ dada por:
\begin{equation*}
    B' = \braces{\Vec{w}_1 , \Vec{w}_2 \dots \Vec{w}_\nth}
\end{equation*}

El primer elemento de $B'$ lo podemos definir de manera arbitraria a partir de cualquier elemento de $B$.
\begin{equation*}
    \Vec{w}_1 = \Vec{v}_1
\end{equation*}

El segundo elemento de $B'$ se define de manera análoga a como se dedujo la proyección ortogonal anteriormente (Prop. \ref{prop:proyOrto}).
A continuación se tiene un esquema equivalente, ya que el proyectado y el director son elementos de la base $B$ por lo que cambia la notación.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/alg-orto-2.pdf_tex}
\end{center}

Vemos que podemos construir un vector ortogonal a $\Vec{v}_1$ restando al proyectado la proyección ortogonal.
Este será el segundo elemento de la base ortogonal $B'$.
\begin{align*}
    \Vec{w}_2 &= \Vec{v}_2 - \proy_{\Vec{v}_1}(\Vec{v}_2)
    \\[1ex]
    &= \Vec{v}_2 - \frac{\intProd{\Vec{v}_2}{\Vec{v}_1}}{\nnorm{\Vec{v}_1}^2} \, \Vec{v}_1
\end{align*}

Si la base original $B$ fuese de 3 o más elementos, el tercer elemento de la nueva base ortogonal $B'$ se obtendría con el mismo razonamiento.
La siguiente es una representación simplista de los vectores de $B$ y sus proyecciones, a partir de las que se obtienen los vectores de $B'$.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/alg-orto-3.pdf_tex}
\end{center}

Quedando el tercer elemento $\Vec{w}_3$ expresado por:
\begin{align*}
    \Vec{w}_3 &= \Vec{v}_3 - \proy_{\Vec{v}_1}(\Vec{v}_3) - \proy_{\Vec{w}_2}(\Vec{v}_3)
    \\[1ex]
    &= \Vec{v}_3 - \frac{\intProd{\Vec{v}_3}{\Vec{v}_1}}{\nnorm{\Vec{v}_1}^2} \, \Vec{v}_1 - \frac{\intProd{\Vec{v}_3}{\Vec{w}_2}}{\nnorm{\Vec{w}_2}^2} \, \Vec{w}_2
\end{align*}

A primera vista uno puede suponer que el término $-\proy_{\Vec{w}_2}(\Vec{v}_3)$ está de más, pero no es así.
En la representación anterior la proyección de $\Vec{v}_3$ sobre $\Vec{w}_2$ es nula, y por eso este término no afecta la ecuación.
Un esquema más representativo sería el siguiente, donde $\Vec{v}_3$ no está contenido en el plano gris y aún así forma parte de una base de $\setR^3$ junto con $\Vec{v}_1$ y $\Vec{v}_2$.

\begin{center}
    \def\svgwidth{0.7\linewidth}
    \input{./images/alg-orto-4.pdf_tex}
\end{center}

De esta forma podemos ver que la resta $\Vec{v}_3-\proy_{\Vec{v}_1}(\Vec{v}_3)$ sobresale del plano gris.
Por este motivo, es necesario restar la componente vertical, que es $\proy_{\Vec{w}_2}(\Vec{v}_3)$.


\section{Complemento ortogonal}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:compOrto}
    \end{defn}
    \cusTi{Complemento ortogonal}
    \begin{equation*}
        S^\perp = \braces{\Vec{w} \in \setV \tq \intProd{\Vec{v}}{\Vec{w}} = 0 \quad \forall \enspace \Vec{v} \in S}
    \end{equation*}
\end{mdframed}

Dado $S=\operatorname{gen}\braces{\Vec{v}_1,\Vec{v}_2 \dots \Vec{v}_\nth}$ se parte de la definición \ref{defn:compOrto} para calcular el complemento ortogonal por propiedades de producto interno (Def. \ref{defn:intProd}).
\begin{gather*}
    \intProd{\Vec{v}}{\Vec{w}} = 0
    \\
    \intProd{\lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth}{\Vec{w}} = 0
    \\
    \lambda_1 \intProd{\Vec{w}}{\Vec{v}_1} + \lambda_2 \intProd{\Vec{w}}{\Vec{v}_2} + \dots + \lambda_\nth \intProd{\Vec{w}}{\Vec{v}_\nth} = 0
\end{gather*}

Ya que esto se va a dar si y solo si:
\begin{equation*}
    \left\{
    \begin{aligned}
        \intProd{\Vec{w}}{\Vec{v}_1} &= 0
        \\
        \intProd{\Vec{w}}{\Vec{v}_2} &= 0
        \\
        \vdots \enspace &
        \\
        \intProd{\Vec{w}}{\Vec{v}_\nth} &= 0
    \end{aligned}
    \right.
\end{equation*}

De manera que se pueden despejar los generadores de $S^\perp$ del sistema anterior.

Una propiedad importante es que el complemento ortogonal de $S$ no comparte ningún elemento con $S$ más allá del origen.
\begin{gather*}
    \textrm{Si} \enspace \Vec{v} \neq \Vec{0} \enspace \land \enspace \Vec{v} \in S \enspace \land \enspace \Vec{v} \in S^\perp \Rightarrow
    \\
    \intProd{\Vec{v}}{\Vec{v}} = 0 \iff \nnorm{\Vec{v}}^2 = 0 \iff \Vec{v} = \Vec{0}
\end{gather*}

Con lo cual inferimos por absurdo:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Dado $S \in \setV$ y su complemento ortogonal $S^\perp \in \setV$ se tiene que:
    \begin{equation*}
        S \cap S^\perp = \Vec{0}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:comp2}
    \end{prop}
    Dado $S \in \setV$ y su complemento ortogonal $S^\perp \in \setV$ se tiene que:
    \begin{equation*}
        S \oplus S^\perp = \setV
    \end{equation*}
\end{mdframed}

Si se tiene una base ortogonal $B^\perp = \braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\nth}$ de un subespacio $\setV$, es posible calcular las coordenadas de un vector en dicha base $\sqb{\Vec{v}}_{B^\perp}=(\lambda_1 , \lambda_2 \dots \lambda_\nth)$ a partir de una aplicación del producto interno.

Escribiendo $\Vec{v}$ como combinación lineal de la base $B^\perp$:
\begin{equation*}
    \Vec{v} = \lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth
\end{equation*}

De manera que al hacer el producto interno entre $\Vec{v}$ y cualquiera de los elementos de la base $B^\perp$ se tiene, por ejemplo para el primer $\Vec{v}_\ith$ de la base:
\begin{align*}
    \intProd{\Vec{v}}{\Vec{v}_1} &= \intProd{\lambda_1 \, \Vec{v}_1 + \lambda_2 \, \Vec{v}_2 + \dots + \lambda_\nth \, \Vec{v}_\nth}{\Vec{v}_1}
    \\
    = \lambda_1 \intProd{\Vec{v}_1}{\Vec{v}_1} &+ \lambda_2 \intProd{\Vec{v}_2}{\Vec{v}_1} + \dots + \lambda_\nth \intProd{\Vec{v}_\nth}{\Vec{v}_1}
    \\
    &= \lambda_1 \intProd{\Vec{v}_1}{\Vec{v}_1}
    \\
    &= \lambda_1 \, \nnorm{\Vec{v}_1}^2
    \\
    \frac{\intProd{\Vec{v}}{\Vec{v}_1}}{\nnorm{\Vec{v}_1}^2}&= \lambda_1
\end{align*}

Y así para cualquier coordenada.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:orto3}
    \end{prop}
    \cusTi{Coordenadas en una base ortogonal}
    \cusTe{Sea $B^\perp = \braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\nth}$ una base ortogonal de $\setV$, dado $\Vec{v}\in\setV$ en base canónica se pueden calcular las coordenadas $\sqb{\Vec{v}}_{B^\perp}=(\lambda_1 , \lambda_2 \dots \lambda_\nth)$ según:}
    \begin{equation*}
        \lambda_\ith = \frac{\intProd{\Vec{v}}{\Vec{v}_\ith}}{\nnorm{\Vec{v}_\ith}^2}
    \end{equation*}
    \noTi{Con $\Vec{v}_\ith \in B^\perp$ para todo $1 \leq \ith \leq \nth$}
\end{mdframed}


\section{Mínima distancia}

Sea $(\setV, \intProd{\,}{\,})$ un espacio euclídeo, $S\in\setV$ un subespacio y $S^\perp\in\setV$ su complemento ortogonal.

Dadas $B=\braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\kth}$ y $B^\perp = \braces{\Vec{v}_{\kth+1} , \Vec{v}_{\kth+2} \dots \Vec{v}_\nth}$ dos bases ortogonales de $S$ y $S^\perp$ respectivamente.

Se define una base ortogonal de $\setV$ según la propiedad \ref{prop:comp2}.
\begin{equation*}
    \setV = \operatorname{gen} \braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\kth , \Vec{v}_{\kth+1} , \Vec{v}_{\kth+2} \dots \Vec{v}_\nth}
\end{equation*}

De manera que podemos escribir cualquier elemento $\Vec{v}\in\setV$ como:
\begin{equation*}
    \Vec{v} = \Vec{s} + \Vec{s}^\perp
\end{equation*}

Tal que:
\begin{itemize}
    \item 
    $\Vec{s} \in S$ es combinación lineal de $B=\braces{\Vec{v}_1 , \Vec{v}_2 \dots \Vec{v}_\kth}$
    
    \item
    $\Vec{s}^\perp \in S^\perp$ es comb. lin. de $B^\perp = \braces{\Vec{v}_{\kth+1} , \Vec{v}_{\kth+2} \dots \Vec{v}_\nth}$
\end{itemize}

Siguiendo el razonamiento que se usó para inferir la propiedad \ref{prop:orto3}, podemos expresar estas combinaciones lineales de la siguiente manera.
\begin{equation}
    \Vec{v} = \Vec{s} + \Vec{s}^\perp =
    \sum_{\ith=1}^\kth \frac{\intProd{\Vec{v}}{\Vec{v}_\ith}}{\nnorm{\Vec{v}_\ith}^2} \Vec{v}_\ith +
    \sum_{\ith=\kth+1}^\nth \frac{\intProd{\Vec{v}}{\Vec{v}_\ith}}{\nnorm{\Vec{v}_\ith}^2} \Vec{v}_\ith
    \label{eqn:orto}
\end{equation}

La ecuación \ref{eqn:orto} es de suma importancia, ya que da cuenta de la simplicidad con la que se puede definir la proyección ortogonal sin perder rigurosidad en la definición.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:proyOrto}
    \end{defn}
    \cusTi{Proyección ortogonal}
    \cusTe{La proyección de un vector $\Vec{v}$ sobre un subespacio $S\subset\setV$ se define como una transformación lineal $\setV \longrightarrow S$ tal que:}
    \begin{equation*}
        \proy_S(\Vec{v}) =
        \left\{
        \begin{aligned}
            \Vec{v} & \iff \Vec{v} \in S
            \\
            \Vec{0} & \iff \Vec{v} \in S^\perp
        \end{aligned}
        \right.
    \end{equation*}
\end{mdframed}

A continuación se tiene un esquema para $\setV=\setR^3$ donde $S$ es un plano y $S^\perp$ una recta.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/alg-proyeccion.pdf_tex}
\end{center}

En la figura anterior se observa que $\Vec{s}$ es la proyección ortogonal de $\Vec{v}$ sobre el subespacio $S$.

Dado que podemos escribir $\Vec{v}$ como la suma de un vector de $S$ más uno de $S^\perp$ y dado que la proyección es una transformación lineal (Def. \ref{defn:TL}), vemos que al plantear la proyección se tiene:
\begin{align*}
    \proy_S(\Vec{v}) &= \proy_S(\Vec{s}+\Vec{s}^\perp)
    \\
    &= \underbrace{\proy_S(\Vec{s})}_{\Vec{s}} + \underbrace{\proy_S(\Vec{s}^\perp)}_{\Vec{0}}
    \\
    &= \Vec{s}
    \\
    &= \sum_{\ith=1}^\kth \frac{\intProd{\Vec{v}}{\Vec{v}_\ith}}{\nnorm{\Vec{v}_\ith}^2} \Vec{v}_\ith
\end{align*}

A partir de esta conclusión no solo vemos que la definición \ref{defn:proyOrto} es correcta.
Sino que además se obtiene la siguiente fórmula para la proyección de $\Vec{v}$ sobre $S$, que es el elemento de $S$ que está a la menor distancia de $\Vec{v}$.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:proyOrto2}
    \end{prop}
    \begin{equation*}
        \proy_S(\Vec{v}) = \sum_{\ith=1}^\kth \frac{\intProd{\Vec{v}}{\Vec{v}_\ith}}{\nnorm{\Vec{v}_\ith}^2} \Vec{v}_\ith
    \end{equation*}
\end{mdframed}


\section{Aproximación de señales por series de Fourier}

Sea $T_N: \left[-\pi;\pi\right] \longrightarrow \setR$ un polinomio trigonométrico de orden $\Nth$ tal que:
\begin{equation*}
    T_\Nth(x) = \frac{a_0}{2} + \sum_{\ith=1}^\Nth \left[ a_\ith \sin(\ith x) + b_\ith \cos(\ith x) \right]
\end{equation*}

Sea $S_\Nth$ un subconjunto, del conjunto de funciones integrables en el intervalo $[-\pi;\pi]$, formado por los polinomios trigonométricos:
\begin{equation*}
    S_\Nth = \braces{T_\Nth: \left[-\pi;\pi\right] \longrightarrow \setR}
\end{equation*}

Dada $B$ una base ortogonal de $S_\Nth$:
\begin{multline*}
    B = \{ 1 , \sin(x) , \cos(x) , \sin(2x) , \cos(2x) ,
    \\
    \dots \sin(\Nth x) , \cos(\Nth x) \}
\end{multline*}

De manera que:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \operatorname{dim}(S) = 2 \Nth + 1
    \end{equation*}
\end{mdframed}

Para el producto interno definido como:
\begin{equation*}
    \intProd{f}{g} = \int_{-\pi}^\pi f(x) \, g(x) \, \dif x
\end{equation*}

Podemos definir la proyección ortogonal de una función $f$ sobre el subespacio $S_\Nth$ de polinomios trigonométricos.
Como se vio en la sección anterior esta proyección sería, de todos los elementos de $S_\Nth$, aquel polinomio que mejor se aproxime a $f$ por estar a la mínima distancia.
Según la propiedad \ref{prop:proyOrto2} esta aproximación estaría dada por:
\begin{multline*}
    \proy_{S_\Nth}(f) = \frac{\intProd{f}{1}}{\nnorm{1}^2} 1 +
    \\[1ex]
    + \frac{\intProd{f}{\sin(x)}}{\nnorm{\sin(x)}^2} \sin(x) + \frac{\intProd{f}{\cos(x)}}{\nnorm{\cos(x)}^2} \cos(x) +
    \\[1ex]
    + \frac{\intProd{f}{\sin(2x)}}{\nnorm{\sin(2x)}^2} \sin(2x) + \frac{\intProd{f}{\cos(2x)}}{\nnorm{\cos(2x)}^2} \cos(2x) + \dots
    \\[1ex]
    + \frac{\intProd{f}{\sin(\Nth x)}}{\nnorm{\sin(\Nth x)}^2} \sin(\Nth x) + \frac{\intProd{f}{\cos(\Nth x)}}{\nnorm{\cos(\Nth x)}^2} \cos(\Nth x)
\end{multline*}

Según el desarrollo en serie de Fourier sabemos que si $f$ es periódica y si el polinomio trigonométrico fuese de orden infinito, la suma convergería exactamente a la función.
En tal caso, los valores que multiplican a los elementos de la base $B$ son llamados coeficientes de Fourier.


\chapter{Funciones reales}


\section{Clasificación de funciones}

Podemos clasificar las funciones según sus conjuntos de partida y llegada.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Funciones escalares}
    \begin{equation*}
        f:\setR \longrightarrow \setR \tq f(x)=y
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Funciones vectoriales}
    \begin{multline*}
        \Vec{F}:\setR \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(x) = \begin{bmatrix} F_1(x) & F_2(x) & \dots & F_\mth(x) \end{bmatrix}
    \end{multline*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Campos escalares}
    \begin{equation*}
        f:\setR^\nth \longrightarrow \setR \tq f(x_1,x_2 \dots x_\nth)=y
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Campos vectoriales}
    \begin{multline*}
        f:\setR^\nth \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(\Vec{x}) = \begin{bmatrix} F_1(\Vec{x}) & F_2(\Vec{x}) & \dots & F_\mth(\Vec{x}) \end{bmatrix}
    \end{multline*}
\end{mdframed}


\section{Dominio}

El dominio es un subconjunto del conjunto de partida de una función.
El dominio es el conjunto de elementos que admiten las variables de una función.

Por ejemplo, suponiendo que la función tenga una división, una raíz de orden par o un logaritmo, los elementos excluidos del dominio son aquellos que anulen el denominador, que hagan negativo el radicando o que hagan menor o igual a cero el logaritmo.
Cualquier otra operación estaría permitida, y estos elementos que no presentan inconvenientes están en el dominio de la función.


\section{Conjuntos de nivel, secciones y trazas}

Los conjuntos de nivel, las secciones y las trazas son herramientas útiles para conocer cómo es la gráfica de una función de forma analítica.


\concept{Conjuntos de nivel}

Un conjunto de nivel es un subconjunto del dominio de una función.
Se define fijando la imagen, dándole un valor arbitrario para poder establecer relaciones entre las variables independientes del dominio, restringiéndolo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Conjunto de nivel}
    \cusTe{El conjunto de nivel $k$ es aquel que forman todos los vectores del dominio tales que la función de esos vectores tiene como valor $k$.}
    \begin{equation*}
        A_{k} = \big\{ \Vec{x} \in \operatorname{Dn}(f) \tq f(\Vec{x})=k \big\}
    \end{equation*}
\end{mdframed}

Dependiendo de la dimensión del conjunto de salida de $f$, las preimágenes de $k$ van a ser, una curva si $n=2$ o una superficie si $n=3$.
Es decir que los conjuntos de nivel son curvas de nivel si se trata de un campo escalar de 2 variables y superficies de nivel si se trata de un campo escalar de 3 variables.

La gráfica del conjunto de nivel tiene una dimensión menos que la gráfica de la función.
Por este mismo motivo podemos graficar una superficie de nivel de una función cuya gráfica estaría en 4 dimensiones, aunque no podamos graficar la función.

Algebraicamente es posible definir conjuntos de nivel en funciones para $n>3$, pero no podrían graficarse.


\concept{Secciones}

Una sección fija alguna de las variables independientes para conocer qué forma tienen los elementos de la imagen en función de las otras variables que no se fijaron.

Para una función de dos variables, visualmente la gráfica de la función se vería cortada por un plano vertical.


\concept{Trazas}

Una traza es la intersección de la gráfica de una función con un plano cualquiera, en cualquier dirección.


\section{Inyectividad y sobreyectividad}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Inyectividad de funciones escalares}
    \cusTe{Si una función escalar es monótona, entonces esta es inyectiva:}
    \begin{gather*}
        \textrm{Dada} \hspace{1ex} f: \setR \longrightarrow \setR
        \\
        \textrm{Si} \hspace{1ex} \frac{\dif}{\dif x} f(x) \neq 0 \quad \forall x \hspace{1ex} \in \operatorname{Dn}(f)
        \\
        \Rightarrow f \hspace{1ex} \textrm{es inyectiva}
    \end{gather*}
\end{mdframed}

De manera general, para funciones de varias variables (es decir campos escalares) se estudia la inyectividad por definición: una función es inyectiva si no existen dos valores distintos del dominio que tengan una misma imagen.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Inyectividad}
    \begin{gather*}
        f: \setR^\nth \longrightarrow \setR \hspace{1ex} \textrm{es inyectiva si se verifica:}
        \\
        f(\Vec{x}_1)=f(\Vec{x}_2) \Rightarrow \Vec{x}_1=\Vec{x}_2
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Inyectividad de funciones vectoriales}
    \cusTe{Una función vectorial es inyectiva si cada una de sus funciones escalares componentes lo son.}
    \begin{multline*}
        \textrm{Dada} \hspace{1ex} \Vec{F}: \setR \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(x) = \begin{bmatrix} f_1(x) & f_2(x) & \ldots & f_\mth(x) \end{bmatrix}
    \end{multline*}
    \begin{gather*}
        \textrm{Si} \hspace{1ex} f_\ith(x) \hspace{1ex} \textrm{es inyectiva} \hspace{1ex} \forall \hspace{1ex} i \in [1;\mth]
        \\
        \Rightarrow \Vec{F} \hspace{1ex} \textrm{es inyectiva}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Inyectividad de campos vectoriales}
    \cusTe{Un campo vectorial es inyectivo si cada uno de sus campos escalares componentes lo son.}
    \begin{multline*}
        \textrm{Dada} \hspace{1ex} \Vec{F}: \setR^\nth \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(\Vec{x}) = \begin{bmatrix} f_1(\Vec{x}) & f_2(\Vec{x}) & \ldots & f_\mth(\Vec{x}) \end{bmatrix}
    \end{multline*}
    \begin{gather*}
        \textrm{Si} \hspace{1ex} f_\ith(\Vec{x}) \hspace{1ex} \textrm{es inyectiva} \hspace{1ex} \forall \hspace{1ex} i \in [1;\mth]
        \\
        \Rightarrow \Vec{F} \hspace{1ex} \textrm{es inyectiva}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Sobreyectividad}
    \cusTe{Una función es sobreyectiva si el conjunto de llegada es igual al conjunto imagen.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Sobreyectividad de funciones escalares}
    \begin{gather*}
        \textrm{Dada} \hspace{1ex} f: \setR \longrightarrow \setR
        \\
        \textrm{Si} \hspace{1ex} \im(f) = \setR
        \Rightarrow f \hspace{1ex} \textrm{es sobreyectiva}
        \\
        \textrm{Si} \hspace{1ex} \lim_{x \to \infty} f(x) = \pm \infty
        \Rightarrow f \hspace{1ex} \textrm{es sobreyectiva}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Sobreyectividad de funciones vectoriales}
    \cusTe{Una función vectorial es sobreyectiva si cada una de sus funciones escalares componentes lo son.}
    \begin{multline*}
        \textrm{Dada} \hspace{1ex} \Vec{F}: \setR \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(x) = \begin{bmatrix} f_1(x) & f_2(x) & \ldots & f_\mth(x) \end{bmatrix}
    \end{multline*}
    \begin{gather*}
        \textrm{Si} \hspace{1ex} f_\ith(x) \hspace{1ex} \textrm{es sobreyectiva} \hspace{1ex} \forall \hspace{1ex} i \in [1;\mth]
        \\
        \Rightarrow \Vec{F} \hspace{1ex} \textrm{es sobreyectiva}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Biyectividad de campos vectoriales}
    \cusTe{Un campo vectorial es biyectivo si es inyectivo y sobreyectivo simultáneamente.
    Esto es:}
    \begin{multline*}
        \textrm{Dada} \hspace{1ex} \Vec{F}: \setR^\nth \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(\Vec{x}) = \begin{bmatrix} f_1(\Vec{x}) & f_2(\Vec{x}) & \ldots & f_\mth(\Vec{x}) \end{bmatrix} = \Vec{y}
    \end{multline*}
    \begin{gather*}
        \Vec{F} \hspace{1ex} \textrm{es biyectiva si:}
        \\
        \forall \hspace{1ex} \Vec{y} \in \setR^\mth \hspace{1ex} \exists ! \hspace{1ex} \Vec{x} \in \operatorname{Dn}(\Vec{F}): \Vec{y} = \Vec{F}(\Vec{x})
        \\[1em]
        \textrm{O bien, si:}
        \\
        \forall \hspace{1ex} \Vec{x} \in \operatorname{Dn}(\Vec{F}) \hspace{1ex} \exists ! \hspace{1ex} \Vec{y} \in \setR^m : \Vec{x} = \Vec{F}^{-1}(\Vec{y})
    \end{gather*}
\end{mdframed}


\section{Funciones homogéneas}

La homogeneidad de una función está relacionada con el grado de su fórmula.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Homogeneidad}
    \begin{gather*}
        f:\setR^\nth \longrightarrow \setR \hspace{1ex} \textrm{es homogénea de grado $k$ si:}
        \\
        \lambda^k \, f(\Vec{x}) = f(\lambda^k \, \Vec{x})
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si una función es homogénea, se puede calcular el límite por reemplazo directo tras hacer un cambio de variables a coordenadas polares.
\end{mdframed}


\chapter{Límites de funciones reales}

El límite de una función cuando las variables independientes tienden a cierto valor, sirve para calcular a qué valor del conjunto de llegada se acerca la función si nos acercamos a un punto del conjunto de partida.

Para que exista el límite la función tiene que tender al mismo valor a medida que nos acercamos más y más por cualquiera de los infinitos caminos posibles.
En una variable, basta con probar el límite por izquierda y por derecha ya que si son iguales se puede decir que el límite existe y es único.
La dificultad principal en varias variables es que el dominio no es solo una recta, y hay infinitas direcciones para acercarse, no solo dos.

Por este motivo, si bien se puede demostrar que el límite no existe si por dos caminos la función tiende a valores distintos, no basta con haber probado por algunos caminos para afirmar que el límite existe: hay infinitos caminos posibles.


\section{Definición de límite}

Dada la función $f:\setR^\nth \longrightarrow \setR$.
Sea $\Vec{x}_0$ un punto de acumulación (Def. \ref{defn:limitPoint}), es decir que $\Vec{x}_0$ no necesariamente pertenece al dominio pero sí lo hace su entorno.

Si se le asignan a la variable independiente $\Vec{x}$ valores cada vez más cercanos al punto $\Vec{x}_0$, la diferencia $\Delta \Vec{x} = \Vec{x}-\Vec{x}_0$ va a tender a cero.

De esta forma, si las imágenes de los valores que va tomando la variable tienden al mismo valor por todos los caminos del entorno de $\Vec{x}_0$, entonces el límite existe.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Límite}
    \begin{gather*}
        \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})=l \iff
        \\
        \forall \hspace{1ex} \varepsilon >0 \hspace{1ex} \exists \hspace{1ex} \delta(\varepsilon,\Vec{x})>0 \tq
        \\[1ex]
        \nnorm{ \Vec{x}-\Vec{x}_0 } < \delta \Rightarrow \nnorm{ f(\Vec{x})-l } < \varepsilon
    \end{gather*}
\end{mdframed}


\section{Métodos algebráicos}

Para calcular el límite de una función $f:\setR^\nth \longrightarrow \setR$ cuando $\Vec{x} \to \Vec{x}_0$ basta con reemplazar $\Vec{x}_0$ en la fórmula de $f$.
Esto será posible siempre y cuando la función no sea partida o el resultado que se obtenga no sea \emph{indeterminado} debido a una operación que no esté definida matemáticamente.

Las siguientes son todas las posibles indeterminaciones que suelen darse al evaluar $\Vec{x}_0$ en la fórmula de la función:

\begin{equation*}
    \dfrac{0}{0} ; \dfrac{\infty}{\infty} ; 1^\infty ; \infty^0 ; 0 \cdot \infty ; \infty-\infty
\end{equation*}

Si al evaluar $f(\Vec{x}_0)$ se obtiene alguna de estas indeterminaciones no es posible determinar, en caso de que exista, el valor al que tiende el límite ni concluir que no existe.

A veces es posible hacer movimientos algebráicos y reescribir $f(\Vec{x})$ usando las siguiente propiedades.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Infinitésimo por acotado}
    \begin{gather*}
        \textrm{Sea} \hspace{1ex} f(\Vec{x}) = g(\Vec{x}) \, h(\Vec{x})
        \\
        \textrm{Si} \hspace{1ex} g(\Vec{x}_0) \to 0 \land h(\Vec{x}_0) \hspace{1ex} \textrm{está acotada}
        \Rightarrow f(\Vec{x}_0) \to 0
    \end{gather*}
\end{mdframed}

% AGREGAR Cambio de Variables
% AGREGAR Factorización

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Diferencia de cuadrados}
    \begin{equation*}
        x^2-y^2 = (x+y)(x-y)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Suma y resta de cubos}
    \begin{equation*}
        x^3 \pm y^3 = (x \pm y)(x^2 \mp xy + y^2)
    \end{equation*}
\end{mdframed}

\concept{Demostración}
\begin{align*}
(x \pm y)^2 &= (x \pm y)(x \pm y)=x^2 \pm xy + y^2
\\
(x \pm y)^3 &= (x \pm y)(x \pm y)^2=(x \pm y)(x^2 \pm xy + y^2)
\\
&= x^3 \pm 3x^2y + 3xy^2 \pm y^3
\\
&= x^3 \pm 3xy(x \pm y) \pm y^3
\\
x^3 \pm y^3 &= (x \pm y)^3 \mp 3xy(x\pm y)
\\
&= (x \pm y) \Big( (x\pm y)^2 \mp 3xy \Big)
\\
&= x^2 \pm 2xy \mp 3xy + y^2
\end{align*}

% AGREGAR El método de completar cuadrados

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Límite fundamental}
    \begin{equation*}
        \lim_{x \to x_0} \dfrac{\sin{\Big( f(x) \Big)}}{f(x)} = 1 \iff f(x_0) \rightarrow 0
    \end{equation*}
\end{mdframed}

% AGREGAR Propiedad de Bernoulli

\section{Propiedades de los límites}

\begin{itemize}
\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} k f(\Vec{x}) = k \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) \quad \textrm{con} \hspace{1ex} k \in \setR$

\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})+g(\Vec{x})=\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) + \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})$

\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) \cdot g(\Vec{x})=\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) \cdot \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})$

\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} \dfrac{f(\Vec{x})}{g(\Vec{x})} = \dfrac{\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})}{\lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})} \quad \textrm{con} \hspace{1ex} g(\Vec{x}) \neq 0$

\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} \Big( f(\Vec{x}) \Big)^{g(\Vec{x})}= \Big( \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) \Big)^{\lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})}$

\item $\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} \ln{\Big( f(\Vec{x}) \Big)} = \ln{\Big( \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) \Big)}$

\item $\displaystyle\lim_{\Vec{x} \to 0} \dfrac{k}{\Vec{x}} = \infty \quad \textrm{con} \hspace{1ex} k \in \setR-0$

\item $\displaystyle\lim_{\Vec{x} \to \infty} \dfrac{k}{\Vec{x}} = 0 \quad \textrm{con} \hspace{1ex} k \in \setR$

\end{itemize}


\section{Unicidad del límite}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si el límite existe, entonces es único.
    \begin{equation*}
        \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) = l_1  \hspace{1ex} \land \hspace{1ex} \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) = l_2 \hspace{1ex} \Rightarrow \hspace{1ex} l_1=l_2
    \end{equation*}
\end{mdframed}

La implicación es en un solo sentido.
Es decir, que no es válido decir que si la función tiende a un mismo valor por dos caminos entonces el límite existe.
Esta propiedad no sirve para demostrar la existencia del límite, ya que no es posible probar la tendencia de los infinitos caminos por los que se puede llegar al punto.

Por el contrario, es posible demostrar la no existencia del límite, encontrando dos caminos que pasen por el punto mediante los cuales la función tienda a un valor diferente.
De esta forma, al no tender a un único valor para todos los caminos, el límite no existiría.

La trayectoria que se tome al analizar el límite no puede pasar por ningún punto que no esté en el dominio.
De lo contrario, sería posible encontrar un límite diferente, pero sería diferente justamente porque se usaron valores que la función ni siquiera admite.

El problema que se puede presentar para demostrar la no existencia de un límite usando el teorema de unicidad es que nunca se sabe si hay otro camino por el cual se puede acercarse al punto que tenga un límite diferente.
A continuación se muestran, en orden de complejidad, los caminos que se pueden ir probando hasta encontrar uno que tenga un límite diferente.

Para facilitar la notación se estudian límites dobles, es decir, límites de campos escalares tipo $f:\setR^2 \longrightarrow \setR$, pero el análisis se puede generalizar a $\setR^n$.
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) = \lim_{\substack{x \to x_0\\y \to y_0}} f(x,y)
\end{equation*}

A continuación se muestran diferentes métodos con algunos de los camínos que se pueden tomar para evaluar la tendencia del límite.
En cada método se propone un tipo de trayectoria distinta.
Para que el límite exista, la tendencia tiene que ser igual para todas las trayectorias de los diferentes métodos así como para las diferentes trayectorias de un mismo método.


\subsection{Límites iterados}

Tomar límites iterados o reiterados es tomar el límite de la función dejando solo una variable libre y las otras fijas.

\begin{equation*}
    \lim_{\substack{x \to x_0\\y \to y_0}} f(x,y)
    = \lim_{\substack{x \to x_0\\y = y_0}} f(x,y)
    = \lim_{\substack{x = x_0\\y \to y_0}} f(x,y)
\end{equation*}

Observar que los límites iterados evalúan las rectas $x=x_0$ e $y=y_0$ y en caso de que sean distintos, quedaría probado que el límite no existe.

\subsection{Límite por rectas implícitas}

Los límites radiales o por rectas analizan la tendencia al punto por una familia de rectas, sin incluir las rectas $x=0$ e $y=0$.
Se usa la ecuación de una recta para que una variable quede en función de la otra.

\begin{equation*}
    \lim_{\substack{x \to x_0\\y \to y_0}} f(x,y)
    = \lim_{\substack{x \to x_0\\y=mx+b}} f(x,y)
    = \lim_{\substack{x=my+b\\y \to y_0}} f(x,y)
\end{equation*}

Si el límite queda en función del parámetro $m$ quiere decir que el valor del límite va a depender de la pendiente $m$ de la recta, con lo que queda demostrada su no existencia.


\subsection{Límite por rectas paramétricas}

A diferencia de usar la ecuación implícita de una recta, si se usa una parametrización también se analiza la tendencia por los ejes coordenados.

\begin{equation*}
    \lim_{\substack{x \to x_0\\y \to y_0}} f(x,y)
    = \lim_{\substack{x=at+x_0\\y=bt+y_0\\t \to 0}} f(x,y)
\end{equation*}

Si el límite depende de alguno de los parametros $a$ o $b$, significa que dependiendo de la recta por la que se aproxime el resultado va a ser distinto, quedando demostrada la inexistencia del límite.


% \subsection{Límite por curvas}

% \subsection{Límite por curvas de nivel}

% \subsection{Límite por familia de curvas}

% \subsection{Límite por familia de curvas más una excluida del dominio}


\section{Teorema de intercalación}

El teorema de intercalación se usa para demostrar que un límite existe.
Antes de demostrar un límite por intercalación hay que haber probado suficientes caminos para tener certeza que por todos los caminos el límite siempre tiende a un mismo valor.

Por la primera propiedad de la norma:
\begin{equation*}
    \nnorm{\Vec{x}} = 0 \iff \Vec{x} = \Vec{0}
\end{equation*}

Y por las propiedades de los límites:
\begin{equation*}
    \displaystyle\lim_{\Vec{x} \to \Vec{x}_0} \norm{f(\Vec{x})}
    = \norm{\displaystyle\lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})}
\end{equation*}

Luego:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) = 0 \iff \lim_{\Vec{x} \to \Vec{x}_0} \norm{f(\Vec{x})} = 0
\end{equation*}

O bien:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} \Big\{ f(\Vec{x}) - l \Big\} = 0
    \iff
    \lim_{\Vec{x} \to \Vec{x}_0} \Big| f(\Vec{x}) - l \Big| = 0
\end{equation*}

Observar que $\norm{f(\Vec{x})-l}$ está acotado inferiormente por ser siempre positivo.
Sea la función $g:\setR^\nth \longrightarrow \setR$ cota superior en un entorno de $\Vec{x}_0$:
\begin{equation*}
    0 \leq |f(\Vec{x})-l| \leq g(\Vec{x})
\end{equation*}

Luego, tomando límite en los tres miembros de la inecuación, si y solo si $\lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})=0$ se tiene:
\begin{equation*}
    0 \leq \lim_{\Vec{x} \to \Vec{x}_0} \norm{f(\Vec{x})-l} \leq 0
\end{equation*}

Al ser el límite mayor igual y menor igual que cero simultaneamente, la única solución posible es que valga solo el igual:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} \norm{f(\Vec{x})-l} = 0
\end{equation*}

Finalmente nuevamente por la primera propiedad de la norma:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x}) = l
\end{equation*}

Con lo cual, basta con demostrar la existencia de una función $g:\setR^\nth \longrightarrow \setR \tq \lim_{\Vec{x} \to \Vec{x}_0} g(\Vec{x})=0$ que sea cota superior para demostrar la existencia del límite.


\concept{Acotaciones importantes}

\begin{equation*}
x^3 \leq |x^3| = \left( \sqrt{x^2} \right)^3 \leq \left( \sqrt{x^2+y^{2n}} \right)^3
\end{equation*}


\section{Continuidad}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Continuidad}
    \cusTe{Una función $f:\setR^\nth \longrightarrow \setR$ es contínua en $\Vec{x}_0$ si se verifica que:}
    \begin{equation*}
        f(\Vec{x}_0)= \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})
    \end{equation*}
\end{mdframed}


\chapter{Derivadas de funciones reales}


\section{Derivadas parciales}

Una derivada es un número que representa la razón de cambio (o cociente incremental) cuando el incremento de una variable independiente tiende a cero.

Para funciones de una variable, es evidente que solo es posible definir un cociente incremental entre la variable dependiente y la independiente:
\begin{gather*}
    f:\setR\longrightarrow\setR \tq f(x)=y
    \\
    f' = \frac{\dif y}{\dif x} = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{x \to x_0} \frac{f(x)-f(x_0)}{x-x_0}
\end{gather*}

Para funciones de varias variables, solo se puede analizar el cociente incremental de una variable a la vez, fijando las demás para que permanezcan constantes.

Las dos derivadas \emph{parciales} de una función de dos variables $f:\setR^2 \longrightarrow \setR$ están definidas como sigue.
\begin{align*}
    f_x' = \frac{\partial}{\partial x} f(x_0,y_0) &= \lim_{x \to x_0} \frac{f(x,y_0)-f(x_0,y_0)}{x-x_0}
    \\[1ex]
    f_y' = \frac{\partial}{\partial y} f(x_0,y_0) &= \lim_{y \to y_0} \frac{f(x_0,y)-f(x_0,y_0)}{y-y_0}
\end{align*}

O bien, definiendo el vector incremento:
\begin{equation*}
    \begin{bmatrix} h & k \end{bmatrix}
    = \begin{bmatrix} \Delta x & \Delta y \end{bmatrix}
    = \begin{bmatrix} x-x_0 & y-y_0 \end{bmatrix}
\end{equation*}

Las derivadas parciales se pueden expresar con la siguiente notación mediante un cambio de variables:
\begin{align*}
    f_x' = \dfrac{\partial}{\partial x} f(x_0,y_0) &= \lim_{h \to 0} \dfrac{f(x_0+h,y_0)-f(x_0,y_0)}{h}
    \\
    f_y' = \dfrac{\partial}{\partial y} f(x_0,y_0) &= \lim_{k \to 0} \dfrac{f(x_0,y_0+k)-f(x_0,y_0)}{k}
\end{align*}

De manera general, se puede definir la derivada parcial para funciones de $\nth$ variables como:

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Derivada parcial}
    \begin{equation*}
        f_{x_\ith}'
        = \dfrac{\partial}{\partial x_\ith} f(\Vec{x}_0)
        = \lim_{x_\ith \to x_{0_\ith}} \dfrac{f(\Vec{x}_\ith)-f(\Vec{x}_0)}{x_\ith-x_{0_\ith}}
    \end{equation*}
\end{mdframed}

Donde $\Vec{x}_0$ es el punto donde se calcula la derivada y $\Vec{x}_\ith$ es ese mismo punto con la componente $x_\ith$ incrementada:
\begin{align*}
    \Vec{x}_0 &= \begin{bmatrix} x_{0_1} & x_{0_2} & \ldots & x_{0_\ith} & \ldots & x_{0_n} \end{bmatrix}
    \\
    \Vec{x}_\ith &= \begin{bmatrix} x_{0_1} & x_{0_2} & \ldots & x_\ith & \ldots & x_{0_n} \end{bmatrix}
\end{align*}

El vector incremento es entonces:
\begin{equation*}
    \Delta \Vec{x} = \Vec{x}_\ith - \Vec{x}_0 = \begin{bmatrix} 0 & 0 & \ldots & x_\ith - x_0 & \ldots & 0 \end{bmatrix}
\end{equation*}

Del cual solo nos interesa analizar la tendencia de la componente $\ith$-ésima haciendo el producto interno con el versor canónico $\versor{e}_\ith=\begin{bmatrix} 0 & 0 & \ldots & 1 & \ldots & 0 \end{bmatrix}$ pudiendo definir:
\begin{equation*}
    h = \Delta \Vec{x} \cdot \versor{e}_\ith = x_\ith - x_{0_\ith}
\end{equation*}

Multiplicando $\versor{e}_\ith$ y sumando $\Vec{x}_0$ en ambos miembros de la ecuación anterior queda definido el cambio de variables para $\Vec{x}_\ith$, que puede que no sea tan obvio como en una variable:
\begin{equation*}
    h \, \versor{e}_\ith + \Vec{x}_0 = \Delta \Vec{x} \underbrace{\left( \versor{e}_\ith \cdot \versor{e}_\ith \right)}_{=1} + \Vec{x}_0 = \Vec{x}_\ith
\end{equation*}

Pudiendo finalmente expresar la derivada parcial con respecto a la variable $\ith$-ésima a partir del cambio de variable.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Derivada parcial}
    \begin{equation*}
    f_{x_\ith}'
    = \frac{\partial}{\partial x_\ith} f(\Vec{x}_0)
    = \lim_{h \to 0} \frac{f(\Vec{x}_0 + h\,\versor{e}_\ith) - f(\Vec{x}_0)}{h}
\end{equation*}
\end{mdframed}

El versor canónico $\versor{e}_\ith$ que se usó para definir las derivadas parciales es un vector que tiene un 1 en la componente $\ith$-ésima y ceros en las demás componentes.
Para derivadas parciales de funciones de dos variables $f:\setR^2 \longrightarrow \setR$ el subíndice de $\versor{e}_i$ solo puede tomar dos valores quedando $\versor{e}_1=(1,0)$ o bien $\versor{e}_2=(0,1)$.
De esta forma, dependiendo del subíndice asignado quedan definidas las derivadas parciales mencionadas anteriormente, al inicio de la sección.

    
\section{Derivadas direccionales}

Notar que el versor $\versor{e}_\ith$ solo puede tener $\nth$ direcciones.
En derivadas parciales de funciones de dos variables $f:\setR^2 \longrightarrow \setR$ solo puede tener las direcciones $\iVer$ o $\jVer$.
En estas direcciones es que la gráfica de la función tiene cierta pendiente, que es lo que calcula la derivada parcial.

Ahora bien, es posible calcular la derivada parcial en una dirección que no sea la de los ejes coordenados.
En funciones de dos variables, la derivada direccional es la pendiente que tiene la función sobre una traza, cortando la gráfica de la función con un plano vertical.
La derivada direccional se define con un versor director $\versor{r}$ en vez de usar el versor canónico.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Derivada direccional}
    \begin{equation*}
        f_{\versor{r}}'
        = \frac{\partial}{\partial \versor{r}} f(\Vec{x}_0)
        = \lim_{h \to 0} \frac{f(\Vec{x}_0+h\,\versor{r})-f(\Vec{x}_0)}{h}
    \end{equation*}
\end{mdframed}

De no usar un versor $(\versor{r})$ se puede usar un vector director $(\Vec{r})$ que no esté normalizado, siempre y cuando luego se contrareste su magnitud.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        f_{\Vec{r}}' = \frac{\partial}{\partial \versor{r}} f(\Vec{x}_0) = \lim_{h \to 0} \dfrac{f(\Vec{x}_0+h\,\Vec{r})-f(\Vec{x}_0)}{h\, \nnorm{\Vec{r}}}
    \end{equation*}
\end{mdframed}


\section{Operadores}

El operador de Hamilton o operador nabla es un vector que tiene por componentes la operación derivada parcial con respecto a las diferentes $\nth$ variables.
Las componentes de este vector no son números ni funciones, si no operaciones.
Por este motivo, por si solo no tiene uso pero sí tiene múltiples aplicaciones.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Operador nabla}
    \begin{equation*}
        \grad = \begin{bmatrix} \dfrac{\partial}{\partial x_1} & \dfrac{\partial}{\partial x_2} & \cdots & \dfrac{\partial}{\partial x_\nth} \end{bmatrix}
    \end{equation*}
\end{mdframed}

El operador de Laplace o Laplaciano es un vector que tiene por componentes la operación derivada parcial de segundo orden con respecto a las diferentes $\nth$ variables.
Así como el operador de Hamilton, las componentes del Laplaciano no son números ni funciones, si no operaciones.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Operador Laplaciano}
    \begin{equation*}
        \grad^2 = \begin{bmatrix} \dfrac{\partial^2}{\partial x_1^2} & \dfrac{\partial^2}{\partial x_2^2} & \cdots & \dfrac{\partial^2}{\partial x_\nth^2} \end{bmatrix}
    \end{equation*}
\end{mdframed}

Para una función $f:\setR^\nth \longrightarrow \setR$, es posible obtener $\nth$ derivadas parciales.
Es conveniente establecer una notación de todas las derivadas parciales y armar así un arreglo vectorial con una derivada parcial en cada componente.
Si la función es \emph{diferenciable}, a este vector de las derivadas se lo llama gradiente, y se define con el operador nabla como sigue.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Gradiente a fin}
    \begin{equation*}
        \grad f(\Vec{x}) = \begin{bmatrix} \dfrac{\partial f(\Vec{x})}{\partial x_1} & \dfrac{\partial f(\Vec{x})}{\partial x_2} & \cdots & \dfrac{\partial f(\Vec{x})}{\partial x_\nth} \end{bmatrix}
    \end{equation*}
\end{mdframed}

Para una función $\Vec{F}:\setR^\nth \longrightarrow \setR^\mth$, es posible obtener $\nth \times \mth$ derivadas parciales.
Es conveniente establecer una notación de todas las derivadas parciales y armar así un arreglo matricial que en cada fila tiene el vector derivada de cada función-componente del campo vectorial.
Notar que si $\mth=1$ es el caso particular del vector derivada de un campo escalar.
Si el campo vectorial es diferenciable la matriz de derivadas parciales lleva el nombre de \emph{Jacobiano}.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Jacobiano a fin}
    \begin{equation*}
        J = \Vec{F}'(\Vec{x}) = 
        \begin{bmatrix}
            \dfrac{\partial f_1(\Vec{x})}{\partial x_{11}} & \cdots & \dfrac{\partial f_1(\Vec{x})}{\partial x_{1\nth}}
            \\
            \vdots & \ddots & \vdots
            \\
            \dfrac{\partial f_\mth(\Vec{x})}{\partial x_{\mth 1}} & \cdots & \dfrac{\partial f_\mth(\Vec{x})}{\partial x_{\mth\nth}}
        \end{bmatrix}
    \end{equation*}
\end{mdframed}

La matriz Hessiana de un campo escalar es un arreglo matricial de derivadas de segundo orden.
La imagen de una función $f:\setR^\nth \longrightarrow \setR$ es un escalar, un valor numérico.
Sus primeras derivadas parciales se pueden representar en un vector derivado de $\nth$ componentes.
Cada derivada del gradiente estará en función de todas las variables, pudiendo por cada componente del gradiente obtener un nuevo vector de derivadas parciales segundas.
A medida que subimos el orden de derivación, se necesita un elemento cuya estructura tenga más dimensiones para representar las derivadas.
Para las derivadas segundas de $f$ se define la \emph{Matriz Hessiana}, que tiene todas las posibles derivadas cruzadas y en la diagonal principal las derivadas segundas que solamente fueron derivadas con respecto a una misma variable.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Matriz Hessiana}
    \begin{equation*}
        H = f''(\Vec{x}) =
        \begin{bmatrix}
            \dfrac{\partial^2 f(\Vec{x})}{\partial x_1^2} & \cdots & \dfrac{\partial^2 f(\Vec{x})}{\partial x_1 \partial x_\nth}
            \\
            \vdots & \ddots & \vdots
            \\
            \dfrac{\partial^2 f(\Vec{x})}{\partial x_\nth \partial x_1} & \cdots & \dfrac{\partial^2 f(\Vec{x})}{\partial x_\nth^2}
        \end{bmatrix}
    \end{equation*}
\end{mdframed}


\section{Regla de la cadena}

Una función compuesta es aquella cuyo conjunto de partida es el conjunto de llegada de otra función.
Es decir que su dominio es la imagen de otra cierta función.
También se puede interpretar como una función cuyas variables dependen a su vez de otro parámetro.
\clearpage
\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función compuesta}
    \cusTe{Dadas:}
    \begin{align*}
        & \Vec{F}:\setR^\nth \longrightarrow \setR^\mth
        \\
        & \Vec{G}:\setR^\nth \longrightarrow \setR^k
        \\
        & \Vec{H}:\setR^k \longrightarrow \setR^\mth
    \end{align*}
    \noTi{Se puede definir $\Vec{F}$ como la composición de $\Vec{H}$ compuesta por $\Vec{G}$ tal que:}
    \begin{equation*}
        \Vec{H} \circ \Vec{G} = \Vec{F}(\Vec{x})=\Vec{H} \left( \Vec{G}(\Vec{x}) \right)
    \end{equation*}
\end{mdframed}

La derivación de funciones compuestas cumple la misma regla que en una variable.
La derivada de $\Vec{H}$ compuesta por $\Vec{G}$ es igual a la derivada de $\Vec{H}$ evaluada en $\Vec{G}$ evaluada en el punto, por la derivada de $\Vec{G}$ evaluada en el punto.
Esto es:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Regla de la cadena}
    \begin{equation*}
        \Vec{F}'(\Vec{x}_0) = \Vec{H}' \left( \Vec{G}(\Vec{x}_0) \right) \cdot \Vec{G} \left( \Vec{x}_0 \right)
    \end{equation*}
\end{mdframed}


\section{Teorema de Schwarz}

Calcular las derivadas sucesivas o derivadas de orden superior conciste en derivar una función tantas veces como el orden $k \in \setN$ de la derivada lo indique.

En funciones de una variable, simplemente es hacer la derivada de la derivada sucesivamente.
La única dificultad que se puede presentar es que calcular la próxima derivada sea cada vez más difícil de computar.

En funciones de varias variables, cada vez que hagamos una derivada parcial hasta llegar al orden deseado, al volver a derivar vamos a tener nuevas derivadas parciales que vamos a tener que derivar por separado y cada una de estas nuevas derivadas va a tener varias derivadas parciales nuevas de manera sucesiva.

La cantidad total $N$ de derivadas parciales que tenga una función $f:\setR^\nth \longrightarrow \setR$ si se la deriva $k$ veces es:
\begin{equation*}
    N = \nth^k\
\end{equation*}

De las cuales va a haber $\nth$ derivadas parciales que se derivaron solamente con respecto a una variable de manera sucesiva.
Eventualmente, estas derivadas no mixtas son diferentes entre si.

Mediante la siguiente sucesión se representan, en cada uno de sus elementos, las derivadas de orden $k$ con respecto a cada variable $x_\ith$:
\begin{align*}
    \braces{a_n}
    &= \prod_{i=1}^k \dfrac{\partial ^i}{\partial x_n^i} f(\Vec{x}_0)
    \\
    &= \braces{
    \dfrac{\partial^k}{\partial x_1^k} f(\Vec{x}_0); \dfrac{\partial^k}{\partial x_2^k} f(\Vec{x}_0); \cdots ; \dfrac{\partial^k}{\partial x_n^k} f(\Vec{x}_0)
    }
\end{align*}

Si en vez de derivar con respecto a una misma variable, se deriva la primera vez con respecto a una variable, luego con respecto a otra y así sucesivamente, se obtiene una derivada mixta.

Va a haber derivadas sucesivas mixtas que se derivaron con respecto a cada variable igual cantidad de veces, de manera curzada.
El Teorema de Schwarz dice que las derivadas mixtas del mismo orden que tengan igual cantidad de variables diferenciadas van a ser iguales.
En otras palabras, que da igual derivar primero con respecto a una variable y luego con respecto a otra que hacerlo al revez.

Las posibles combinaciones de derivadas que se pueden hacer son muchas.
Pero por el Teorema de Schwarz, sabemos que hay varias que van a ser iguales, ya que la cantidad $(M)$ de grupos de derivadas eventualmente distintas entre sí, es:
\begin{equation*}
    M = \dfrac{(k+\nth-1)!}{k!\left(\nth-1\right)!}
\end{equation*}


\chapter{Diferenciabilidad en funciones reales}


\section{Teorema de Taylor}

Dada la función $f:\setR^\nth \longrightarrow \setR$ \emph{diferenciable} en un punto $\Vec{x}_0$, se define la aproximación $T(\Vec{x})$, tal que en torno al punto esta tiende al límite de la función:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} T(\Vec{x}) = \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})
\end{equation*}

Y tal que para el resto del dominio, la función se pueda igualar a un valor aproximado $(T)$ más cierto error $(E)$ calculable:
\begin{equation*}
    f(\Vec{x}) = T(\Vec{x}) + E(\Vec{x}) \quad \forall \hspace{1ex} x \in \operatorname{Dn}(f)
\end{equation*}

Bajo estas hipótesis dadas, tomando el límite en la ecuación anterior se hace evidente que el error tiende a cero.
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} f(\Vec{x})
    = 
    + \lim_{\Vec{x} \to \Vec{x}_0} E(\Vec{x})
\end{equation*}

Dependiendo del valor de $\nth$, la aproximación puede ser una recta tangente, un plano tangente o un hiperplano tangente, todos dados por las derivadas sucesivas de $f$.
La aproximación está definida con la derivada de la función.
Cuanto más elevado sea el orden de la derivada, más precisa va a ser la aproximación.
Cuanto más lejos del punto $\Vec{x}_0$ se evalúe, mayor va a ser el error.

A continuación se presentan algunos casos particulares de \emph{aproximaciones a fin}, según la cantidad $\nth$ de variables y el orden $k$ de las derivadas.
Se la llama \emph{a fin} dado que es posible proponerla pero no es posible afirmar que es una buena aproximación hasta no demostrar que la función que intenta aproximar es efectivamente diferenciable.

\concept{Recta tangente}

Es una aproximación lineal de primer orden para funciones de variable real $\nth=1$.
Está formada por un vector director que es la tasa de crecimiento o derivada en ese punto más el valor de la función en el punto.
\begin{equation*}
    R(x) = f(x_0) + \frac{\dif}{\dif x} f(x_0) \left( x-x_0 \right)
\end{equation*}

\concept{Plano tangente}

Es una aproximación lineal (a fin) de primer orden para funciones de $\nth=2$ variables.
Está formado por dos vectores directores que son las derivadas parciales en el punto más el valor de la función en el punto.
\begin{equation*}
    T(\Vec{x}) = f(\Vec{x}_0) + \frac{\partial}{\partial x} f(\Vec{x}_0) \left(x-x_0\right) + \frac{\partial}{\partial y} f(\Vec{x}_0) \left(y-y_0\right)
\end{equation*}

\concept{Hiperplano tangente}

Es una aproximación lineal (a fin) de primer orden para funciones de $n$ variables.
Está formado por el producto interno entre el vector derivada de $f$ y el vector incremento, más el valor de la función en el punto.
\begin{equation*}
    T(\Vec{x}) = f(\Vec{x}_0) + \grad f(\Vec{x}_0) \cdot \Delta \Vec{x}
\end{equation*}

Donde:
\begin{equation*}
    \Delta \Vec{x} = \begin{bmatrix} x_1-x_{0_1} & x_2-x_{0_2} & \dots & x_n-x_{0_n} \end{bmatrix}
\end{equation*}

\concept{Aproximación de segundo orden}

Una aproximación lineal puede ser una recta o un plano, en funciones de 1 o 2 variables respectivamente.
Una aproximación no lineal más precisa se obtiene haciendo el desarrollo de Taylor con derivadas de orden superior.
Particularmente, la aproximación de segundo orden se hace mediante el Hessiano:
\begin{equation*}
    T(\Vec{x}) = f(\Vec{x}_0) + \grad f(\Vec{x}_0) \cdot \Delta \Vec{x} + \frac{1}{2} H(\Vec{x}_0) \cdot \Delta \Vec{x}
\end{equation*}

Donde:
\begin{equation*}
    \Delta \Vec{x} = \begin{bmatrix} x_1-x_{0_1} & x_2-x_{0_2} & \dots & x_n-x_{0_n} \end{bmatrix}
\end{equation*}


\section{Incremento vs. Diferencial}

Consideremos dos puntos $P=\begin{bmatrix} P_x & P_y \end{bmatrix}$ y $Q=\begin{bmatrix} Q_x & Q_y \end{bmatrix}$ que pertenecen al gráfico de una curva plana.
Una curva se puede interpretar como el gráfico de $f:\setR \longrightarrow \setR \tq f(x)=y$.

Realizando la resta vectorial $Q-P$ queda el segmento de recta secante que pasa por estos dos puntos.
\begin{equation*}
    Q-P = \begin{bmatrix} Q_x-P_x & Q_y-P_y \end{bmatrix}
\end{equation*}

Podemos determinar que uno de los dos puntos de la diferencia quede fijo y el otro sea variable pudiendo tomar cualquier valor de la curva.
\begin{gather*}
    \textrm{Sean} \quad
    Q = \begin{bmatrix}x&y\end{bmatrix} \quad P = \begin{bmatrix}x_0&y_0\end{bmatrix}
    \\
    \begin{bmatrix}Q_x-P_x & Q_y-P_y\end{bmatrix} = \begin{bmatrix}x-x_0 & y-y_0\end{bmatrix}
\end{gather*}

Cada componente de esta diferencia entre vectores se conoce como el incremento en las variables $x$ e $y$ y se denota de la siguiente manera:
\begin{align*}
    \Delta x &= x-x_0
    \\
    \Delta y &= y-y_0
\end{align*}

Analicemos la relación entre las variables $x$ e $y$ de la función $y(x)$ cuyo gráfico genera la curva.
Haciendo una aproximación lineal de $y(x)$, que en el gráfico sería la recta tangente, se tiene:
\begin{align*}
    y(x) &= R(x)+E(x)
    \\
    y(x) &= y(x_0) + \frac{\dif}{\dif x} y(x_0) \, \Delta x +E(x)
    \\
    \underbrace{y(x)}_y - \underbrace{y(x_0)}_{y_0} &= \frac{\dif}{\dif x} y(x_0) \, \Delta x +E(x)
    \\
    \Delta y &= \frac{\dif}{\dif x} y(x_0) \, \Delta x +E(x)
\end{align*}

Si hacemos que el punto $Q$ tienda a estar cerca de $P$ de manera que $x \to x_0$, entonces $\Delta x \to 0$.
En tal caso, según el teorema de Taylor, el error tiende a ser nulo quedando:
\begin{equation*}
    \Delta y = \frac{\dif}{\dif x} y(x_0) \, \Delta x \quad \textrm{con} \hspace{1ex} \Delta x \to 0
\end{equation*}

Así definimos el diferencial $\dif y$ como la transformación lineal que mejor aproxima al incremento $\Delta y$:
\begin{gather*}
    \dif y(x) = \dfrac{\dif}{\dif x} y(x_0) \, \Delta x
    \\
    \textrm{Donde} \hspace{1ex} \Delta x \to 0 \Rightarrow \Delta y \approx \dif y(x)
\end{gather*}

Notar que solo en el caso en que $\Delta x \to 0$ se puede afirmar que $\Delta x = \dif x$ siempre y cuando $y(x)$ sea diferenciable.


\section{Diferenciabilidad de funciones de 1 variable}

Dada $f: \setR \longrightarrow \setR \tq f(x)=y$ derivable en $x_0$ cuya aproximación lineal de Taylor es:
\begin{align*}
    T(x) &= f(x_0)+\frac{\dif}{\dif x} f(x_0) \left(x-x_0\right)
    \\
    T(x_0 + \Delta x) &= f(x_0)+\frac{\dif}{\dif x} f(x_0) \, \Delta x
\end{align*}

Al reconstruir la función por el teorema de Taylor se tiene:
\begin{equation*}
    f(x_0 + \Delta x) = \underbrace{f(x_0)+\frac{\dif}{\dif x} f(x_0) \, \Delta x}_{T(x_0 + \Delta x)} + E(x_0 + \Delta x)
\end{equation*}

Restando $f(x_0)$ y dividiendo por $\Delta x$ en ambos miembros:
\begin{equation*}
    \frac{f(x_0 + \Delta x)-f(x_0)}{\Delta x} = \frac{\dif}{\dif x} f(x_0) + \frac{E(x_0 + \Delta x)}{\Delta x}
\end{equation*}

Tomando el límite cuando $\Delta x \to 0$ se tiene:
\begin{multline*}
    \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x)-f(x_0)}{\Delta x} =
    \\
    = \lim_{\Delta x \to 0} \braces{ \frac{\dif}{\dif x} f(x_0) } + \lim_{\Delta x \to 0} \frac{E(x_0 + \Delta x)}{\Delta x}
\end{multline*}

De un lado de la igualdad se tiene la definición de derivada en el punto, que se puede reemplazar por la derivada en el punto porque la función era derivable por hipótesis.
Del otro lado se tiene el límite de una constante más el error.
Como la expresión entre corchetes es la derivada evaluada en el punto, para que se cumpla la igualdad el error debe tender a cero:
\begin{equation*}
    \lim_{\Delta x \to 0} \frac{E(x_0 + \Delta x)}{\Delta x} = 0
\end{equation*}

Por lo tanto, si una función de una variable es derivable, entonces es diferenciable.


\section{Diferenciabilidad de funciones de 2 variables}

Que una función de dos variables sea diferenciable implica que su gráfica sea suave.
Es decir, que no tenga picos, simas, esquinas ni dobleces.

Las funciones de 2 o más variables tienen varias derivadas parciales.
Por lo tanto, por más que una función sea derivable no significa que sea continua y que exista una buena aproximación tangente.

Dada $f: \setR^2 \longrightarrow \setR \tq f(x,y)=z$ derivable en $(x_0,y_0)$ cuya aproximación lineal de Taylor es:
\begin{multline*}
    T(x,y) =
    \\
    = f(x_0,y_0) + \frac{\partial}{\partial x} f(x_0) \left( x-x_0 \right) + \frac{\partial}{\partial y} f(y_0) \left( y-y_0 \right)
\end{multline*}
\begin{multline*}
    T(x_0 + \Delta x,y_0 + \Delta y) =
    \\
    = f(x_0,y_0) + \frac{\partial}{\partial x} f(x_0) \, \Delta x + \frac{\partial}{\partial y} f(y_0) \, \Delta y
\end{multline*}

La función reconstruida, según el teorema de Taylor, es:
\begin{multline*}
    f(x_0 + \Delta x,y_0 + \Delta y) =
    \\
    = \underbrace{f(x_0,y_0) + \frac{\partial}{\partial x} f(x_0) \, \Delta x + \frac{\partial}{\partial y} f(y_0) \, \Delta y}_{T(x_0 + \Delta x,y_0 + \Delta y)} +
    \\
    + E(x_0 + \Delta x,y_0 + \Delta y)
\end{multline*}

Restando el plano tangente, se tiene:
\begin{multline*}
    f(x_0 + \Delta x,y_0 + \Delta y) - T(x_0 + \Delta x,y_0 + \Delta y) =
    \\
    = E(x_0 + \Delta x,y_0 + \Delta y)
\end{multline*}

En este caso el incremento es vectorial, y si bien no podemos dividir por un vector podemos dividir por su norma, ya que no nos interesa la dirección si no la magnitud.
\begin{multline*}
    \frac{f(x_0 + \Delta x,y_0 + \Delta y) - T(x_0 + \Delta x,y_0 + \Delta y)}{\nnorm{(\Delta x,\Delta y)}} =
    \\
    = \frac{E(x_0 + \Delta x,y_0 + \Delta y)}{\nnorm{(\Delta x,\Delta y)}}
\end{multline*}

El límite cuando $(\Delta x,\Delta y) \to (0,0)$ es:
\begin{multline*}
    \lim_{\substack{\Delta x \to 0 \\ \Delta y \to 0}} \frac{f(x_0 + \Delta x,y_0 + \Delta y) - T(x_0 + \Delta x,y_0 + \Delta y)}{\nnorm{(\Delta x,\Delta y)}}
    \\
    = \lim_{\substack{\Delta x \to 0 \\ \Delta y \to 0}} \frac{E(x_0 + \Delta x,y_0 + \Delta y)}{\nnorm{(\Delta x,\Delta y)}}
\end{multline*}

En el numerador del miembro izquierdo no necesariamente se tiene una resta de dos cosas iguales.
Por lo tanto el miembro derecho no necesariamente es cero como pasaba en funciones de una variable.
Para que $f(x,y)$ sea diferenciable, y el error de la aproximación sea infinitésimo, hay que demostrar que:
\begin{equation*}
    \lim_{\substack{\Delta x \to 0 \\ \Delta y \to 0}} \frac{f(x_0 + \Delta x,y_0 + \Delta y) - T(x_0 + \Delta x,y_0 + \Delta y)}{\nnorm{(\Delta x,\Delta y)}} = 0
\end{equation*}

O lo que es lo mismo haciendo el cambio de variables $(\Delta x,\Delta y)=(x-x_0,y-y_0)$ se tiene que una función de dos variables $f:\setR^2 \longrightarrow \setR$ es diferenciable solo si se cumple lo que se conoce como condición de tangencia:
\begin{equation*}
    \lim_{\substack{x \to x_0 \\ y \to y_0}} \frac{f(x,y) - T(x,y)}{\nnorm{(x-x_0,y-y_0)}} = 0
\end{equation*}

Siendo el plano tangente $T(x,y)$ una buena aproximación lineal dada por:
\begin{multline*}
    T(x,y) =
    \\
    = f(x_0,y_0) + \frac{\partial}{\partial x} f(x_0) \left(x-x_0\right) + \frac{\partial}{\partial y} f(y_0) \left(y-y_0\right)
\end{multline*}


\section{Definición general de diferenciabilidad}

Dada $f:\setR^\nth \longrightarrow \setR$ contínua y derivable en $\Vec{x}_0$.

Sean $\Delta \Vec{x}$ el vector incremento:
\begin{equation*}
    \Delta \Vec{x} = \begin{bmatrix} x_1-x_{0_1} & x_2-x_{0_2} & \dots & x_\nth-x_{0_\nth} \end{bmatrix}
\end{equation*}

Y $T(\Vec{x})$ la aproximación lineal a fin:
\begin{equation*}
    T(\Vec{x}) = f(\Vec{x}) + \grad f(\Vec{x}_0) \cdot \Delta \Vec{x}
\end{equation*}

Se tiene que $f(\Vec{x})$ es diferenciable en $\Vec{x}_0$ solo si:
\begin{equation*}
    \lim_{\Vec{x} \to \Vec{x}_0} \frac{f(\Vec{x})-T(\Vec{x})}{\nnorm{\Delta \Vec{x}}} = 0
\end{equation*}

Las siguientes proposiciones sirven para demostrar si una función es o no diferenciable.

\begin{itemize}
    \item Si ambas funciones derivadas parciales son continuas en un punto, entonces la función es diferenciable en el punto.
    \item Si alguna de las funciones derivadas parciales no es continua en un punto, entonces no sabemos si la función es o no es diferenciable en ese punto.
    \item Si la función es diferenciable en un punto, entonces la misma es continua en ese punto.
    \item Si la función no es continua en un punto, entonces la misma no es diferenciable en ese punto.
    \item Si la función es diferenciable en un punto, entonces la misma es derivable en ese punto.
    \item Si la función no es derivable en un punto, entonces la misma no es diferenciable en ese punto.
\end{itemize}

Y, definiendo la clase de una función de manera que:
\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Clase}
    \cusTe{Una función de clase $\class[\kth]$ tiene derivadas de orden $\kth$ contínuas.}
\end{mdframed}

Pueden ser resumidas en la siguiente propiedad.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Lógica de funciones diferenciables}
    \begin{gather*}
        f(\Vec{x}_0) \hspace{1ex} \textrm{es de clase} \hspace{1ex} \class
        \\
        \Downarrow
        \\
        f(\Vec{x}_0) \hspace{1ex} \textrm{es diferenciable}
        \\
        \Downarrow
        \\
        f(\Vec{x}_0) \hspace{1ex} \textrm{es contínua}
        \land
        f(\Vec{x}_0) \hspace{1ex} \textrm{es derivable}
    \end{gather*}
\end{mdframed}


\section{Aplicaciones del gradiente}

La derivada direccional es la proyección del vector gradiente sobre un versor $\versor{r}$.
Si $f(\Vec{x})$ es diferenciable en $\Vec{x}_0$, podemos calcular la derivada multiplicando el vector gradiente $\grad f(\Vec{x})$ por el versor director $\versor{r}$.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Derivada direccional}
    \begin{equation*}
        \frac{\partial}{\partial \versor{r}} f(\Vec{x}) = \grad f(\Vec{x}) \cdot \versor{r} = \nnorm{ \grad f(\Vec{x}) } \cos{(\theta)}
    \end{equation*}
\end{mdframed}

El gradiente de una función diferenciable pertenece al mismo espacio que pertenece el dominio de la función.
La dirección del gradiente tiene la particularidad de ser la de mayor crecimiento de la función.
Que el gradiente de la función sea igual al versor director implica y está implicado por que la derivada direccional en la dirección del versor es la mayor de todas las direcciones posibles.
Esto es, la derivada máxima de la sucesión de derivadas direccionales.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Máximo crecimiento de la función}
    \begin{equation*}
        \grad f(\Vec{x}) = \versor{r} \iff \frac{\partial}{\partial \versor{r}} f(\Vec{x}) = \operatorname{max} \braces{ a_{\versor{r}} }
    \end{equation*}
    \noTi{Donde:}
    \begin{gather*}
        \braces{ a_{\versor{r}} } = \braces{ \frac{\partial}{\partial \versor{r}} f(\Vec{x}) }
        \\
        \versor{r} = (a,b) \tq \sqrt{a^2 + b^2}=1
    \end{gather*}
\end{mdframed}

Podemos demostrar la no diferenciabilidad de una función si las derivadas direccionales tienen una relación no lineal según cambie el versor director.
Esto es, que para alguna dirección la función crezca o disminuya con un orden mayor que para el resto de las direcciones.

Observar que la derivada direccional es, usando la definición del gradiente, una multiplicación entre dos vectores y las componentes de ambos son números, entonces siempre vamos a tener una combinación lineal.
Si calculamos la derivada direccional por definición usando límite y este nos arroja un resultado no lineal, entonces la función no es diferenciable.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Condición de linealidad de la derivada}
    \begin{equation*}
        f \hspace{1ex} \textrm{es diferenciable} \Rightarrow \grad f(\Vec{x}) \cdot \versor{r} = \sum_{\ith=1}^\nth r_i \, \frac{\partial}{\partial x_\ith} f(\Vec{x}_0)
    \end{equation*}
\end{mdframed}

Para el caso particular $f:\setR^2 \longrightarrow \setR$ se tiene:
\begin{align*}
    \grad f(\Vec{x}) \cdot \versor{r} &= a \, \underbrace{\dfrac{\partial}{\partial x} f(x_0,y_0)}_{\lambda_1} + b \, \underbrace{\dfrac{\partial}{\partial y} f(x_0,y_0)}_{\lambda_2}
    \\
    \frac{\partial}{\partial \versor{r}} f(\Vec{x}) &= a\,\lambda_1 + b\,\lambda_2 \iff \textrm{es combinación lineal}
\end{align*}


\section{Teorema de Dini} % Teorema de la función implícita

Sea $f:D \subseteq \setR^2 \longrightarrow \setR$ con $D$ abierto, sea $f \in \mathcal{C}^1$, sea $(x_0,y_0) \in E = \braces{ (x,y) \in \setR^2 \tq f(x,y)=0 }$.

Luego:
\begin{equation*}
    \exists ! \hspace{1ex} F:E_{x_o, \delta} \subseteq \setR \longrightarrow E_{y_o, \varepsilon} \subseteq \setR \tq F(x)=y
\end{equation*}

Con:
\begin{equation*}
    F(x) \in \mathcal{C}^1(E_{x_o, \delta}) \land f \big( x,F(x) \big) = 0
\end{equation*}

Tal que:
\begin{equation*}
    \frac{\partial}{\partial x} F(x_0) = - \dfrac{\dfrac{\partial}{\partial x} \, f \left( x_0,F(x_0) \right)}{\dfrac{\partial}{\partial y} \, f \left( x_0,F(x_0) \right)}
\end{equation*}

\concept{Demostración}

Con el fin de deducir la fórmula de $\tfrac{\partial}{\partial x} F(x_0)$ para funciones $f:\setR^2 \longrightarrow \setR$, definimos el diferencial $\dif f(x,y)$:
\begin{equation*}
    \dif f(x,y) = \dfrac{\partial}{\partial x} f(x_0,y_0) \cdot \Delta x + \dfrac{\partial}{\partial y} f(x_0,y_0) \cdot \Delta y
\end{equation*}

La función $f(x,y)$ está igualada a cero por hipótesis.
Es decir que no hay variación en la imagen de $f(x,y)$, como si se tratara de un conjunto de nivel.
Por lo tanto, el diferencial $\dif f(x,y)$ es nulo:
\begin{equation*}
    0 = \dfrac{\partial}{\partial x} f(x_0,y_0) \cdot \Delta x + \dfrac{\partial}{\partial y} f(x_0,y_0) \cdot \Delta y
\end{equation*}

Dividiendo por el incremento $\Delta x$ se tiene:
\begin{equation*}
    0 = \dfrac{\partial}{\partial x} f(x_0,y_0)+ \dfrac{\partial}{\partial y} f(x_0,y_0) \dfrac{\Delta y}{\Delta x}
\end{equation*}

Como el Teorema de Dini se aplica en un entorno cercano a $(x_0,y_0)$, se tiene que $\Delta x \to 0$ y podemos reemplazar los incrementos por diferenciales:
\begin{equation*}
    0 = \dfrac{\partial}{\partial x} f(x_0,y_0)+ \dfrac{\partial}{\partial y} f(x_0,y_0) \dfrac{\partial y}{\partial x}
\end{equation*}

Observar que no se puede simplificar los diferenciales $\partial y$ por más que estén multiplicando y dividiendo, porque $y$ es en realidad una función de $x$.
Esta función $y(x)$ es la que se enuncia en el teorema como $F(x)$, cuya fórmula no conocemos, pero que en torno al punto $x_0$ es derivable.
Definiendo la ecuación anterior correctamente y despejando $F(x_0)$ se tiene la fórmula de derivación implícita:
\begin{equation*}
    0 = \frac{\partial}{\partial x} f(x_0,y_0)+ \frac{\partial}{\partial y} f(x_0,y_0) \, \frac{\partial}{\partial x} F(x_0)
\end{equation*}

\concept{Generalización del teorema}

Sea $\Vec{f}:D \subseteq \setR^\nth \longrightarrow \setR^\mth$ con $D$ abierto y sea $\Vec{f} \in \mathcal{C}^\infty$, lo cual implica:
\begin{equation*}
    \left\{
    \begin{aligned}
        f_1 \big( x_1,x_2 \dots x_\nth, F_1(\Vec{x}), F_2(\Vec{x}) \dots F_\mth(\Vec{x}) \big) &= 0
        \\
        f_2 \big( x_1,x_2 \dots x_\nth, F_1(\Vec{x}), F_2(\Vec{x}) \dots F_\mth(\Vec{x}) \big) &= 0
        \\
        \vdots \hspace{2.7cm} &
        \\
        f_\mth \big( x_1,x_2 \dots x_\nth, F_1(\Vec{x}), F_2(\Vec{x}) \dots F_\mth(\Vec{x}) \big) &= 0
    \end{aligned}
    \right.
\end{equation*}

Y así mismo verifica:
\begin{equation*}
    \operatorname{det}
    \begin{bmatrix}
        \dfrac{\partial f_1}{\partial F_1} & \cdots & \dfrac{\partial f_1}{\partial F_m}
        \\
        \vdots & \ddots & \vdots
        \\
        \dfrac{\partial f_m}{\partial F_1} & \cdots & \dfrac{\partial f_m}{\partial F_m}
    \end{bmatrix}
    \neq 0
\end{equation*}

Por lo tanto:
\begin{equation*}
    \exists ! \hspace{1ex} \Vec{F}: B_{x_0,\delta} \subseteq \setR^\nth \longrightarrow B_{y_0,\varepsilon} \subseteq \setR^\mth
\end{equation*}

Con:
\begin{equation*}
    \Vec{F} \in \mathcal{C}^1(E_{x_0, \delta}) \land \Vec{f}(\Vec{x}, \Vec{F}(\Vec{x}))=0
\end{equation*}


\chapter{Extremos de funciones reales}


El conjunto de puntos críticos es la unión de los conjuntos de puntos estacionarios y puntos de no diferenciabilidad.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Puntos críticos}
    \begin{multline*}
        A = \braces{ \Vec{x} \in \setR^\nth \tq \grad f(\Vec{x}) = \Vec{0} }
        \\
        \cup \braces{ \Vec{x} \in \setR^\nth \tq f(\Vec{x}) \hspace{1ex} \textrm{no es diferenciable} }
    \end{multline*}
\end{mdframed}

Los puntos críticos son los elementos del dominio que pueden ser un máximo, un mínimo o un punto de ensilladura.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Puntos críticos}
    \begin{gather*}
        \Vec{x}_0 \hspace{1ex} \textrm{es extremo} \lor \Vec{x}_0 \hspace{1ex} \textrm{es punto de ensilladura}
        \\
        \Rightarrow \Vec{x}_0 \hspace{1ex} \textrm{es un punto crítico.}
    \end{gather*}
\end{mdframed}

Un extremo local o relativo evaluado en la función toma el valor más alto o más bajo de la imagen, en el entorno del punto.
Un extremo absoluto o global es un máximo o mínimo no solo para su entorno, sino también para todos los valores que tome la función en el resto del dominio.

\begin{itemize}
    \item Mínimo local: $f(\Vec{x}_0)<f(\Vec{x}) \quad \forall \hspace{1ex} \Vec{x} \in B(\Vec{x}_0, \delta)$

    \item Máximo local: $f(\Vec{x}_0)>f(\Vec{x}) \quad \forall \hspace{1ex} \Vec{x} \in B(\Vec{x}_0, \delta)$

    \item Mínimo absoluto: $f(\Vec{x}_0)<f(\Vec{x}) \quad \forall \hspace{1ex} \Vec{x} \in \operatorname{Dn}(f)$

    \item Máximo absoluto: $f(\Vec{x}_0)>f(\Vec{x}) \quad \forall \hspace{1ex} \Vec{x} \in \operatorname{Dn}(f)$
\end{itemize}

Si $\Vec{x}_0 \in \operatorname{Dn}(f)$ es un punto crítico y no es un extremo, entonces es un punto de ensilladura.

Si trasladamos la gráfica de manera que en el punto en cuestión la imagen sea nula, las secciones de la función en un punto silla son, o bien funciones cúbicas donde la función cambia de signo, o bien parábolas donde en una dirección la parábola es cóncava hacia arriba y en otra hacia abajo.


\section{Método de Sylvester}

Este método para clasificar puntos críticos conciste en calcular el determinante de la matriz Hessiana de la función.
\begin{equation*}
    \left\{
    \begin{aligned}
        \operatorname{det}(H) < 0 & \Rightarrow \Vec{x}_0 \hspace{1ex} \textrm{es punto de ensilladura}
        \\
        \operatorname{det}(H) = 0 &\Rightarrow \hspace{1ex} \textrm{el criterio no aplica}
        \\
        \operatorname{det}(H) > 0 &\Rightarrow \Vec{x}_0 \hspace{1ex} \textrm{es extremo}
    \end{aligned}
    \right.
\end{equation*}

En el último caso podemos determinar si el extremo se trata de un máximo o un mínimo estudiando la derivada segunda.
\begin{equation*}
    \left\{
    \begin{aligned}
        \frac{\partial^2}{\partial x_1^2} f(\Vec{x}_0) < 0 &\Rightarrow \Vec{x}_0 \hspace{1ex} \textrm{es máximo}
        \\[1em]
        \frac{\partial^2}{\partial x_1^2} f(\Vec{x}_0) > 0 &\Rightarrow \Vec{x}_0 \hspace{1ex} \textrm{es mínimo}
    \end{aligned}
    \right.
\end{equation*}


\section{Extremos Condicionados}

El método por multiplicadores de Lagrange sirve para buscar extremos de la función para un conjunto restringido.

Dada $f:D_1 \subseteq \setR^\nth \longrightarrow \setR$ restringida a $g:D_2 \subseteq \setR^\nth \longrightarrow \setR \tq g(\Vec{x})=0 \land \grad g(\Vec{x}) \neq \Vec{0}$ se puede construir la función $L$ que verifica:
\begin{equation*}
    L:D_3 \subseteq \setR^{\nth+1} \longrightarrow \setR \tq L(\Vec{x}, \lambda) = f(\Vec{x}) - \lambda g(\Vec{x})
\end{equation*}

Siendo los candidatos a extremos o puntos de ensilladura de $f$ los mismos puntos críticos de $L$.
Por lo tanto, si $f$ y $g$ son campos escalares de dos variables, se tiene que:
\begin{gather*}
    \grad L(x,y,\lambda)=0 \iff
    \\[1em]
    \left\{
    \begin{aligned}
        \frac{\partial}{\partial x} f(x,y) - \lambda \frac{\partial}{\partial x} g(x,y) &= 0
        \\[1ex]
        \frac{\partial}{\partial y} f(x,y) - \lambda \frac{\partial}{\partial y} g(x,y) &= 0
        \\[1ex]
        \frac{\partial}{\partial \lambda} f(x,y) - \lambda \frac{\partial}{\partial \lambda} g(x,y) &= 0
    \end{aligned}
    \right.
    \\[1em]
    \left\{
    \begin{aligned}
        \frac{\partial}{\partial x} f(x,y) &= \lambda \frac{\partial}{\partial x} g(x,y)
        \\[1ex]
        \frac{\partial}{\partial y} f(x,y) &= \lambda \frac{\partial}{\partial y} g(x,y)
        \\[1ex]
        g(x,y) &= 0
    \end{aligned}
    \right.
\end{gather*}


\chapter{Geometría diferencial}


\section{Curvas}


Una curva puede ser entendida como un camino que a priori es recto pero instante a instante va cambiando de dirección, doblándose conforme es recorrido.

Las curvas pueden estar dadas en 2 dimensiones contenidas en un plano o pueden darse en las 3 dimensiones espaciales.

Si tuviesemos un hilo tirado en el piso o un alambre doblado colgando, ambos de grosor nulo, podríamos referirnos al espacio que ocupan como curvas en dos y tres dimensiones respectivamente.

Una curva puede ser definida mediante una ecuación paramétrica o una ecuación implícita.
Geométricamente, ambas formas de representación describen lo mismo.
Pero la ecuación paramétrica indica cómo la curva es ``recorrida'', mientras que la ecuación implícita indica cuáles son los puntos que la delimitan.


\subsection{Ecuación paramétrica}

La ecuación paramétrica de una curva es una función vectorial que toma valores reales y entrega vectores de 2 o 3 coordenadas.
A medida que el parámetro $t$ aumenta, la imagen de la función va trazando los puntos del recorrido.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Curva en el plano}
    \begin{equation*}
        \Vec{c}: \setR \longrightarrow \setR^2 \tq \Vec{c}(t) = \begin{bmatrix} x(t) & y(t) \end{bmatrix}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Curva en el espacio}
    \begin{equation*}
        \Vec{c}: \setR \longrightarrow \setR^3 \tq \Vec{c}(t) = \begin{bmatrix} x(t) & y(t) & z(t) \end{bmatrix}
    \end{equation*}
\end{mdframed}

Puede haber más de una parametrización correcta para una misma curva.


\subsection{Ecuación implícita}

Las ecuaciones implícitas de una curva delimitan los puntos que la componen.
Indican la geometría que tiene el conjunto ($C$) de puntos que conforman la curva en un plano o en el espacio.

Una curva plana puede estar dada de manera implícita como la gráfica de funciones escalares o como el conjunto de nivel de un campo escalar de 2 variables.

\concept{Gráfica de funciones escalares}
\begin{gather*}
    \textrm{Dada} \hspace{1ex} f:\setR \longrightarrow \setR \tq f(x)=y
    \\
    C = \braces{ (x,y) \in \setR^2 \tq f(x)-y=0 }
\end{gather*}

\concept{Curva de nivel de un campo escalar}
\begin{gather*}
    \textrm{Dada} \hspace{1ex} f:\setR^2 \longrightarrow \setR \tq f(x,y)=k
    \\
    C = \braces{ (x,y) \in \setR^2 \tq f(x,y)-k=0 }
\end{gather*}

Una curva en el espacio puede resultar de la intersección de superficies.
En este caso se cumplen simultáneamente las ecuaciones de un sistema que expresa, en cada ecuación, cada una de las superficies que se intersectan.

\concept{Intersección entre superficies}
\begin{gather*}
    C = \{ (x,y,z) \in \setR^3 \tq S_1 \cap S_2 \ldots \cap S_k \}
    \\[1ex]
    \left\{
    \begin{aligned}
        & S_1 (x,y,z)=0
        \\
        & S_2 (x,y,z)=0
        \\
        & \hspace{1.3cm} \vdots
        \\
        & S_k (x,y,z)=0
    \end{aligned}
    \right.
\end{gather*}

\subsection{Parametrización de la implícita}

La imagen de la ecuación paramétrica de una curva son los puntos de dicho plano o espacio que están delimitados por la ecuación implícita.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    La parametrización $\Vec{c}$ es solución de la ecuación implícita $C$.
    \begin{equation*}
        \im (\Vec{c}) \subseteq C
    \end{equation*}
\end{mdframed}

Algunas parametrizaciones no arrojan exactamente todos los puntos de la ecuación implícita, ya que pueden tener discontinuidades numerables.
Para una única ecuación implícita, es posible encontrar varias parametrizaciones que sean soluciónes de la misma.
Para cada parametrización, existe al menos una ecuación implícita que eventualmente se puede calcular.

\concept{Parametrización de curvas en un plano}

Si se puede poner una variable en función de la otra, entonces existe una parametrización.

\concept{Parametrización de curvas en el espacio}

Primero se proyecta la curva sobre alguno de los planos coordenados, de manera tal que esa proyección sea una curva conocida.
Luego, se despeja la coordenada no tenida en cuenta en la proyección anterior.

% Agregar vectores tangente y normal. c'(t) es tangente, pero ¿c''(t) es normal?

\subsection{Clasificación de curvas}

Una curva cerrada es aquella que no tiene principio ni fin.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Curva cerrada}
    \begin{equation*}
        \Vec{c}: [t_0;t_1] \subset \setR \longrightarrow \setR^2 \tq \Vec{c}(t_0) = \Vec{c}(t_1)
    \end{equation*}
\end{mdframed}

Una curva simple no se corta a sí misma.
Una curva es simple si tiene una parametrización inyectiva.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Curva simple}
    \begin{equation*}
        t_1 \neq t_2 \Rightarrow \Vec{c}(t_1) \neq \Vec{c}(t_2)
    \end{equation*}
\end{mdframed}

Por definición, si una parametrización es simple, la implícita también lo es.

Una curva regular tiene una parametrización diferenciable.
Una curva es regular si tiene una parametrización cuya derivada es no nula para todos los puntos del dominio.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Curva regular}
    \begin{equation*}
        \frac{\dif}{\dif t} \Vec{c}(t) \neq 0
    \end{equation*}
\end{mdframed}

Por definición, si una parametrización es regular, la implícita también.

Aquellos puntos en los que la curva no es regular se conocen como puntos singulares.
Los puntos singulares son puntos en los que una curva presenta una anomalía, que puede implicar que no sea simple (y tenga un punto de cruce), que tenga un punto cuspidal, que tenga un punto de estrangulación (o tacnodo) o que tenga un punto aislado.

Así como el gradiente de un plano nos da los coeficientes de la ecuación del plano, todas las superficies son caracterizadas por su gradiente.

Si hacemos el producto vectorial entre los gradientes de las ecuaciones implícitas de dos superficies que se intersectan, estaremos calculando un vector que es ortogonal a ambos gradientes.

Supongamos que la intersección de esas superficies es una curva.
Si en algún punto este producto vectorial se anula $\grad S_1 \times \grad S_2 = (0,0,0)$, significa que la curva tiene versor normal nulo y por lo tanto que no es regular.


\section{Catálogo de curvas planas}

\concept{Secciones cónicas}

Las secciones cónicas son curvas planas que resultan de la intersección de un cono con un plano inclinado.
La curva resultante va a depender de la inclinación del plano y el radio del cono, entre otros parámetros.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Circunferencia}
    \begin{gather*}
        \left(x-x_0\right)^2 + \left(y-y_0\right)^2 = r^2
        \\[1em]
        \Vec{c}:[0;2 \pi) \subset \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t)= \begin{bmatrix} x_0 + r \cos{(t)} & y_0 + r \sin{(t)} \end{bmatrix}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elipse}
    \begin{gather*}
        \frac{(x-x_0)^2}{a^2}+\dfrac{(y-y_0)^2}{b^2}=1
        \\[1em]
        \Vec{c}:[0;2 \pi) \subset \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t)= \begin{bmatrix} x_0 + a\cos{(t)} & y_0 + b\sin{(t)} \end{bmatrix}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Parábola horizontal}
    \begin{gather*}
        \left(y-y_0\right)^2 = 2p\left(x-x_0\right)
        \\[1em]
        \Vec{c}:\setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t) = \begin{bmatrix} x_0 + \dfrac{t^2}{2p} & y_0 + t \end{bmatrix}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Parábola vertical}
    \begin{gather*}
        \left(x-x_0\right)^2=2p\left(y-y_0\right)
        \\[1em]
        \Vec{c}:\setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t) = \begin{bmatrix} x_0 + t & y_0 + \dfrac{t^2}{2p} \end{bmatrix}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Hipérbola}
    \begin{gather*}
        \dfrac{(x-x_0)^2}{a^2}-\dfrac{(y-y_0)^2}{b^2}=1
        \\[1em]
        \Vec{c}_1:[0;2 \pi) \subset \setR \longrightarrow \setR^2
        \\
        \Vec{c}_1(t) = \begin{bmatrix} x_0 + a\cosh{(t)} & y_0 + b\sinh{(t)} \end{bmatrix}
        \\[1em]
        \Vec{c}_2:\left[-\tfrac{\pi}{2}; \tfrac{3}{2} \pi\right) \subset \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}_2(t)= \begin{bmatrix} x_0 + a\tan{(t)} & y_0 + b\sec{(t)} \end{bmatrix}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Hipérbola conjugada}
    \begin{gather*}
        \dfrac{(x-x_0)^2}{a^2}-\dfrac{(y-y_0)^2}{b^2}=-1
        \\[1em]
        \Vec{c}:[0;2 \pi) \subset \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t)= \begin{bmatrix} x_0 + a\sinh{(t)} & y_0 + b\cosh{(t)} \end{bmatrix}
    \end{gather*}
\end{mdframed}

\concept{Secciones cónicas degeneradas}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Recta}
    \begin{gather*}
        \dfrac{(x-x_0)}{a}=\dfrac{(y-y_0)}{b}
        \\[1em]
        \Vec{c}: \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t) = \begin{bmatrix} x_0 + at & y_0 + bt \end{bmatrix}
    \end{gather*}
\end{mdframed}

\concept{Espirógrafos}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Cardioide}
    \begin{gather*}
        \left( (x-x_0)^2 - 2a(x-x_0) + (y-y_0)^2 \right)^2 = \\
        = \left( 2a(x-x_0) \right)^2 + \left( 2a(y-y_0) \right)^2
        \\[1em]
        \Vec{c}:[0;2 \pi) \subset \setR \longrightarrow \setR^2 \tq
        \\
        \Vec{c}(t) = \left\{
        \begin{aligned}
            x(t) &= x_0 + \cos{(t)} \big( a\cos{(t)} \big) \\
            y(t) &= y_0 + \sin{(t)} \big( a\cos{(t)} \big)
        \end{aligned}
        \right.
    \end{gather*}
\end{mdframed}


\section{Superficies}

Una superficie puede ser entendida como un plano que se ``arruga'' a medida que sus dos vectores directores lo recorren.
Formalmente, decimos que una superficie es una inmersión que transforma el plano $\setR^2$ y lo aplica en el espacio $\setR^3$.

Las superficies generalmente están dadas en 3 dimensiones, aunque un plano de 2 dimensiones es un caso particular de una superficie.

Si tuviesemos una hoja de papel arrugada, una sábana suspendida en el aire, un maple de huevos, o una carpa de camping, todos de grosor nulo, podriamos referirnos al espacio que ocupan como superficies dadas en tres dimensiones.


\subsection{Ecuación paramétrica}

Una diferencia con las curvas, es que las superficies requieren de dos parametros para ser recorridas completamente.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Superficie}
    \begin{multline*}
        \Vec{s}: \setR^2 \longrightarrow \setR^3 \tq
        \\
        \Vec{s}(u,v) = \begin{bmatrix} x(u,v) & y(u,v) & z(u,v) \end{bmatrix}
    \end{multline*}
\end{mdframed}


\subsection{Ecuación Implícita}

Las ecuaciónes implícitas de una superficie delimitan los puntos que la componen.
Indican la geometría que tiene el conjunto ($S$) de puntos que conforman la superficie.

Una superficie puede estar dada de manera implícita como la gráfica de un campo escalar de 2 Variables, como el conjunto de nivel de un campo escalar de 3 variables, y como la intersección de sólidos y superficies.

\concept{Gráfica de campos escalares}
\begin{gather*}
    \textrm{Dada} \hspace{1ex} f:\setR^2 \longrightarrow \setR \tq f(x,y)=z
    \\
    S= \braces{ (x,y,z) \in \setR^3 \tq f(x,y)-z=0 }
\end{gather*}

\concept{Conjunto de nivel de un campo escalar}
\begin{gather*}
    \textrm{Dada} \hspace{1ex}  f:\setR^3 \longrightarrow \setR \tq f(x,y,z)=k
    \\
    S= \braces{ (x,y,z) \in \setR^3 \tq f(x,y,z)-k=0 }
\end{gather*}

\concept{Intersección entre sólidos y superficies}

Una superficie puede resultar de la intersección entre solidos, superficies, curvas y cualquier conjunto de puntos en general.
En este caso, se cumple el sistema de ecuaciones e inecuaciones que expresan los sólidos, superficies o conjuntos que se intersecten.


\subsection{Parametrización de la implícita}

La imagen de la ecuación paramétrica de una superficie son los puntos del espacio que están delimitados por las ecuaciones implícitas.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    La parametrización $\Vec{s}$ es solución de las ecuaciones implícitas $S$.
    \begin{equation*}
        \im (\Vec{s}) \subseteq S
    \end{equation*}
\end{mdframed}

Al igual que ocurría con las curvas, la implícita puede representar puntos que la parametrización no alcanza a recorrer.


\subsection{Vectores tangente y normal}

Al haber dos parámetros, hay dos vectores tangentes linealmente independientes.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Vectores tangente}
    \begin{equation*}
        \Vec{\tang}_1 = \frac{\partial}{\partial u} \Vec{s}(u,v)
        \quad
        \Vec{\tang}_2 = \frac{\partial}{\partial v} \Vec{s}(u,v)
    \end{equation*}
\end{mdframed}

Los vectores tangentes se pueden hacer unitarios dividiendolos por su norma.

Al ser los vectores tangentes las derivadas parciales, su dirección no es cualquiera si no que entre si forman un ángulo recto.
Calculando el producto vectorial, se obtiene el vector que es normal a los vectores tangentes, y por lo tanto normal a la superficie.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Vector normal}
    \begin{equation*}
        \Vec{n} = \Vec{\tang}_1 \times \Vec{\tang}_2
    \end{equation*}
\end{mdframed}

Por otro lado, se puede calcular a partir de la ecuación implícita de una superficie.
Así como los coeficientes de la implícita de un plano son los componentes del vector normal, el gradiente de la ecuación implícita de una superficie es el vector normal a la misma.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Vector normal}
    \begin{equation*}
        \Vec{n} = \grad S
    \end{equation*}
\end{mdframed}


\subsection{Clasificación de superficies}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Superficie orientable}
    \cusTe{Una superficie es orientable cuando se pueden definir dos versores normales opuestos que la recorran.}
\end{mdframed}

Se dice que una superficie está orientada positivamente cuando el versor normal en un punto de la misma apunta hacia afuera.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Superficie simple}
    \cusTe{Una superficie es simple si tiene una parametrización inyectiva.}
\end{mdframed}

Una superficie es regular si es suave y no tiene picos, ni cimas, ni dobleces, ni esquinas.
Esto equivale a que el vector normal formado a partir del producto vectorial entre los vectores tangentes nunca se anule.
O lo que es lo mismo, que el rango de la matriz de $3 \times 2$ derivada de la ecuación paramétrica sea menor que dos.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Superficie regular}
    \begin{equation*}
        \Vec{n} \neq 0 \iff \operatorname{ran} \begin{bmatrix} \Vec{\tang}_1^{\,t} & \Vec{\tang}_2^{\,t} \end{bmatrix} <2
    \end{equation*}
\end{mdframed}


\section{Catálogo de superficies}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Plano}
    \begin{equation*}
        a\,x +b\,y +c\,z = d
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El área de un plano es igual a la norma del producto vectorial entre sus vectores directores, es decir la norma del vector normal.
\end{mdframed}

\concept{Superficies elípticas}

Las superficies elípticas no tienen simetría axial, pero son simétricas son respecto a un plano cartesiano.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Cono elíptico}
    \begin{equation*}
        \frac{(x-x_0)^2}{a^2} + \frac{(y-y_0)^2}{b^2} = \frac{(z-z_0)^2}{c^2}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elipsoide}
    \begin{gather*}
        \frac{(x-x_0)^2}{a^2} + \frac{(y-y_0)^2}{b^2} + \frac{(z-z_0)^2}{c^2} = 1
        \\[1em]
        \Vec{s}_1(t) = \left\{
        \begin{aligned}
            x(u,v) &= x_0 + a \cos{(u)} \sin{(v)} \\
            y(u,v) &= y_0 + b \sin{(u)} \sin{(v)} \\
            z(u,v) &= z_0 + c \cos{(v)}
        \end{aligned}
        \right.
        \\
        \textrm{Con} \hspace{1ex} 0 \leq u < 2 \pi \quad 0 \leq v \leq \pi
        \\[1ex]
        \Vec{s}_2(t) = \left\{
        \begin{aligned}
            x(u,v) &= x_0 + a \cos{(u)} \cos{(v)} \\
            y(u,v) &= y_0 + b \sin{(u)} \cos{(v)} \\
            z(u,v) &= z_0 + c \sin{(v)}
        \end{aligned}
        \right.
        \\
        \textrm{Con} \hspace{1ex} 0 \leq u < 2 \pi \quad -\dfrac{\pi}{2} \leq v \leq \dfrac{\pi}{2}
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Paraboloide elíptico}
    \begin{equation*}
        \dfrac{(x-x_0)^2}{a^2} + \dfrac{(y-y_0)^2}{b^2} = \dfrac{z-z_0}{c}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Paraboloide hiperbólico}
    \begin{equation*}
        \dfrac{(x-x_0)^2}{a^2} - \dfrac{(y-y_0)^2}{b^2} = \dfrac{z-z_0}{c}
    \end{equation*}
\end{mdframed}

\concept{Superficies de revolución:}

Las superficies de revolución son simétricas con respecto a un eje.
Se pueden concebir rotando media vuelta las curvas cónicas sobre dicho eje de simetría y luego uniendo las posiciones durante el giro.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Cono de revolución}
    \begin{equation*}
        (x-x_0)^2 + (y-y_0)^2 = m\left(z-z_0\right)^2
    \end{equation*}
\end{mdframed}

Intersectando un cono de revolución con un plano, se obtienen las secciones cónicas y secciones cónicas degeneradas.
Pero además, este cono se genera por revolución a partir de una recta.
Esta, a su vez, es una sección cónica degenerada, lo cual resulta un tanto paradójico.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Esfera}
    \begin{gather*}
        (x-x_0)^2 + (y-y_0)^2 + (z-z_0)^2 = r^2
        \\[1em]
        \Vec{s}(t) = \left\{
        \begin{aligned}
            x(u,v) &= x_0 + r \sin{(u)} \cos{(v)} \\
            y(u,v) &= y_0 + r \sin{(u)} \sin{(v)} \\
            z(u,v) &= z_0 + r \cos{(u)}
        \end{aligned}
        \right.
        \\
        \textrm{Con} \hspace{1ex} 0 \leq u < \pi \quad 0 \leq v < 2 \pi
    \end{gather*}
\end{mdframed}

Cualquiera de las parametrizaciones del elipsoide pueden ser usadas reemplazando los parámetros por el radio de modo que $a=b=c=r$ para parametrizar una esfera.

A la siguiente superficie de revolución también se la conoce como esferoide.
Las ecuaciones implícita y paramétrica son idénticas a la del elipsoide tradicional, con la particularidad de que dos de los parámetros dos iguales entre si.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elipsoide de revolución}
    \begin{equation*}
        a=b \enspace \lor \enspace b=c \enspace \lor \enspace a=c
    \end{equation*}
\end{mdframed}

Si $a=c \enspace \lor \enspace b=c$ se lo llama Esferoide Oblato, y la simetría sería con respecto al eje $x$ o al eje $y$.
Si $a=b$ se la llama Esferoide Prolato y la simetría sería con respecto al eje $z$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Paraboloide de revolución}
    \begin{equation*}
        (x-x_0)^2 + (y-y_0)^2 = m(z-z_0)
    \end{equation*}
\end{mdframed}

La simetría de esta superficie se va a dar respecto al eje cuyo término no esté elevado al cuadrado.
Por otro lado, si $m$ es negativo va a tener una orientación negativa.
De $m$ depende, además, qué tan ancho sea el paraboloide.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Hiperboloide de 1 hoja}
    \begin{equation*}
        \dfrac{(x-x_0)^2}{a^2} + \dfrac{(y-y_0)^2}{b^2} - \dfrac{(z-z_0)^2}{c^2} = 1
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Hiperboloide de 2 hojas}
    \begin{equation*}
        \dfrac{(x-x_0)^2}{a^2} + \dfrac{(y-y_0)^2}{b^2} - \dfrac{(z-z_0)^2}{c^2} = -1
    \end{equation*}
\end{mdframed}

\concept{Superficies de extrusión}

Las superficies de extrusión se obtienen trasladando o ``copiando'' una curva a lo largo de un eje.
En las ecuaciones implícitas de las superficies por extrusión, la componente sobre la que se crea el copiado no impone ninguna condición.
La extrusión también puede darse en los otros ejes, no solo en el $z$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Cilindro}
    \begin{equation*}
        S = (x,y,z) \in \setR^3 \tq (x-x_0)^2 + (y-y_0)^2 = r^2
    \end{equation*}
\end{mdframed}

\concept{Otras superficies}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Toro}
    \begin{gather*}
        \scale{0.98}{ \left( a - \sqrt{(x-x_0)^2+(y-y_0)^2)} \right)^2 + (z-z_0)^2 = r^2 }
        \\[1em]
        \Vec{s}(t) = \left\{
        \begin{aligned}
            x(u,v) &= x_0 + (a+r \cos{(u)}) \cos{(v)} \\
            y(u,v) &= y_0 + (a+r \cos{(u)}) \sin{(v)} \\
            z(u,v) &= z_0 + r \sin{(u)}
        \end{aligned}
        \right.
        \\
        \textrm{Con} \hspace{1ex} -\pi \leq u < \pi \quad 0 \leq v \leq 2 \pi
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Cinta de Möbius}
    \begin{gather*}
        \Vec{s}(t) = \left\{
        \scale{0.98}{
        \begin{aligned}
            x(u,v) &= x_0 + \cos{(u)} \left( \dfrac{v}{2} \cos{\Big(\dfrac{u}{2}\Big)}+a \right) \\[1ex]
            y(u,v) &= y_0 + \sin{(u)} \left( \dfrac{v}{2} \cos{\Big(\dfrac{u}{2}\Big)}+a \right) \\[1ex]
            z(u,v) &= z_0 + \dfrac{v}{2} \sin{\Big(\dfrac{u}{2}\Big)}
        \end{aligned}
        }
        \right.
        \\
        \textrm{Con} \hspace{1ex} \leq u < 2 \pi \quad -\dfrac{1}{2} \leq v \leq \dfrac{1}{2}
    \end{gather*}
\end{mdframed}


\chapter{Campos vectoriales}


Para entender qué es un campo vectorial, primero hay que entender qué es un vector anclado a un punto.
También se suele decir vector aplicado en un punto, vector fijado en un punto o vector plantado en un punto.

Recordemos que un vector $\overline{AB}$ es un elemento de un espacio vectorial que tiene origen o \emph{cola} en $A$ y \emph{extremo} en $B$.
Si $A=\Vec{0}$ entonces la cola está en el origen de coordenadas, y $\overline{AB}=B-A$ se denota simplemente como el vector $\Vec{B}$ o el punto $B$.

Un vector anclado a un punto tiene como extremo la suma del vector más el punto y como cola el punto.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Vector anclado}
    \begin{gather*}
        \textrm{Sea} \hspace{1ex} \Vec{v} = (v_1,v_2 \dots v_\nth) = \overline{A_v B_v} \in \setR^\nth
        \\
        \textrm{Sea} \hspace{1ex} \Vec{x} = (x_1,x_2 \dots x_\nth) = \overline{A_x B_x} \in \setR^\nth \tq A_x = \Vec{0}
        \\
        \textrm{Se define} \hspace{1ex} \Vec{v}_{\Vec{x}} = (v_1,v_2 \dots v_\nth)_{(x_1,x_2 \dots x_\nth)} = \overline{AB}
        \\
        \textrm{Tal que} \hspace{1ex} A=\Vec{x} \land B=\Vec{x}+\Vec{v}
    \end{gather*}
\end{mdframed}

Esto equivale a decir que el vector anclado $\Vec{v}_{\Vec{x}}$ es la traslación del vector $\Vec{v}$ tal que su origen coincida con el de $\Vec{x}$.
Cuando se trata de un vector anclado, sí importa dónde grafiquemos el vector.
Los vectores anclados juegan un rol muy importante en los campos vectoriales, ya que un vector anclado a un punto puede estar en función de ese punto:
\begin{equation*}
    \Vec{v}_{\Vec{x}} (\Vec{x}) = \big( v_1(\Vec{x}),v_2(\Vec{x}) \dots v_n(\Vec{x}) \big)_{(x_1,x_2 \dots x_\nth)}
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Campo vectorial}
    \cusTe{Un campo vectorial es una aplicación o función que para cada vector del espacio $\setR^n$ asigna un vector anclado.}
    \begin{multline*}
        \Vec{F}:\setR^\nth \longrightarrow \setR^\mth \tq
        \\
        \Vec{F}(\Vec{x}) = 
        \begin{bmatrix}
            F_1(\Vec{x}) & F_2(\Vec{x}) & \dots & F_\mth(\Vec{x})
        \end{bmatrix}
    \end{multline*}
\end{mdframed}

Un campo vectorial es una función que toma valores vectoriales y les asigna otros valores vectoriales.
En dos dimensiones, se puede decir que un campo vectorial es una aplicación que a cada punto del plano le asigna un vector en dicho plano.
\begin{equation*}
    \Vec{F}:\setR^2 \longrightarrow \setR^2 \tq \Vec{F}(x,y)= 
    \begin{bmatrix}
        P(x,y) & Q(x,y)
    \end{bmatrix}
\end{equation*}

Observar que cada componente de $\Vec{F}(\Vec{x})$ es un campo escalar ya que tanto $P(x,y)$ como $Q(x,y)$ son funciones de tipo $\setR^2 \longrightarrow \setR$.

Es importante notar que, para graficar un campo vectorial, cada vector de la imagen se grafica con su respectivo origen, ya que son todos vectores anclados, lo cual no se suele aclarar en la notación con el subíndice.

Por otra parte, si se quiere graficar un campo vectorial estrictamente, como en un plano o en el espacio hay infinitos puntos, la gráfica quedaría como una ``mancha negra'' en todo el espacio.
Por este motivo se se acostumbra graficar solo algunos puntos y con la magnitud de los vectores disminuida, para que las flechas no se solapen.

\section{Operaciones}

La divergencia de un campo vectorial es un número escalar que se obtiene haciendo el producto interno entre el operador de 
Hamilton y el campo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Divergencia}
    \begin{equation*}
        \operatorname{div}(\Vec{F}) = \grad \cdot \Vec{F}(\Vec{x}) = \sum_{i=1}^n \dfrac{\partial}{\partial x_i} \Vec{F}_i(\Vec{x})
    \end{equation*}
\end{mdframed}

El rotor de un campo vectorial se obtiene haciendo el producto vectorial entre el operador de Hamilton y el campo.
Al ser un producto vectorial solo está definido para matrices de $3 \times 3$, es decir que solo se puede calcular el rotor de campos en el espacio.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Rotor}
    \begin{equation*}
        \operatorname{rot} (\Vec{F}) = \grad \times \Vec{F}(\Vec{x})
    \end{equation*}
\end{mdframed}

Operando matricialmente se obtiene la siguiente expresión para el rotor:
\begin{equation*}
    \operatorname{rot} (\Vec{F}) = \operatorname{det}
    \begin{bmatrix}
        \versor{e}_1 & \versor{e}_2 & \versor{e}_3 \\[1em]
        \dfrac{\partial}{\partial x_1} & \dfrac{\partial}{\partial x_2} & \dfrac{\partial}{\partial x_3} \\[1em]
        F_1(\Vec{x}) & F_2(\Vec{x}) & F_3(\Vec{x})
    \end{bmatrix}
\end{equation*}
\begin{multline*}
    = \left( \dfrac{\partial F_3(\Vec{x})}{\partial x_2} - \dfrac{\partial F_2(\Vec{x})}{\partial x_3} \right) \iVer \, +
    \\
    + \left( \dfrac{\partial F_1(\Vec{x})}{\partial x_3} - \dfrac{\partial F_3(\Vec{x})}{\partial x_1} \right) \jVer \, +
    \\ + \left( \dfrac{\partial F_2(\Vec{x})}{\partial x_1} - \dfrac{\partial F_1(\Vec{x})}{\partial x_2} \right) \kVer
\end{multline*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    La divergencia del rotor de un campo vectorial es nula.
    \begin{equation*}
        \operatorname{div} \left( \operatorname{rot}(\Vec{F}) \right) = \grad \cdot \left( \grad \times \Vec{F} \right) = 0
    \end{equation*}
\end{mdframed}


\section{Campos de gradiente}

Un campo vectorial se llama campo de gradiente o campo conservativo cuando sus componentes son las derivadas parciales de un campo escalar, conocido como función potencial.
La notación usada es el producto escalar entre el operador de Hamilton y dicha función potencial.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Campo de gradiente}
    \begin{equation*}
        \Vec{F}(\Vec{x}) = \grad f(\Vec{x})
    \end{equation*}
\end{mdframed}

Las derivadas de un campo de gradiente son en realidad las derivadas segundas de la función potencial:
\begin{align*}
    \Vec{F}: \setR^2 \longrightarrow \setR^2 \tq
    \\
    \Vec{F}(x,y) &= \grad f(x,y)
    \\
    &= \begin{bmatrix}
        \dfrac{\partial}{\partial x} f(x,y) &     \dfrac{\partial}{\partial y} f(x,y)
    \end{bmatrix}
    \\
    &= \begin{bmatrix} P(x,y) & Q(x,y) \end{bmatrix}
\end{align*}

De manera que si hacemos las derivadas segundas:
\begin{gather*}
    \left\{
    \begin{aligned}
        \dfrac{\partial}{\partial x} P(x,y) &=& f''_{xx}
        \\[1ex]
        \dfrac{\partial}{\partial y} P(x,y) &=& f''_{yx}
    \end{aligned}
    \right.
    \\[1em]
    \left\{
    \begin{aligned}
        \dfrac{\partial}{\partial x} Q(x,y) &=& f''_{xy}
        \\[1ex]
        \dfrac{\partial}{\partial y} Q(x,y) &=& f''_{yy}
    \end{aligned}
    \right.
\end{gather*}

Y aplicamos el Teorema de Schwarz se tiene:
\begin{gather*}
    \textrm{Dado que} \hspace{1ex} \Vec{F}(x,y) = \grad f(x,y) \Rightarrow
    \\
    \dfrac{\partial}{\partial y} P(x,y) = \dfrac{\partial}{\partial x} Q(x,y)
\end{gather*}
    
Obteniendo la siguiente propiedad que si $\Vec{F}$ es diferenciable en todo el plano, la implicancia vale en ambos sentidos.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El rotor de un campo de gradiente es nulo.
    \begin{equation*}
        \operatorname{rot}(\Vec{F})= \grad \times \grad f(\Vec{x}) = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Un campo de gradiente es ortogonal a las superficies de nivel de la función potencial asociada al campo.
    Cualquier curva contenida en la superficie también va a ser ortogonal.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si un campo conservativo físicamente representa un campo de fuerza, el trabajo del campo a lo largo de una trayectoria cerrada es nulo.
    \begin{equation*}
        \Vec{F}(x,y) = \grad f(x,y) \iff \oint \Vec{F} \cdot d \Vec{s} = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si un campo conservativo físicamente representa un campo de fuerza, el trabajo del campo es independiente de la trayectoria.
    \begin{equation*}
        \Vec{F}(x,y) = \grad f(x,y) \Rightarrow \int_C \Vec{F} \cdot d \Vec{s} = f(\Vec{x}_1)-f(\Vec{x}_0)
    \end{equation*}
\end{mdframed}

Si $\Vec{F}$ es diferenciable en todo el plano, la implicancia anterior vale en ambos sentidos.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si el dominio de un campo vectorial es abierto y simplemente conexo, entonces la forma diferencial asociada es exacta.
    Por lo tanto, el campo vectorial es un campo de gradientes y admite función potencial.
\end{mdframed}

Por definición, la función potencial está dada por la anti derivada de cualquiera de las componentes de un campo de gradientes más una constante de integración:
\begin{equation*}
    f(x,y) = \int F_\ith(\Vec{x}) \partial x_\ith + c
\end{equation*}

Para un campo de gradiente $\Vec{F}:\setR^2 \longrightarrow \setR^2$ podemos definir la función potencial de cualquiera de las siguientes dos maneras.
Hay que tener en cuenta que la ``constante'' de integración en realidad está en función de la variable que no se integre.
\begin{gather*}
    f(x,y) = \int P(x,y) \partial x + c(y)
    \\
    f(x,y) = \int Q(x,y) \partial y + c(x)
\end{gather*}

Para calcular la ``constante'' de integración hay que igualar las dos formas de calcular la derivada parcial del campo.
Una es derivando $f(x,y)$ con respecto de la variable que tome $c$.
La otra es tomar la componente del campo que anteriormente no se usó para definir $f(x,y)$, ya que por definición es la derivada de la función potencial.
Luego, se integra esta igualdad y se obtiene $c$.

Para campos de gradiente $\Vec{F}:\setR^\nth \longrightarrow \setR^\nth$ este mismo procedimiento es válido pero hay que realizarlo $\nth$ veces, ya que cada vez que se integre $c$ va a haber una nueva incógnita pero vamos a tener suficientes ecuaciones para igualar porque se puede definir $f$ de $\nth$ formas distintas.


\section{Líneas de flujo}

Las curvas integrales o líneas de flujo de un campo vectorial dan una idea de qué trayectoria va a seguir y a qué velocidad va a ir una partícula que se coloque en un punto del campo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Líneas de flujo}
    \cusTe{Las líneas de flujo son aquella familia de curvas que son tangentes a los vectores anclados del campo vectorial.}
    \begin{equation*}
        \Vec{F} \big( \Vec{c}(t) \big) = \frac{\dif}{\dif t} \Vec{c}(t)
    \end{equation*}
\end{mdframed}


\chapter{Integrales de funciones reales}


Al igual que la derivación, la integración solo puede operar con una variable a la vez.
Pero a diferencia de las derivadas, es posible computar una unica integral y llegar a un resultado numérico integrando con respecto de todas las variables.

Según cómo esté definida la integración va a variar conceptualmente su aplicación.
Integrar funciones de una variable equivale a calcular el área debajo de la curva.
Integrar funciones de varias variables también es posible, y en el caso de un campo escalar de dos variables, la integral calcula el volumen por debajo de la superficie.
Aunque esta noción de la integral es una primera idea intuitiva, ya que la integración tiene muchas otras aplicaciones.


\section{Integrales de Riemann en 2D}

A continuación, se ve que una integral sobre un recinto rectangular $R$ calcula el volumen que hay por debajo del gráfico de un campo escalar de dos variables.

Sea $f$ una función contínua y positiva que no vale constantemente uno:
\begin{equation*}
    f:D \subseteq \setR^2 \longrightarrow \setR \tq 1 \neq f(x,y) > 0
\end{equation*}

Sea $R$, una partición de $D$, el área dada por el producto cartesiano entre intervalos de los ejes $x$ e $y$:
\begin{equation*}
    R = [a,b] \times [c,d] = [x_0,x_n] \times [y_0,y_n]
\end{equation*}

Nótese que el recinto $R$ es un rectángulo de puntos $\begin{bmatrix} x & y \end{bmatrix} \in \setR^2$ que pertenecen a un plano en el dominio de la función.
Mientras que los puntos de la superficie $S$ están definidos como $\begin{bmatrix} x & y & f(x,y) \end{bmatrix} \in \setR^3$ y pertenecen al espacio.

Esto implica que los puntos de la superficie $S$ dada por el gráfico de $f(x,y)$ no solamente están ``elevados'' con respecto a los puntos del recinto $R$.
Si no que, además, no tienen porqué tener la misma geometría.
De hecho, ni siquera pertenecen al mismo espacio vectorial.

Justamente el hecho de que tengan otro ``relieve'' va a permitir definir un volumen que no sea el de un cubo, donde bastaría con multiplicar sus lados para calcular el volumen.
Por este motivo, se definió $f(x,y) \neq 1$.

Una vez que se tiene el recinto particionado, se establece para cara partición un paralelepípedo de base igual al área de la partición y de altura tal que uno de sus vértices ``toque'' la gráfica de la función.

En la siguiente imagen se muestran graficados la superficie $S$ que es la gráfica de $f(x,y)$, un recinto $R$ de $n \times n = 4 \times 4 = 16$ particiones pudiendo $n$ ser cualquier número natural, y uno de los 16 paralelepípedos:

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-integral-1.pdf_tex}
\end{center}

El volumen del paralelepípedo graficado es:
\begin{equation*}
    V_{3,0}= \underbrace{(x_4-x_3)}_{\textrm{largo}} \times \underbrace{(y_1-y_0)}_{\textrm{ancho}} \times \underbrace{f(x_3,y_0)}_{\textrm{alto}}
\end{equation*}

El volumen de cualquier paralelepípedo es:
\begin{equation*}
    V_{\ith,\jth}= \underbrace{(x_{\ith+1} - x_\ith)}_{\Delta x_\ith} \times \underbrace{(y_{\jth+1} - y_\jth)}_{\Delta y_\jth} \times f(x_\ith,y_\jth)
\end{equation*}

El volumen de todos los paralelepípedos del recinto es la suma de cada volumen.
Esta aproximación del volumen total $V$ que hay por debajo de $S$ es:
\begin{equation*}
    V \approx \sum_{\ith,\jth=0}^{\nth-1} V_{\ith,\jth} = \sum_{\ith,\jth=0}^{\nth-1} f(x_\ith,y_\jth) \, \Delta x_\ith \, \Delta y_\jth
\end{equation*}

El largo $\Delta x_\ith$ y el ancho $\Delta y_\jth$ de cada paralelepípedo depende de la cantidad $\nth$ de particiones del recinto y de los extremos $a$, $b$, $c$ y $d$ de los intervalos:
\begin{equation*}
    \left\{
    \begin{aligned}
        \Delta x_\ith &=& x_{\ith+1} - x_\ith = \dfrac{b-a}{\nth}
        \\[1em]
        \Delta y_\jth &=& y_{\jth+1} - y_\jth = \dfrac{d-c}{\nth}
    \end{aligned}
    \right.
\end{equation*}

Al aumentar $\nth$ aumenta la cantidad de particiones y por ende la cantidad de paralelepípedos, no así el tamaño del recinto.
Por lo tanto, el grosor de los paralelepipedos tiene que disminuir.
Tomar el límite cuando $n\to\infty$ implica sumar los volúmenes de los infinitos paralelepípedos de grosor nulo que hay en el recinto.
De esta forma, la aproximación del volumen bajo la superficie pasa a ser una igualdad, quedando definida una integral doble:
\begin{align*}
    V &= \lim_{\nth\to\infty} \sum_{\ith,\jth=0}^{\nth-1} f(x_\ith,y_\jth) \, \Delta x_\ith \, \Delta y_\jth
    \\
    &= \int_c^d \int_a^b f(x,y) \, \partial x \, \partial y
\end{align*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de Fubini}
    \cusTe{Si el recinto de integración es rectangular, entonces los extremos de los intervalos son permutables.}
\end{mdframed}


\section*{Cambio de variables en 2D}

Sea $\Vec{T}:A' \subset \setR^2 \longrightarrow A \subset \setR^2$ una transformación de clase $\Vec{T} \in \class (A')$ y sea el determinante de la matriz jacobiana $\operatorname{det}(J \, \Vec{T}(u,v)) \neq 0$, se tiene que:
\begin{multline*}
    \iint_A f(x,y) \, \partial x \, \partial y =
    \\
    = \iint_{A'} f \left( \Vec{T}(u,v) \right) \norm{ det(J \Vec{T}(u,v)) } \, \partial u \, \partial v
\end{multline*}

\begin{center}
    \def\svgwidth{0.8\linewidth}
    \input{./images/calc-transformacion.pdf_tex}
\end{center}

% \section*{Transformaciones lineales}
% \section*{Coordenadas polares}
% \section*{Coordenadas elípticas}
% \section*{Cambio de variables en 3D}
% \section*{Coordenadas cilíndricas}
% \section*{Coordenadas esféricas}


\section{Aplicaciones}


\subsection{Distancia lineal}

Dada la función $f:D \subset \setR \longrightarrow \setR \tq f(x)=1$.
El cálculo de la integral sobre un intervalo $\Delta x$ da como resultado el largo del intervalo.
\begin{equation*}
    A = \int_{x_0}^{x_1} \dif x = 1 \cdot \Delta x
\end{equation*}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-integral-2.pdf_tex}
\end{center}


\subsection{Área bajo una curva plana}

Dada una función $f:D \subset \setR \longrightarrow \setR \tq f(x)>0$.
El cálculo de la integral sobre un intervalo $\Delta x$ da como resultado el área bajo la gráfica de $f$.
\begin{equation*}
    A = \int_{x_0}^{x_1} f(x) \, \dif x
\end{equation*}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-integral-3.pdf_tex}
\end{center}


\subsection{Área del recinto $A$}

Dada la función $f:D \subset \setR^2 \longrightarrow \setR \tq f(x,y)=1$.
El cálculo de la integral sobre un recinto $A$ da como resultado el área del recinto.
\begin{equation*}
    A = \iint_A \partial \, x \partial y = 1 \cdot \Delta x \, \Delta y
\end{equation*}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-integral-4.pdf_tex}
\end{center}


\subsection{Volumen bajo una superficie}

Dada una función $f:D \subset \setR^2 \longrightarrow \setR \tq f(x,y)>0$.
El cálculo de la integral sobre un recinto $A$ da como resultado el volumen bajo la gráfica de $f$.
\begin{equation*}
    V = \iint_A f(x,y) \partial x \, \partial y
\end{equation*}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-integral-5.pdf_tex}
\end{center}


\subsection{Volumen del recinto $V$}

Dada la función $f:D \subset \setR^3 \longrightarrow \setR \tq f(x,y,z)=1$.
El cálculo de la integral sobre un volumen de integración $V$ da como resultado dicho volumen.
\begin{equation*}
    V = \iiint_V \partial x \, \partial y \, \partial z = 1 \cdot \Delta x \, \Delta y \, \Delta z
\end{equation*}


\subsection{Longitud de una curva plana}

El largo $L$ de un arco de curva plana está dado por:
\begin{equation*}
    L = \int_{t_0}^{t_1} \nnorm{ \dfrac{\dif}{\dif t} \Vec{c}(t) \dif t } \, \dif t
\end{equation*}


\subsection{Área de una superficie}

El área $A$ de una superficie está dada por:
\begin{align*}
    A &= \iint_S \nnorm{\Vec{n}(u,v)} \, \partial u \, \partial v
    \\[1ex]
    &= \iint_S \nnorm{ \dfrac{\partial}{\partial u} \Vec{s}(u,v) \times \dfrac{\partial}{\partial v} \Vec{s}(u,v) } \, \partial u \, \partial v
\end{align*}


\subsection{Masa de un sólido}

Sea $\rho:\setR^3 \longrightarrow \setR$ una función que determina la densidad en cada punto del espacio ocupado por cierto sólido.
La integral de $\rho (x,y,z)$ sobre el volumen $V$ calcula la masa del sólido.
\begin{equation*}
    m = \iiint_V \rho (x,y,z) \, \partial x \, \partial y \, \partial z
\end{equation*}


\section{Integrales sobre curvas y superficies}

Las integrales curvilíneas e integrales de superficie son integrales dobles o triples.
Pero como se evalúan sobre curvas o superficies, las variables con respecto a las que se integra se reducen a una o dos, respectivamente.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Diferencial de distancia}
    \begin{equation*}
        \dif \Vec{s} = \versor{t} \, \dif s = \left( \partial x, \partial y, \partial z \right)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elemento de arco}
    \begin{equation*}
        \dif s = \nnorm{ \dfrac{\dif}{\dif t} \Vec{c}(t) } \dif t = \nnorm{ \dif \Vec{s} }
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Diferencial de área}
    \begin{equation*}
        \dif \Vec{S} = \versor{n} \, \dif S = \dfrac{\grad S(x,y,z)}{\norm{ \dfrac{\partial}{\partial z} S(x,y,z) } } \, \partial x \, \partial y
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elemento de área}
    \begin{equation*}
        \dif S = \nnorm{ \dfrac{\partial}{\partial u} \Vec{s}(u,v) \times \dfrac{\partial}{\partial v} \Vec{s}(u,v) } \, \partial u \, \partial v = \nnorm{ \dif \Vec{S} }
    \end{equation*}
\end{mdframed}


\concept{Integrales de tipo 1}

Las integrales sobre curvas y superficies de tipo 1 se realizan sobre campos escalares.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Integral curvilínea de tipo 1}
    \begin{equation*}
        \int_C f \, \dif s = \int_{t_0}^{t_1} f \big( \Vec{c}(t) \big) \, \nnorm{ \dfrac{\dif}{\dif t} \Vec{c}(t) } \, \dif t
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Integral de superficie de tipo 1}
    \begin{equation*}
        \int_S f \, \dif S = \iint f \big( \Vec{s}(u,v) \big) \, \nnorm{ \Vec{n} } \, \partial u \, \partial v
    \end{equation*}
\end{mdframed}


\concept{Integrales de tipo 2}

Las integrales sobre curvas y superficies de tipo 2 se realizan sobre campos vectoriales.

El trabajo es la cantidad de energía que se necesita para que una partícula recorra un arco de curva.
El movimiento puede darse en 2 o 3 dimensiones espaciales, pero en ambos casos se estaría calculando el trabajo sobre una curva.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:type2Int}
    \end{defn}
    \cusTi{Integral curvilínea de tipo 2 (Trabajo)}
    \begin{equation*}
        W = \int_C \Vec{F} \cdot \dif \Vec{s}
        = \int_{t_0}^{t_1} \Vec{F} \big( \Vec{c}(t) \big) \, \dfrac{\dif}{\dif t} \Vec{c}(t) \, \dif t
    \end{equation*}
\end{mdframed}

El flujo en 2 dimensiones espaciales mide que tanto ``Caudal de Fuerza'' fluye a través de una curva en cierto intervalo de tiempo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Integral curvilínea de tipo 2 (Flujo 2D)}
    \begin{equation*}
        \Phi = \int_C \Vec{F} \cdot \dif \Vec{S}
        = \int_{t_0}^{t_1} \Vec{F} \big( \Vec{c}(t) \big) \cdot \versor{n} \, \dif s
    \end{equation*}
\end{mdframed}

El flujo en 3 dimensiones espaciales mide que tanto ``Caudal de Fuerza'' fluye a través de una superficie en cierto intervalo de tiempo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Integral de superficie de tipo 2 (Flujo 3D)}
    \begin{equation*}
        \Phi = \int_S \Vec{F} \cdot \dif \Vec{S}
        = \iint \Vec{F} \big( \Vec{s}(u,v) \big) \cdot \versor{n} \, \dif S
    \end{equation*}
\end{mdframed}


\chapter{Teoremas del cálculo integral}


\section{Teo. de Green-Riemann}

El Teorema de Green relaciona la integral curvilínea de tipo 2.1 con la integral doble de la tercera componente del rotacional.

Sea $C$ una curva cerrada y simple, orientada positivamente y sea $A$ un recinto simplemente conexo, se tiene que:
\begin{align*}
    \oint_C \Vec{F} \cdot \dif \Vec{s}
    &= \iint_A \left( \grad \times \Vec{F} \right) \kVer \, \partial x \, \partial y
    \\[1ex]
    &= \left( \dfrac{\partial}{\partial x} F_2 - \dfrac{\partial}{\partial y} F_1 \right) \partial x \, \partial y
\end{align*}


\section{Teo. de Stokes}

El Teorema de Stokes relaciona la integral curvilínea de tipo 2.1 con la integral de superficie del rotacional.

Sea $\Vec{F}$ un campo vectorial de clase $\class (\setR^3)$ y sea $S$ una superficie cerrada, simple, diferenciable y compatible con la orientación, se tiene que:
\begin{equation*}
    \oint_C \Vec{F} \cdot \dif \Vec{s} = \oiint_S \left( \grad \times \Vec{F} \right) \dif \Vec{S}
\end{equation*}


\section{Teo. de Gauss-Ostrogradsky}

El Teorema de Gauss relaciona la integral de superficie de tipo 2.2 con la divergencia de una fuerza en un volumen.

Sea $\Vec{F}$ un campo vectorial de clase $\class (\setR^3)$ sea $S$ una superficie cerrada, simple, diferenciable y compatible con la orientación, se tiene que:
\begin{equation*}
    \oiint_S \Vec{F} \cdot \dif \Vec{S} = \iiint_V \left( \grad \cdot \Vec{F} \right) \partial x \, \partial y \, \partial z
\end{equation*}


\chapter{Números complejos}


Los números complejos surgen de la necesidad de inventar una solución a la ecuación $x^2=-1$.
Es por esto que se define el número imaginario $\iu$ de manera tal que $\iu^2=-1$.

El número $\iu$ pertenece al conjunto $\setI$ de números imaginarios.
Los números imaginarios se forman multiplicando el número $\iu$ por un número real $y$.

Combinando los números reales $\setR \subset \setC$ con los números imaginarios $\setI \subset \setC$ surge el conjunto de los números complejos $\setC=\setR \times \setI$.

A diferencia de los reales, el conjunto de números complejos no es un conjunto ordenado.
Los números complejos se representan gráficamente en un plano.
El plano complejo tiene en el eje $x$ la conocida recta real mientras que el eje vertical representa los números $\iu \, y$ imaginarios puros.

Los complejos se pueden expresar de varias formas que se relaciónan entre sí según la fórmula de Euler:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:EulerFormula}
    \end{prop}
    \cusTi{Fórmula de Euler}
    \begin{equation*}
        \cos{(\theta)} \pm \iu \sin{(\theta)} = e^{\pm \iu \theta}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Fórmula de Moivre}
    \begin{equation*}
        \cos{(n \theta)} \pm \iu \sin{(n \theta)} = e^{\pm \iu n \theta}
    \end{equation*}
\end{mdframed}

Un número complejo se puede definir como la suma de un número real con un número imaginario.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:BinomialForm}
    \end{defn}
    \cusTi{Forma binomial}
    \begin{equation*}
        z = x + \iu y
    \end{equation*}
\end{mdframed}

Se puede definir como un par ordenado perteneciente al plano complejo, donde la primer componente es la parte real y la segunda es la parte imaginaria.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:VectorForm}
    \end{defn}
    \cusTi{Forma vectorial}
    \begin{equation*}
        z = (x,y) = \left[\Re(z),\Im(z)\right] \in \setC
    \end{equation*}
\end{mdframed}

Podemos representar un número complejo de forma trigonométrica como un vector de cierto largo $\rho=\norm{z}$ que forma cierto ángulo $\theta = \artan{(x/y)}$ con el eje real.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:TrigForm}
    \end{defn}
    \cusTi{Forma polar}
    \begin{equation*}
        z = \rho \, \big( \cos{(\theta)} + \iu \sin{(\theta)} \big)
    \end{equation*}
\end{mdframed}

O bien podemos expresar un complejo de forma exponencial usando la fórmula de Euler.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:ExpForm}
    \end{defn}
    \cusTi{Forma exponencial}
    \begin{equation*}
        z = \rho \, e^{\iu \theta}
    \end{equation*}
\end{mdframed}


\section{Producto}

El producto de dos números complejos da como resultado otro número complejo.
Si bien los numeros complejos pueden ser expresados vectorialmente, el producto no es el producto interno de vectores convencionales.

El producto se define como la multiplicación de números complejos expresados en forma binomial.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Producto}
    \begin{equation*}
        (x_1 + \iu y_1) \cdot (x_2 + \iu y_2) = (x_1x_2-y_1y_2) + \iu (y_1x_2+x_1y_2)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Para $z_3=z_1 \cdot z_2 = \norm{z_3} e^{\iu \arg(z_3) } $ se tiene que:
    \begin{gather*}
        \norm{z_3} = \norm{z_1} \cdot \norm{z_2}
        \\[1ex]
        \arg(z_3) = \arg(z_1) + \arg(z_2)
    \end{gather*}
\end{mdframed}

Al multiplicar un número por su inverso, se obtiene la unidad.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:Inverse}
    \end{defn}
    \cusTi{Inverso multiplicativo}
    \begin{equation*}
        z^{-1}=\frac{1}{z} \iff z \cdot z^{-1} = 1
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        z^{-1}=\dfrac{\conj{z}}{\norm{z}^2}
    \end{equation*}
\end{mdframed}

\concept{Demostración por definición}

Dado $x + \iu y$ y su inverso $u + \iu v$, por definición su producto es $1 + \iu 0$ o bien, expresado en forma vectorial:
\begin{gather*}
    (x,y) \cdot (u,v) = (1,0)
    \\
    (xu-yv,yu+xv) = (1,0)
\end{gather*}

Obteniendo un sistema de ecuaciones del que es posible despejar las componentes del vector que representan al inverso.
\begin{gather*}
    \left\{
    \begin{aligned}
        1 &= xu-yv
        \\
        0 &= yu+xv
    \end{aligned}
    \right.
    \\
    u = -\frac{xv}{y} \Rightarrow 1= -\frac{x^2v}{y}-yv = -v\left( \dfrac{x^2}{y}+y \right)
    \\
    v=\frac{-1}{\frac{x^2}{y}+y}=\dfrac{-y}{\frac{x^2y}{y}+y^2}
\end{gather*}

Luego:
\begin{gather*}
    \left\{
    \begin{aligned}
        u &= \frac{x}{x^2+y^2}
        \\[1ex]
        v &= \frac{-y}{x^2+y^2}
    \end{aligned}
    \right.
    \\
    z^{-1} = \dfrac{x}{x^2+y^2}-\dfrac{\iu y}{x^2+y^2}
\end{gather*}

\concept{Demostración por el conjugado}

Esta demostración es más rápida pero en ella se utilizan elementos todavía no definidos.
Ver primero la definición \ref{defn:conjugate}, y las propiedades \ref{prop:eAbsoluteValue} y \ref{prop:|z|^2=z.z*}.
\begin{align*}
    z \cdot \conj{z} &= \norm{z}^2
    \\
    \frac{z \cdot \conj{z}}{\norm{z}^2} &= 1
    \\
    \frac{\conj{z}}{\norm{z}^2} &= \dfrac{1}{z}=z^{-1}
\end{align*}


\section{Módulo}

El módulo de un número complejo es el largo del vector que lo representaría si este se graficase en el plano complejo.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Módulo}
    \begin{equation*}
        \norm{z} = \nnorm{(x,y)} = \sqrt{x^2+y^2}
    \end{equation*}
\end{mdframed}

El módulo de la resta de dos números complejos es la distancia que hay entre esos dos puntos:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:distance}
    \end{prop}
    \begin{equation*}
        \operatorname{dist}(z_1,z_2)=\norm{z_1-z_2}
    \end{equation*}
\end{mdframed}

El módulo del número $e^\iu$ se puede expresar como:
\begin{equation*}
    \norm{e^{\iu \theta}} = \norm{\cos(\theta) + \iu \sin(\theta)} = \sqrt{\cos^2(\theta) + \sin^2(\theta)}
\end{equation*}

Con lo cual, $e^\iu$ tiene módulo unitario.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:eAbsoluteValue}
    \end{prop}
    \begin{equation*}
        \norm{e^{\iu \theta}} = 1
    \end{equation*}
\end{mdframed}

El módulo de un número es mayor que sus componentes:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:zAbsoluteValue}
    \end{prop}
    \begin{equation*}
        \Re(z) \leq \norm{\Re(z)} \leq \norm{z} \geq \norm{\Im(z)} \geq \Im(z)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:triangleInequality}
    \end{prop}
    \cusTi{Desigualdad triangular}
    \begin{equation*}
        \big| \norm{z_1}-\norm{z_2} \big| \leq \big| z_1 \pm z_2 \big|\leq \norm{z_1} + \norm{z_2}
    \end{equation*}
\end{mdframed}

\concept{Demostración}

La parte derecha de la desigualdad es evidente geométricamente, y la otra se demuestra como un corolario de la primera.

Al plantear los dos siguientes casos, quedan demostradas las dos situaciones que contempla el módulo en la desigualdad izquierda de la inecuación:

\begin{itemize}
    \item Caso $\norm{z_2} \leq \norm{z_1}$
    \begin{gather*}
        \norm{z_1} = \norm{(z_1+z_2)-z_2} \leq \norm{z_1+z_2}+\norm{-z_2}
        \\
        \norm{z_1} - \norm{-z_2} \leq \norm{z_1+z_2}
        \\
        \norm{z_1} - \norm{z_2} \leq \norm{z_1+z_2}
    \end{gather*}

    \item Caso $\norm{z_1} \leq \norm{z_2}$
    \begin{gather*}
        \norm{z_2} = \norm{(z_2+z_1)-z_1} \leq \norm{z_2+z_1}+\norm{-z_1}
        \\
        \norm{z_2} - \norm{-z_1} \leq \norm{z_2+z_1}
        \\
        \norm{z_2} - \norm{z_1} \leq \norm{z_1+z_2}
    \end{gather*}
\end{itemize}


\section{Conjugado}

El conjugado $(\conj{z})$ de un número $x+iy$ se define cambiando el signo del termino imaginario.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:conjugate}
    \end{defn}
    \cusTi{Conjugado}
    \begin{equation*}
        \conj{z} = x - \iu y
    \end{equation*}
\end{mdframed}

El conjugado de un número es simétrico a este con respecto del eje real, por lo tanto ambos tienen el mismo módulo:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \norm{z} = \norm{\conj{z}}
    \end{equation*}
\end{mdframed}

El conjugado de la suma de dos números es la suma de los conjuados, y análogamente para el producto:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:zw*=(z*w)*}
    \end{prop}
    \begin{gather*}
        \conj{z_1 \pm z_2} = \conj{z_1} \pm \conj{z_2}
        \\
        \conj{z_1 \cdot z_2} = \conj{z_1} \cdot \conj{z_2}
    \end{gather*}
\end{mdframed}

Sumando el conjugado de un número con este, se obtiene el doble de la parte real y análogamente para la parte imaginaria al hacer la diferencia.
\begin{gather*}
        z+\conj{z} = 2x
        \\
        z-\conj{z} = 2 \iu y
    \end{gather*}

Quedando definida la siguiente propiedad.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:ReIm}
    \end{prop}
    \begin{align*}
        \Re (z) &= \frac{z+\conj{z}}{2}
        \\[1ex]
        \Im (z) &= \frac{z-\conj{z}}{2 \iu}
    \end{align*}
\end{mdframed}

Multiplicando un complejo por su conjugado queda:
\begin{equation*}
    z \cdot \conj{z} = x^2+y^2
\end{equation*}

Que es el módulo al cuadrado.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:|z|^2=z.z*}
    \end{prop}
    \begin{equation*}
        z \cdot \conj{z} = \norm{z}^2
    \end{equation*}
\end{mdframed}

Si un número complejo es igual a su conjugado, entonces se trata de un número real.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        z = \conj{z} \iff z \in \setR
    \end{equation*}
\end{mdframed}

Si el cuadrado de un número es igual al cuadrado de su conjugado, entonces se trata o bien de un real o bien de un imaginario puro.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        z^2=\conj{z}^2 \iff z \in \setR \enspace \lor \enspace z \in \setI
    \end{equation*}
\end{mdframed}


\section{Potenciación}

Elevar un complejo a la n-ésima potencia se define, para $\nth \in \setZ$, a partir de la forma exponencial de la siguiente manera.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:nPower}
    \end{defn}
    \cusTi{Potencia n-ésima}
    \begin{equation*}
        z^\nth = \rho^\nth e^{\iu \nth \theta}
    \end{equation*}
\end{mdframed}

El conjugado de un número complejo elevado es igual a la potencia del conjugado.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \conj{z^\nth} = \left( \conj{z} \right)^\nth
    \end{equation*}
\end{mdframed}

Al igual que podemos calcular las raíces de los polinomios reales de segundo grado, podemos deducir la fórmula resolvente para un polinomio complejo con coeficientes $a,b,c \in \setR$ genéricos:
\begin{equation*}
    az^2 + bz + c = 0
\end{equation*}

En primer lugar, se completan cuadrados:
\begin{gather*}
    z^2 + \left(\frac{b}{a} \right) z + \frac{c}{a} = 0
    \\
    \left( z+\frac{b}{2a} \right)^2 + \left( \frac{c}{a}-\frac{b^2}{4a^2} \right) = 0
    \\
    \left( z+\frac{b}{2a} \right)^2 - \left( \frac{b^2}{4a^2} - \frac{c}{a} \right) = 0
    \\
    \norm{z+\frac{b}{2a}} = \sqrt{\frac{b^2}{4a^2} - \frac{c}{a}}
    = \sqrt{\frac{a \, b^2 - 4 \, a^2 \, c}{4 \, a^3}}
    = \frac{\sqrt{b^2 - 4ac}}{\norm{2a}}
    \\
    z_{1,2} = \frac{-b \pm \sqrt{b^2-4ac}}{2a}
\end{gather*}

Donde:
\begin{equation*}
    \textrm{Si} \enspace \frac{b^2}{4a^2} - \frac{c}{a} \geq 0 \Rightarrow b^2 \geq 4ac \Rightarrow z \in \setR
\end{equation*}

Observar que en el despeje algebráico queda una raíz cuadrada.
De tratarse solamente de números reales tendría un radicando positivo.
Pero si proponemos una solución compleja, puede tener un radicando negativo.
Para expresar esto, vamos a valernos de la definición del número $\iu$ partiendo de la segunda ecuación:
\begin{gather*}
    \left( z+\frac{b}{2a} \right)^2 + \left( \frac{c}{a} - \frac{b^2}{4a^2} \right) = 0
    \\
    \left( z+\frac{b}{2a} \right)^2 - \iu^2 \left( \frac{c}{a}-\frac{b^2}{4a^2} \right) = 0
    \\
    \left( z+\frac{b}{2a} \right)^2 - \left( \iu \sqrt{\frac{4ac-b^2}{4a^2}} \right)^2 = 0
    \\
    \scale{0.98}
    {
    \left( z+\frac{b}{2a} + \iu \sqrt{\frac{4ac-b^2}{4a^2}} \right) \left( z+\frac{b}{2a} - \iu \sqrt{\frac{4ac-b^2}{4a^2}} \right) = 0
    }
\end{gather*}

Y para que el producto anterior sea igual a cero se tiene, o bien:
\begin{equation*}
    z + \frac{b}{2a} + \frac{\iu \sqrt{4ac-b^2}}{2a} = 0
\end{equation*}

O bien:
\begin{equation*}
    z + \frac{b}{2a} - \frac{\iu \sqrt{4ac-b^2}}{2a} = 0
\end{equation*}

Infiriendo así la fórmula resolvente para polinomios complejos de segundo grado:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Fórmula resolvente}
    \begin{equation*}
        z_{1,2} = \frac{-b \pm \iu \sqrt{4ac-b^2}}{2a}
    \end{equation*}
\end{mdframed}

Donde:
\begin{equation*}
    \textrm{Si} \enspace \frac{b^2}{4a^2} - \frac{c}{a} \leq 0 \Rightarrow b^2 \leq 4ac \Rightarrow z \in \setC
\end{equation*}

Así como se pueden buscar las raíces que anulan los polinomios de números reales, se pueden calcular las raíces complejas que, elevadas a la $n$-ésima potencia, dan como resultado cierto $z_0$ conocido.

El conjunto de raíces n-ésimas se define como:
\begin{align*}
    C &= \braces{z \in \setC / z^n=z_0}
    \\
    &= z_1,z_2,z_3 \ldots z_\nth
\end{align*}

Aplicando la definición de potencia $n$-ésima (\ref{defn:nPower}) en la parte izquierda de la ecuación y escribiendo la parte derecha usando la forma exponencial, se puede deducir una fórmula para despejar $z$, la incógnita.
\begin{gather*}
    \rho^\nth \cdot e^{\iu \nth \theta} = \rho_0 \cdot e^{\iu \theta_0}
    \\
    \left\{
    \begin{aligned}
        \rho^\nth &= \rho_0
        \\
        \nth \theta &= \theta_0 + 2\kth \pi \quad \textrm{con} \enspace \kth \in \setZ
    \end{aligned}
    \right.
    \\[1ex]
    z_{\kth+1} = \sqrt[\nth]{\rho_0} \enspace \sqrt[\nth]{e^{\iu (\theta_0 + 2 \kth \pi)}} \quad \textrm{con} \enspace \kth \in \{0,1,2 \ldots \nth-1\}
\end{gather*}

Nótese que el subíndice $\kth+1$ de cada raiz es solamente para evitar cometer abuso de notación usando la notación $z_0$ tanto para la primera raiz como para el número al que le estamos calculando las raices.

Se obtiene entonces:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        z_{\kth+1} = \sqrt[\nth]{\rho_0} \enspace e^{\iu \left( \tfrac{\theta_0}{\nth} + \tfrac{2 \kth \pi}{\nth} \right)}
    \end{equation*}
\end{mdframed}

En la notación se suele diferenciar cualquier raiz $n$-ésima $z_{k+1} = z_0^{1/\nth}$ de la única raíz positiva, que recibe el nombre de raíz principal y se denota $z_1=\sqrt[n]{z_0}$.

Se puede observar que la fórmula anterior puede generar $\nth$ valores distintos.
De no acotar los valores que puede tomar $\kth$, la formula daría valores repetidos.
Las raíces $n$-ésimas van a estar distribuidas en el plano complejo formando un polígono de $\nth$ vértices, separados cada uno del otro por el mismo ángulo.

Separando el exponente, cada una de las $\nth$ raíces que arroja la fórmula se puede expresar como un producto en el que uno de los factores siempre va a ser la raíz principal $z_1$.
De esta forma, podemos definir una notación equivalente para el conjunto $C$ de raíces $n$-ésimas:
\begin{equation*}
    C = \braces{z_1 , z_1 \, w , z_1 \, w^2 \ldots z_1 \, w^{n-1}}
\end{equation*}

Donde:
\begin{equation*}
    w = e^{\iu \left( \tfrac{2 \pi}{\nth} \right)}
\end{equation*}

Observar que este análisis sirve para estudiar raíces de monomios.
En caso de tratarse de un polinomio de más de un término, no podemos conocer las raíces por definición directamente.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \left( 1 + z + z^2 + z^3 \ldots + z^{\nth-1} \right) \left( 1 - z \right) = 1 - z^\nth
    \end{equation*}
\end{mdframed}


\section{Geometría de conjuntos}

Podemos representar un conjunto de números complejos como un conjunto de puntos en el plano complejo.
Algunas secciones cónicas, en vez de ser expresadas con sus ecuaciones tradicionales en función de las partes real $(x)$ e imaginaria $(y)$, se pueden expresar en función de los puntos del plano complejo $(z)$ según la propiedad \ref{prop:distance} del módulo.
Es posible verificar que los complejos expresados en su forma binomial verifican también las ecuaciones tradicionales de las cónicas.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Línea}
    \cusTe{Conjunto de puntos $z \in \setC$ que, por simetría, están a igual distancia de dos puntos $z_1$ y $z_2$, verificando:}
    \begin{equation*}
        \norm{z-z_1} = \norm{z-z_2}
    \end{equation*}
\end{mdframed}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-geom-1.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Circunferencia}
    \cusTe{Conjunto de los puntos $z \in \setC$ que están a cierta distancia $R \in \setR$ de un centro $z_0$ en el plano complejo, verificando:}
    \begin{equation*}
        \norm{z-z_0} = R
    \end{equation*}
\end{mdframed}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-geom-2.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Elipse}
    \cusTe{Conjunto de puntos $z \in \setC$ que verifican que la suma de las distancias desde los mismos hasta cada uno de dos focos $F_n \in \setC$ vale constantemente $D \in \setR$, verificando:}
    \begin{equation*}
        \norm{z-F_1} + \norm{z-F_2} = D
    \end{equation*}
\end{mdframed}

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-geom-3.pdf_tex}
\end{center}


\chapter{Funciones complejas}


Una función de una variable compleja es una aplicación que toma un conjunto ($D$) de números complejos y los transforma en otro conjunto ($S$) de números complejos $w \in S$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:complexFunc1}
    \end{defn}
    \cusTi{Función compleja}
    \begin{equation*}
        f: D \subseteq \setC \longrightarrow S \subseteq \setC \tq f(z) = w
    \end{equation*}
\end{mdframed}

Las funciones complejas no se pueden graficar, porque tanto los elementos del conjunto de partida como los elementos del conjunto de llegada son puntos del plano complejo.
Es por esto que para tener ciertas nociones geométricas de la función, podemos hacer dos gráficos en 2D de dichos conjuntos de partida y llegada.
Es posible graficar el módulo de una función en 3D.
Se usan dos variables independientes para el conjunto de partida y una dependiente para el conjunto de llegada.

Una vez aplicada una función, en ocaciones suele ser útil factorizar todos los términos que tienen multiplicado a $\iu$ de los que no para obtener la expresión binomial de la imagen.
\begin{equation*}
    w = u + \iu v
\end{equation*}

Así, las imágenes se pueden expresar como binomios complejos o como pares ordenados, sabiendo que basta con sumar la primer componente ($u$) más la segunda componente ($v$) multiplicada por $\iu$ para obtener nuevamente la forma binomial.
\begin{equation*}
    u + \iu v \equiv (u,v)
\end{equation*}

Siguiendo este criterio, suele usarse el vector $(x,y)$ de variables independientes para representar la variable compleja $x + \iu y$ como notación equivalente.
\begin{equation*}
    x + \iu y \equiv (x,y)
\end{equation*}

Pudiendo definir $\Vec{F}: \setR^2 \longrightarrow \setR^2$ y $f: \setR^2 \longrightarrow \setC$, ambas equivalentes a la función compleja dada en la definición \ref{defn:complexFunc1}.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función compleja}
    \begin{equation*}
        f(x,y) = u(x,y) + \iu \, v(x,y)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Campo vectorial asociado a una función compleja}
    \begin{equation*}
        \Vec{F}(x,y) = \begin{bmatrix} u(x,y) & v(x,y) \end{bmatrix}
    \end{equation*}
\end{mdframed}


\section{Límite y continuidad}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Límite de una función compleja}
    \begin{multline*}
        \lim_{z \to z_0} f(z) = w_0 \iff \forall \varepsilon > 0 \enspace \exists \enspace \delta > 0 \tq
        \\
        0 < \norm{z-z_0} < \delta \Rightarrow \norm{f(z)-w_0} < \varepsilon
    \end{multline*}
\end{mdframed}

El límite de una función de variable compleja es en realidad un límite doble pués, según la propiedad \ref{prop:distance}, los módulos involucrados representan distancias en el plano complejo.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:limit}
    \end{prop}
    \begin{equation*}
        \lim_{z \to z_0} f(z) = \lim_{\substack{x \to x_0\\y \to y_0}} f(x + \iu y)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Continuidad}
    \cusTe{Una función compleja es continua si y solo si:}
    \begin{equation*}
        f(z_0)=\lim_{z \to z_0}f(z)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El campo vectorial $\Vec{F}$ asociado a una función compleja $f$ es continuo si y solo si dicha función también lo es:
    \begin{equation*}
        \textrm{$f$ es continua $\iff \Vec{F}$ es continuo}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función acotada}
    \cusTe{Una función compleja es acotada en un entorno reducido de un punto $(z_0)$ si existe algún valor real $(m,M \in \setR)$ tal que el módulo de la función sea mayor o menor a dicho valor.}
    \begin{gather*}
        \norm{f(z)} \geq m \quad \lor \quad \norm{f(z)} \leq M
        \\[1ex]
        \forall \quad z: 0 < \norm{z-z_0} < R
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Infinitésimo por acotado}
    \begin{gather*}
        \textrm{Dada} \enspace f(z)=g(z) \cdot h(z)
        \\
        \textrm{Si} \enspace g(z) \to 0 \enspace \land \enspace h(z) \enspace \textrm{es acotada}
        \\
        \textrm{Entonces} \enspace \lim_{z \to z_0} f(z) = 0
    \end{gather*}
\end{mdframed}


\section{Argumento}

El argumento de un número complejo es el ángulo que este forma con el eje $x$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Argumento de un número complejo}
    \begin{equation*}
        \arg(z) = \arctan \bb{\frac{y}{x}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \arg(z) = - \iu \ln \left( \frac{z}{\norm{z}} \right)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El argumento de un complejo es el opuesto del argumento de su inverso.
    \begin{equation*}
        \arg(z) = - \arg(z^{-1})
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El argumento del producto de dos números es la suma de los argumentos de esos números.
    \begin{equation*}
        \arg (z_1 \, z_2) = \arg(z_1) + \arg(z_2)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    El argumento de la división de dos números es la resta de los argumentos de esos números.
    \begin{equation*}
        \arg \left( \dfrac{z_1}{z_2} \right) = \arg(z_1) - \arg(z_2)
    \end{equation*}
\end{mdframed}

Para poder definir funciones complejas, es necesario que cada elemento del dominio tenga solo una imagen.

Al representar un número complejo en su forma trigonométrica o exponencial es evidente que un mismo punto puede ser expresado mediante distintos números complejos por tener diferentes ángulos:
\begin{equation*}
    f(z_0) = w_k = \rho \, e^{\iu \left( \theta + 2 \kth \pi \right)} \quad \textrm{con} \enspace \kth \in \setZ
\end{equation*}

Por esto, para definir una función se restringen los valores que puede tomar el argumento de la imagen a un intervalo de $2\pi$ radianes a partir de cierto ángulo $\alpha$ arbitrario:
\begin{equation*}
    f(z) = w_\alpha \tq \arg(w_\alpha) \in [\alpha;\alpha+2\pi)
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Argumento principal}
    \cusTe{Se lo define como aquel que toma un valor entre $-\pi$ y $\pi$ y se lo denota con mayúscula para distinguirlo, de modo que:}
    \begin{equation*}
        -\pi \leq \operatorname{Arg}(z) \leq \pi
    \end{equation*}
\end{mdframed}


\section{Exponenciales y logarítmicas}

Plantear una ecuación exponencial tiene como soluciones el conjunto $\sub{S}{exp}$ de los $w \in \setC$ que resultan de elevar el número $e$ a una potencia compleja $z \in \setC$.
\begin{equation*}
    \sub{S}{exp} = \braces{w \in \setC\ \tq e^z = w} = e^z
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Sea $w = e^z$ dado que $e^{x + \iu y} = e^x \, e^{\iu y}$ se tiene:
    \begin{equation*}
        \norm{w} = e^x \quad \land \quad \arg(w) = y + 2 \kth \pi
    \end{equation*}
\end{mdframed}

De forma similar a la ecuación exponencial, para el logaritmo neperiano se puede plantear el conjunto de números $w \in \setC$ que, de estar en el exponente del número $e$, dan como resultado $z \in \setC$.
\begin{equation*}
    \sub{S}{ln} = \braces{w \in \setC \tq e^w = z} = \ln(z)
\end{equation*}

Expresando un número complejo en su forma exponencial y aplicando propiedades logarítmicas
\begin{align*}
    z &= \norm{z} \, e^{\iu \arg(z)}
    \\
    \ln (z) &= \ln \left( \norm{z} \, e^{\iu \arg(z)} \right)
\end{align*}

Se obtiene la siguiente expresión alternativa para el logaritmo.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \ln(z) = \ln \norm{z} + \iu \left( \theta + 2 \kth\pi \right) \quad \textrm{con} \enspace k \in \setZ
    \end{equation*}
\end{mdframed}


\section{Trigonométricas}

Partiendo de la propiedad de Euler (Prop. \ref{prop:EulerFormula}) y por la propiedad \ref{prop:ReIm} se tiene:
\begin{gather*}
    \left\{
    \begin{aligned}
        w &= e^{\iu z} = \cos(z) + \iu \sin(z)
        \\[1ex]
        \conj{w} &= e^{-\iu z} = \cos(z) - \iu \sin(z)
    \end{aligned}
    \right.
    \\[1em]
    \left\{
    \begin{aligned}
        w + \conj{w} &= e^{\iu z} + e^{-\iu z} = 2 \cos(z)
        \\[1ex]
        w - \conj{w} &= e^{\iu z} - e^{-\iu z} = 2 \iu \sin(z)
    \end{aligned}
    \right.
\end{gather*}

Despejando las ecuaciones del sistema anterior quedan definidas expresiones para el seno y el coseno de números complejos.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Coseno complejo}
    \begin{equation*}
        \cos(z) = \frac{e^{\iu z} + e^{-\iu z}}{2}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Seno complejo}
    \begin{equation*}
        \sin(z) = \frac{e^{\iu z} - e^{-\iu z}}{2 \iu}
    \end{equation*}
\end{mdframed}

O bien, expresando $z$ en su forma binomial
\begin{equation*}
    \left\{
    \begin{aligned}
        \cos(z) = \frac{e^{\iu \left(x + \iu y \right)} + e^{-\iu \left(x + \iu y \right)}}{2}
        \\[1ex]
        \sin(z) = \frac{e^{\iu \left(x + \iu y \right)} - e^{-\iu \left(x+\iu y\right)}}{2 \iu}
    \end{aligned}
    \right.
\end{equation*}

Si $x \neq 0 \land y \neq 0 \Rightarrow z \in \setC$ quedan expresiones alternativas para el coseno y el seno de números complejos:
\begin{equation*}
    \left\{
    \begin{aligned}
        \cos(z) &= \frac{e^{\iu x - y} + e^{-\iu x + y}}{2}
        \\[1ex]
        \sin(z) &= \frac{e^{\iu x - y} - e^{-\iu x + y}}{2\iu}
    \end{aligned}
    \right.
\end{equation*}

Si $y=0 \Rightarrow z \in \setR$ quedan expresiones para el coseno y el seno de números reales:
\begin{equation*}
    \left\{
    \begin{aligned}
        \cos(x) &= \frac{e^{\iu x} + e^{-\iu x}}{2}
        \\[1ex]
        \sin(x) &= \frac{e^{\iu x} - e^{-\iu x}}{2\iu}
    \end{aligned}
    \right.
\end{equation*}

Y si $x=0 \Rightarrow z \in \setI$ quedan expresiones para el coseno hiperbólico y el seno hiperbólico de números imaginarios puros:
\begin{equation*}
    \left\{
    \begin{aligned}
        \cos(\iu y) &= \frac{e^{-y} + e^y}{2} = \cosh(\iu y)
        \\[1ex]
        \sin(\iu y) &= \frac{e^{-y} - e^y}{2\iu} = -\iu \sinh{(\iu y)}
    \end{aligned}
    \right.
\end{equation*}


\chapter{Propiedades complejas}


\section{Identidades de ángulos dobles}

Sea $z=\rho \, e^{\iu \theta}$ un número complejo expresado en forma polar, por un lado podemos expresar el cuadrado como:
\begin{align*}
    z^2 &= \rho^2 \, e^{2 \iu \theta}
    \\
    &= \rho^2 \left[ \cos(2 \theta) + \iu \sin(2 \theta) \right]
\end{align*}

O bien desarrollando el cuadrado por definición del producto como:
\begin{align*}
    z^2 &= \rho^2 \, \left(e^{\iu \theta}\right)^2
    \\
    &= \rho^2 \, \left[ \cos(\theta) + \iu \sin(\theta) \right]^2
    \\
    &= \rho^2 \, \left[ \cos^2(\theta) - \sin^2(\theta) + 2\iu \cos(\theta) \sin(\theta) \right]
\end{align*}

Luego, tomando las partes real e imaginaria de $z^2$ y dividiendo por $\rho^2$ se tiene:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Identidades de ángulos dobles}
    \begin{gather*}
        \frac{\Re (z^2)}{\rho^2} = \cos(2 \theta) = \cos^2(\theta) - \sin^2(\theta)
        \\[1ex]
        \frac{\im (z^2)}{\rho^2} = \sin(2 \theta) = 2\cos(\theta) \sin(\theta)
    \end{gather*}
\end{mdframed}


\section{Cuadrado del módulo del binomio}

Sea $w=z_1+z_2$ la suma de dos números complejos dados por $z_1=\norm{z_1} \, e^{\theta_1}$ y $z_1=\norm{z_2} \, e^{\theta_2}$, al elevar al cuadrado el módulo de $w$ se tiene:
\begin{equation*}
    \norm{w}^2 = \norm{z_1+z_2}^2
\end{equation*}

Usando la propiedad \ref{prop:|z|^2=z.z*} queda:
\begin{equation*}
    \norm{w}^2 = \left(z_1+z_2\right) \left( \conj{z_1+z_2} \right)
\end{equation*}

Usando la propiedad \ref{prop:zw*=(z*w)*} primero para la suma y luego para el producto se tiene:
\begin{align*}
    \norm{w}^2 &= \left(z_1+z_2\right) \left(\conj{z_1}+\conj{z_2}\right)
    \\
    &= z_1 \, \conj{z_1} + z_2 \, \conj{z_2} + z_1 \, \conj{z_2} + \conj{z_1} \, z_2
    \\
    &= z_1 \, \conj{z_1} + z_2 \, \conj{z_2} + z_1 \, \conj{z_2} + \conj{z_1 \, \conj{z_2}}
\end{align*}

Finalmente, según la propiedad \ref{prop:ReIm} y nuevamente la propiedad \ref{prop:|z|^2=z.z*} se tiene:
\begin{align*}
    \norm{w}^2 &= z_1 \, \conj{z_1} + z_2 \, \conj{z_2} + 2 \, \Re \left( z_1 \, \conj{z_2} \right)
    \\
    &= \norm{z_1}^2 + \norm{z_2}^2 + 2 \, \Re \left( z_1 \, \conj{z_2} \right)
    \\
    &= \norm{z_1}^2 + \norm{z_2}^2 + 2 \, \Re \left( \norm{z_1} \, e^{\theta_1} \, \norm{z_2} \, e^{-\theta_2} \right)
    \\
    &= \norm{z_1}^2 + \norm{z_2}^2 + 2 \, \norm{z_1} \, \norm{z_2} \, \Re \left( e^{\theta_1-\theta_2} \right)
\end{align*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \norm{z_1 + z_2}^2 = \norm{z_1}^2 + \norm{z_2}^2 + 2 \, \norm{z_1} \, \norm{z_2} \, \cos (\theta_1-\theta_2)
    \end{equation*}
\end{mdframed}


\chapter{Derivadas de funciones complejas}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Derivada}
    \cusTe{La derivada de $f(z)$ en $z_0 \in \setC$ se define como:}
    \begin{align*}
        \frac{\dif}{\dif z} f(z_0) &= \lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0}
        \\[1ex]
        &= \lim_{h \to 0} \frac{f(z_0+h) - f(z_0)}{h}
    \end{align*}
\end{mdframed}

Nótese que tanto la variable $z$ como el punto $z_0$ son complejos.
Por lo tanto el incremento $h=z-z_0$ también lo es.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-deriv.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:partialDerivate}
    \end{prop}
    La derivada toma el mismo valor en todas las direcciones $(\Vec{r})$ del plano y particularmente en los ejes real e imaginario.
    \begin{equation*}
        \frac{\dif}{\dif z} f(z_0) = \frac{\partial}{\partial x} f(z_0) = \frac{\partial}{\partial y} f(z_0) = \frac{\partial}{\partial \Vec{r}} f(z_0)
    \end{equation*}
\end{mdframed}

Por la propiedad \ref{prop:limit} del límite, la derivada es un límite doble:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{gather*}
        \frac{\dif}{\dif z} f(z_0) = \lim_{\substack{x \to x_0\\y \to y_0}} \frac{f(x + \iu y) - f(x_0 + \iu y_0)}{(x-x_0) + \iu (y-y_0)}
        \\[1em]
        = \lim_{\substack{h_x \to 0\\h_y \to 0}}\frac{f \Big( (x_0+h_x) + \iu (y_0+h_y) \Big) - f(x_0 + \iu y_0)}{h_x + \iu h_y}
    \end{gather*}
\end{mdframed}


\section{Funciones analíticas}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función analítica}
    \cusTe{Una función se dice analítica en $z_0$ si es derivable en un entorno abierto del punto.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si una función es analítica, es de clase $f \in \class[\infty]$ y sus derivadas sucesivas son analíticas.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función entera}
    \cusTe{Una función se dice entera si es analítica en todo el plano complejo.}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si una función es entera y acotada, entonces es constante.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función armónica}
    \cusTe{Una función se dice armónica si es un campo escalar real $u: \setR^2 \longrightarrow \setR$ que verifica $\grad^2 u = 0$.
    Esto es, si cumple:}
    \begin{equation*}
        \frac{\partial^2}{\partial x^2} u(x,y) + \frac{\partial^2}{\partial y^2} u(x,y) = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Las componentes $u(x,y)$ y $v(x,y)$ del campo vectorial asociado a una función compleja son armónicas solo si la función es entera.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si un campo de gradientes $\Vec{F}=\grad u(x,y)$ verifica $\operatorname{div}(\Vec{F})=0$ entonces la función potencial $u(x,y)$ es armónica.
    Esto es:
    \begin{equation*}
        \grad^2 u(x,y) = u_{xx}'' + u_{yy}'' = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Las curvas de nivel de $v(x,y)$ son las líneas de flujo del campo del cual $u(x,y)$, en caso de ser armónica, es el potencial.
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si $u: D \subseteq \setC \longrightarrow \setC$ está definida en un conjunto $D$ simplemente conexo, entonces $u(x,y)$ es armónica, existe la función $v(x,y)$ conjugada armónica de $u(x,y)$ y, además, $f(x,y)=u(x,y)+iv(x,y)$ es entera.
\end{mdframed}

\concept{Demostración}

Por las ecuaciones de Cauchy-Riemann (Def. \ref{defn:CRequations}) y por el teorema de Cauchy-Schwarz, se tiene:
\begin{gather*}
    \left\{
    \begin{aligned}
        u_x' &= v_y'
        \\
        u_y' &= -v_x'
    \end{aligned}
    \right.
    \\[1ex]
    \left\{
    \begin{aligned}
        u_{xx}'' &= v_{yx}''
        \\
        u_{yy}'' &= -v_{xy}''
    \end{aligned}
    \right.
    \\[1ex]
    u_{xx}'' + u_{yy}''= v_{yx} - v_{xy} = 0
    \\
    \therefore u \enspace \textrm{es armónica}
    \\[1ex]
    \left\{
    \begin{aligned}
        u_x' &= v_y'
        \\
        u_y' &= -v_x'
    \end{aligned}
    \right.
    \\[1ex]
    \left\{
    \begin{aligned}
        u_{xy}'' &= v_{yy}''
        \\
        u_{yx}'' &= - v_{xx}''
    \end{aligned}
    \right.
    \\[1ex]
    u_{yx}'' - u_{xy}'' = - v_{xx} - v_{yy} = 0
    \\
    \therefore v \enspace \textrm{es la conjugada armónica de} \enspace u
\end{gather*}


\section{Teorema de Cauchy-Riemann}

El teorema de Cauchy-Riemann relaciona la derivada de una función con las derivadas parciales de los campos escalares $u(x,y)$ y $v(x,y)$ componentes del campo vectorial asociado a la función.
Si una función es derivable entonces se cumple la siguiente propiedad.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:CRtheorem}
    \end{prop}
    \begin{equation*}
        \frac{\dif}{\dif z} f(z_0)= \frac{\partial}{\partial x} f(x_0,y_0)= -\iu \, \frac{\partial}{\partial y} f(x_0,y_0)
    \end{equation*}
\end{mdframed}

Nótese que las derivadas de la ecuación anterior son las parciales de los campos escalares $u(x,y)$ y $v(x,y)$, componentes del campo vectorial asociado.
Es importante no confundirlas con la derivada (\ref{prop:partialDerivate}) en las direcciones de los ejes real e imaginario.

Aplicando la propiedad \ref{prop:CRtheorem} a una función compleja expresada en la forma binomial, podemos obtener dos ecuaciones de gran relevancia.
\begin{gather*}
    \left\{
    \begin{aligned}
        \frac{\partial}{\partial x} f(x,y) &= \frac{\partial}{\partial x} u(x,y) + \iu \frac{\partial}{\partial x} v(x,y)
        \\
        \frac{\partial}{\partial y} f(x,y) &= \frac{\partial}{\partial y} u(x,y) + \iu \frac{\partial}{\partial y} v(x,y)
    \end{aligned}
    \right.
    \\[1ex]
    \frac{\partial}{\partial x} u(x,y) + \iu \frac{\partial}{\partial x} v(x,y) = - \iu \left( \frac{\partial}{\partial y} u(x,y) + \iu \frac{\partial}{\partial y} v(x,y) \right)
    \\[1ex]
    \frac{\partial}{\partial x} u(x,y) + \iu \frac{\partial}{\partial x} v(x,y) = \frac{\partial}{\partial y} v(x,y) - \iu \frac{\partial}{\partial y} u(x,y)
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:CRequations}
    \end{defn}
    \cusTi{Ecuaciones de C-R}
    \cusTe{Si una función es derivable entonces se cumple que:}
    \begin{align*}
        \frac{\partial}{\partial x} u(x,y) &= \frac{\partial}{\partial y} v(x,y)
        \\[1em]
        \frac{\partial}{\partial x} v(x,y) &= - \frac{\partial}{\partial y} u(x,y)
    \end{align*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si el campo vectorial asociado a una función compleja es diferenciable y se verifican las ecuaciones de Cauchy-Riemann, entonces la función es derivable.
\end{mdframed}

Aplicando una transformación biyectiva $(T)$ al campo vectorial $(\Vec{F})$ asociado a una función compleja $(f)$, al calcular la matriz de derivadas, o matriz Jacobiana $(J)$, se pueden deducir las relaciones entre las derivadas parciales en coordenadas polares.

Dada la siguiente función compleja y su campo vectorial asociado:
\begin{equation*}
    f(z) = u(z) + \iu v(z) \iff \Vec{F} = \begin{bmatrix} u(x,y) & v(x,y) \end{bmatrix}
\end{equation*}

Dada la siguiente transformación biyectiva:
\begin{multline*}
    \Vec{T}: (0;+\infty) \times (\alpha;\alpha+2\pi) \longrightarrow \setR^2 - L_\alpha \tq
    \\
    \Vec{T}(\rho,\theta) = \begin{bmatrix} \rho \, \cos(\theta) & \rho \, \sin(\theta) \end{bmatrix}
\end{multline*}

Se tiene que:
\begin{equation*}
    \operatorname{det} \big( J \Vec{T}(\rho,\theta) \big) = \rho \, \cos^2(\theta) + \rho \, \sin^2(\theta) = \rho \neq 0
\end{equation*}

Por lo tanto, al aplicar la transformación queda:
\begin{multline*}
    \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) =
    \\
    = \begin{bmatrix} u \Big( \rho \, \cos{(\theta)} , \rho \, \sin(\theta) \Big) & v \Big( \rho \, \cos(\theta) , \rho \, \sin(\theta) \Big) \end{bmatrix}
\end{multline*}

Y aplicando la regla de la cadena:
\begin{equation*}
    J \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) = J_{xy} \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) \cdot J_{\rho\theta} \Vec{T}(\rho,\theta)
\end{equation*}

Esto, representado de forma matricial, es:
\begin{equation*}
    \scale{0.9}{
    \begin{pmatrix}
        \dfrac{\partial u}{\partial \rho} & \dfrac{\partial u}{\partial \theta}
        \\[1em]
        \dfrac{\partial v}{\partial \rho} & \dfrac{\partial v}{\partial \theta}
    \end{pmatrix}_{\Vec{T}(\rho,\theta)}
    =
    \begin{pmatrix}
        \dfrac{\partial u}{\partial x} & \dfrac{\partial u}{\partial y}
        \\[1em]
        \dfrac{\partial v}{\partial x} & \dfrac{\partial v}{\partial y}
    \end{pmatrix}_{\Vec{T}(\rho,\theta)}
    %
    \begin{pmatrix}
        \dfrac{\partial T_1}{\partial \rho} & \dfrac{\partial T_1}{\partial \theta}
        \\[1em]
        \dfrac{\partial T_2}{\partial \rho} & \dfrac{\partial T_2}{\partial \theta}
    \end{pmatrix}_{(\rho,\theta)}
    }
\end{equation*}

O bien, representado en un sistema de ecuaciones:
\begin{equation*}
    \left\{
    \begin{aligned}
        \frac{\partial u}{\partial \rho} &=
        \frac{\partial u}{\partial x} \cdot \frac{\partial T_1}{\partial \rho} + \frac{\partial u}{\partial y} \cdot \frac{\partial T_2}{\partial \rho}
        \\[1ex]
        \frac{\partial u}{\partial \theta} &=
        \frac{\partial u}{\partial x} \cdot \frac{\partial T_1}{\partial \theta} + \frac{\partial u}{\partial y} \cdot \frac{\partial T_2}{\partial \theta}
        \\[1ex]
        \frac{\partial v}{\partial \rho} &=
        \frac{\partial v}{\partial x} \cdot \frac{\partial T_1}{\partial \rho} + \frac{\partial v}{\partial y} \cdot \frac{\partial T_2}{\partial \rho}
        \\[1ex]
        \frac{\partial v}{\partial \theta} &=
        \frac{\partial v}{\partial x} \cdot \frac{\partial T_1}{\partial \theta} + \frac{\partial v}{\partial y} \cdot \frac{\partial T_2}{\partial \theta}
    \end{aligned}
    \right.
\end{equation*}

Resolviendo las derivadas de la transformación, que son conocidas, se tiene:
\begin{equation*}
    \left\{
    \begin{aligned}
        \frac{\partial u}{\partial \rho} &= \frac{\partial u}{\partial x} \cos(\theta) + \frac{\partial u}{\partial y} \sin(\theta)
        \\[1ex]
        \frac{\partial u}{\partial \theta} &= -  \frac{\partial u}{\partial x} \rho \sin(\theta) + \frac{\partial u}{\partial y} \rho \cos(\theta)
        \\[1ex]
        \frac{\partial v}{\partial \rho} &= \frac{\partial v}{\partial x} \cos(\theta) + \frac{\partial v}{\partial y} \sin(\theta)
        \\[1ex]
        \frac{\partial v}{\partial \theta} &= - \frac{\partial v}{\partial x} \rho \sin(\theta) + \frac{\partial v}{\partial y} \rho \cos(\theta)
    \end{aligned}
    \right.
\end{equation*}

Aplicando las ecuaciones de Cauchy-Riemann (Def. \ref{defn:CRequations}) a las dos primeras ecuaciones del sistema anterior, se tiene:

\begin{equation*}
    \left\{
    \begin{aligned}
        \frac{\partial u}{\partial \rho} &= \frac{\partial v}{\partial y} \cos(\theta) - \frac{\partial v}{\partial x} \sin(\theta)
        \\[1ex]
        \frac{\partial u}{\partial \theta} &= -  \frac{\partial v}{\partial y} \rho \sin(\theta) - \frac{\partial v}{\partial x} \rho \cos(\theta)
        \\[1ex]
        \frac{\partial v}{\partial \rho} &= \frac{\partial v}{\partial x} \cos(\theta) + \frac{\partial v}{\partial y} \sin(\theta)
        \\[1ex]
        \frac{\partial v}{\partial \theta} &= -  \frac{\partial v}{\partial x} \rho \sin(\theta) + \frac{\partial v}{\partial y} \rho \cos(\theta)
    \end{aligned}
    \right.
\end{equation*}

Para finalmente, al sacar factor común $\rho$:
\begin{equation*}
    \left\{
    \begin{aligned}
        \frac{\partial u}{\partial \rho} &= \frac{\partial v}{\partial y} \cos(\theta) - \frac{\partial v}{\partial x} \sin(\theta)
        \\[1ex]
        \frac{\partial u}{\partial \theta} &= -\rho \left(  \frac{\partial v}{\partial y} \sin(\theta) + \frac{\partial v}{\partial x} \cos(\theta) \right)
        \\[1ex]
        \frac{\partial v}{\partial \rho} &= \frac{\partial v}{\partial x} \cos(\theta) + \frac{\partial v}{\partial y} \sin(\theta)
        \\[1ex]
        \frac{\partial v}{\partial \theta} &= \rho \left( -  \frac{\partial v}{\partial x} \sin(\theta) + \frac{\partial v}{\partial y} \cos(\theta) \right)
    \end{aligned}
    \right.
\end{equation*}

Establecer una relación para las derivadas parciales en coordenadas polares:

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:CRpolarEquations}
    \end{defn}
    \cusTi{Ecuaciones de C-R en polares}
    \cusTe{Si una función es derivable entonces se cumple que:}
    \begin{align*}
        \frac{\partial}{\partial \rho} u(\rho,\theta) &= \frac{\partial}{\partial \theta}  v(\rho,\theta) \left( \frac{1}{\rho} \right)
        \\[1em]
        \frac{\partial}{\partial \rho} v(\rho,\theta) = - \frac{\partial}{\partial \theta} u(\rho,\theta) \left( \frac{1}{\rho} \right)
    \end{align*}
\end{mdframed}


\subsection{Implicaciones geométricas}

Dado que:
\begin{equation*}
    \frac{\dif}{\dif z} f(z) \neq 0 \Rightarrow  J \Vec{F} \neq \Vec{0} \Rightarrow \grad u \neq \Vec{0} \land \grad v \neq \Vec{0}
\end{equation*}

Donde:
\begin{equation*}
    \left\{
    \begin{aligned}
        \grad u(x,y) &= \frac{\partial}{\partial x} u(x,y) + \iu \frac{\partial}{\partial y} u(x,y)
        \\[1ex]
        \grad v(x,y) &= \frac{\partial}{\partial x} v(x,y) + \iu \frac{\partial}{\partial y} v(x,y)
    \end{aligned}
    \right.
\end{equation*}

De manera que al aplicar las ecuaciones de Cauchy-Riemann (Def. \ref{defn:CRequations}) sobre el sistema anterior se tiene:
\begin{align*}
    \grad v(x,y) &= \frac{\partial}{\partial x} v(x,y) + \iu \frac{\partial}{\partial y} v(x,y)
    \\
    &= - \frac{\partial}{\partial y} u(x,y) + \iu \frac{\partial}{\partial x} u(x,y)
    \\[1ex]
    \grad v(x,y) &= \iu \left( \frac{\partial}{\partial x} u(x,y) + \iu \frac{\partial}{\partial y} u(x,y) \right)
\end{align*}

Por lo tanto:
\begin{equation*}
    \iu \grad u(x,y) = \grad v(x,y) \enspace \textrm{siendo} \enspace \norm{\grad u(x,y)} = \norm{\grad v(x,y)}
\end{equation*}


\section{Derivación en polares}

Aplicando una transformación biyectiva $(T)$ al campo vectorial $(\Vec{F})$ asociado a una función compleja $(f)$, al calcular la matriz de derivadas, o matriz Jacobiana $(J)$, se puede deducir una fórmula para calcular la derivada a partir de las coordenadas polares.

Dada la siguiente función compleja y su campo vectorial asociado:
\begin{equation*}
    f(z) = u(z) + \iu v(z) \iff \Vec{F} = \begin{bmatrix} u(x,y) & v(x,y) \end{bmatrix}
\end{equation*}

Dada la siguiente transformación biyectiva:
\begin{multline*}
    \Vec{T}: (0;+\infty) \times (\alpha;\alpha+2\pi) \longrightarrow \setR^2 - L_\alpha \tq
    \\
    \Vec{T}(\rho,\theta) = \begin{bmatrix} \rho \, \cos(\theta) & \rho \, \sin(\theta) \end{bmatrix}
\end{multline*}

Se tiene que:
\begin{equation*}
    \operatorname{det} \big( J \Vec{T}(\rho,\theta) \big) = \rho \, \cos^2(\theta) + \rho \, \sin^2(\theta) = \rho \neq 0
\end{equation*}

Por lo tanto, al aplicar la transformación queda:
\begin{multline*}
    \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) =
    \\
    = \begin{bmatrix} u \Big( \rho \, \cos{(\theta)} , \rho \, \sin(\theta) \Big) & v \Big( \rho \, \cos(\theta) , \rho \, \sin(\theta) \Big) \end{bmatrix}
\end{multline*}

Y aplicando la regla de la cadena:
\begin{equation*}
    J \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) = J_{xy} \Vec{F} \Big( \Vec{T}(\rho,\theta) \Big) \cdot J_{\rho\theta} \Vec{T}(\rho,\theta)
\end{equation*}

Esto, representado de forma matricial donde las filas de las matrices se notan con gradientes, es:
\begin{equation*}
    \begin{bmatrix}
        \grad_{\rho\theta} u
        \\
        \grad_{\rho\theta} v
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}
    =
    \begin{bmatrix}
        \grad_{xy} u
        \\
        \grad_{xy} v
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}
    %
    \begin{bmatrix}
        J_{\rho\theta} \Vec{T}
    \end{bmatrix}_{(\rho,\theta)}
\end{equation*}

Luego, tomando solo uno de los gradientes:
\begin{equation*}
    \begin{bmatrix}
        \grad_{\rho\theta} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}
    =
    \begin{bmatrix}
        \grad_{xy} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}
    %
    \begin{bmatrix}
        J_{\rho\theta} \Vec{T}
    \end{bmatrix}_{(\rho,\theta)}
\end{equation*}

Se puede calcular el gradiente $(\grad_{xy})$ multiplicando por la matriz inversa del jacobiano $(J^{-1})$:
\begin{gather*}
    \begin{bmatrix}
        J_{\rho\theta} \Vec{T}
    \end{bmatrix}_{(\rho,\theta)}^{-1}
    %
    \begin{bmatrix}
        \grad_{\rho\theta} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}^t
    =
    \begin{bmatrix}
        \grad_{xy} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}^t
    \\[1em]
    \begin{bmatrix}
        \cos(\theta) & -\dfrac{\sin(\theta)}{\rho}
        \\[1em]
        \sin(\theta) & \dfrac{\cos(\theta)}{\rho}
    \end{bmatrix}
    %
    \begin{bmatrix}
        \grad_{\rho\theta} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}^t
    =
    \begin{bmatrix}
        \grad_{xy} u
    \end{bmatrix}_{\Vec{T}(\rho,\theta)}^t
\end{gather*}

\begin{multline*}
    \frac{\partial}{\partial x} u \Big( \Vec{T}(\rho,\theta) \Big) =
    \\
    = \frac{\partial}{\partial \rho} u \Big( \Vec{T}(\rho,\theta) \Big) \cos(\theta) - \frac{\partial}{\partial \theta} u \Big( \Vec{T}(\rho,\theta) \Big) \frac{\sin(\theta)}{\rho}
\end{multline*}

\begin{multline*}
    \frac{\partial}{\partial y} u \Big( \Vec{T}(\rho,\theta) \Big) =
    \\
    = \frac{\partial}{\partial \rho} u \Big( \Vec{T}(\rho,\theta) \Big) \sin(\theta) + \frac{\partial}{\partial \theta} u \Big( \Vec{T}(\rho,\theta) \Big) \frac{\cos(\theta)}{\rho}
\end{multline*}

Por otro lado, aplicando las ecuaciones de Cauchy-Riemann (Def. \ref{defn:CRequations}) en la propiedad \ref{prop:CRtheorem}, se tiene que:
\begin{equation*}
    \frac{\dif}{\dif z} f(z) = \frac{\partial}{\partial x} u(x,y) - \iu \frac{\partial}{\partial y} u(x,y)
\end{equation*}

Por lo que, reemplazando las ecuaciones del sistema en la ecuación anterior se tiene:
\begin{multline*}
    \frac{\dif}{\dif z} f(z) = \left( \frac{\partial u}{\partial \rho} \cos(\theta) - \frac{\partial u}{\partial \theta} \frac{\sin(\theta)}{\rho} \right) +
    \\
    - \iu \left( \frac{\partial u}{\partial \rho} \sin(\theta) + \frac{\partial u}{\partial \theta} \frac{\cos(\theta)}{\rho} \right)
\end{multline*}

Sacando factor común en grupos:
\begin{multline*}
    \frac{\dif}{\dif z} f(z) = \frac{\partial u}{\partial \rho} \Big( \cos(\theta) - \iu \sin(\theta) \Big) +
    \\
    - \frac{1}{\rho} \frac{\partial u}{\partial \theta} \Big( \sin(\theta) + \iu \cos(\theta) \Big)
\end{multline*}

Aplicando las ecuaciones de Cauchy-Riemann en coordenadas polares (Def. \ref{defn:CRpolarEquations}) se tiene:
\begin{multline*}
    \frac{\dif}{\dif z} f(z) = \frac{\partial u}{\partial \rho} \Big( \cos(\theta) - \iu \sin(\theta) \Big) +
    \\
    + \frac{\partial v}{\partial \rho} \Big( \sin(\theta) + \iu \cos(\theta) \Big)
\end{multline*}

Multiplicando y dividiendo por $\iu$ se tiene:
\begin{multline*}
    \frac{\dif}{\dif z} f(z) = \frac{\partial u}{\partial \rho} \Big( \cos(\theta) - \iu \sin(\theta) \Big) +
    \\
    + \iu \frac{\partial v}{\partial \rho} \Big( \cos(\theta) - \iu \sin(\theta) \Big)
\end{multline*}
\begin{gather*}
    \dfrac{\dif}{\dif z} f(z) = \left( \frac{\partial u}{\partial \rho} + \iu \frac{\partial v}{\partial \rho} \right) \Big( \cos(\theta) - \iu \sin(\theta) \Big)
    \\[1em]
    \frac{\dif}{\dif z} f(z) = \left( \frac{\partial}{\partial \rho} u \big( \Vec{T}(\rho,\theta) \big) + \iu \frac{\partial}{\partial \rho} v \big( \Vec{T}(\rho,\theta) \big) \right) e^{-\iu\theta}
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \frac{\dif}{\dif z} f(z) = \left[ \frac{\partial}{\partial \rho} u \big( \Vec{T}(\rho,\theta) \big) + \iu \frac{\partial}{\partial \rho} v \big( \Vec{T}(\rho,\theta) \big) \right] e^{-\iu\theta}
    \end{equation*}
\end{mdframed}


\chapter{Integrales de funciones complejas}


\section{Curvas en el plano complejo}

Las curvas dadas en el plano complejo son funciones $(c)$ que toman valores reales $(t\in\setR)$ y los transforman en números complejos $(x + \iu y \in\setC)$.
\begin{equation*}
    c: D\subseteq \setR \longrightarrow \setC \tq c(t) = x(t) + \iu y(t)
\end{equation*}

Las curvas se derivan y se integran componente a componente.
Es decir que es válido:
\begin{gather*}
    \frac{\dif}{\dif t} c(t) = \frac{\dif}{\dif t} x(t) + \iu \frac{\dif}{\dif t} y(t)
    \\[1em]
    \int_{t_1}^{t_2} c(t) \, \dif t = \int_{t_1}^{t_2} x(t) \, \dif t + \iu \int_{t_1}^{t_2} y(t) \, \dif t
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \Re \left( \int_{t_1}^{t_2} c(t) \, \dif t \right) = \int_{t_1}^{t_2} \Re \Big( c(t) \Big) \dif t
    \end{equation*}
\end{mdframed}

Haciendo:
\begin{equation*}
    \int_{t_1}^{t_2} c(t) \, \dif t = z = \rho \, e^{\iu \theta}
\end{equation*}

\begin{align*}
    \rho &= \int_{t_1}^{t_2} e^{-\iu \theta} \, c(t) \, \dif t \quad \textrm{con} \enspace \rho = \norm{z}
    \\
    &= \Re (\rho) = \Re \left( \int_{t_1}^{t_2} e^{-\iu \theta} \, c(t) \, \dif t \right)
    \\
    &= \int_{t_1}^{t_2} \Re \left( e^{-\iu \theta} c(t) \right) \dif t 
\end{align*}

\begin{equation*}
    \rho \leq \int_{t_1}^{t_2} \norm{e^{-\iu \theta} \, c(t)} \, \dif t = \int_{t_1}^{t_2} \norm{c(t)} \, \dif t
\end{equation*}

Se obtiene la siguiente propiedad:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \norm{\int_{t_1}^{t_2} c(t) \, \dif t} \leq \int_{t_1}^{t_2} \norm{c(t)} \, \dif t
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Longitud de curva}
    \begin{equation*}
        \operatorname{long} (C) = \int_C \norm{dz} = \int_{t_1}^{t_2} \norm{\frac{\dif}{\dif t} c(t)} \, \dif t
    \end{equation*}
\end{mdframed}


\section{Integrales curvilíneas}

La integración de funciones complejas no está definida.
Las integrales de una función compleja tienen sentido si se evalúan sobre una curva del plano complejo.
La definición es parecida a las integrales de tipo 2 de cálculo real multivariable (Def. \ref{defn:type2Int}), pero en vez de tener un producto interno se tiene un producto complejo.
\begin{gather*}
    \textrm{Sea} \enspace f: \setC \longrightarrow \setC \tq f(z) = u(z) + \iu v(z)
    \\[1ex]
    \int_{C} f(z) \, \dif z = \int_{t_1}^{t_2} \left\{ f \Big( c(t) \Big) \cdot \frac{\dif}{\dif t} c(t) \right\} \dif t
    \\[1ex]
    \scale{0.98}{
    \int_{C} f(z) \, \dif z = \int_{t_1}^{t_2} \Big( u \, x' - v \, y' \Big) \dif t + \iu \int_{t_1}^{t_2} \Big( v \, x' + u \, y' \Big) \dif t
    }
\end{gather*}

Podemos reescribir la ecuación anterior definiendo dos campos vectoriales reales.
La integral curvilínea sería entonces un número complejo que tiene como partes real e imaginaria, integrales curvilíneas reales de tipo 2.1 respectivamente.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:integral}
    \end{defn}
    \cusTi{Integral curvilínea}
    \begin{equation*}
        \int_{C} f(z) \, \dif z = \int_{C} \Vec{G} \cdot \dif \Vec{s} + \iu \int_{C} \Vec{H} \cdot \dif \Vec{s}
    \end{equation*}
    Donde:
    \begin{align*}
        \Vec{G}(x,y) &= \begin{bmatrix} u(x,y) & -v(x,y) \end{bmatrix}
        \\
        \Vec{H}(x,y) &= \begin{bmatrix} v(x,y) & u(x,y) \end{bmatrix}
    \end{align*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \norm{\int_C f(z) \, \dif z} \leq M \, \operatorname{long}(C)
    \end{equation*}
\end{mdframed}


\section{Teorema de la primitiva}
\label{sec:primitive}

Una función compleja $f(z)$ tiene primitiva $F(z)$ en un subconjunto abierto $A$ del plano complejo solo si la integral es independiente de la trayectoria.
Esto implica que la integral sobre una trayectoria cerrada es nula:
\begin{equation*}
    \exists \, F(z) \iff \oint f(z) \, \dif z = 0
\end{equation*}

En tal caso, la integral se computa evaluando la primitiva en el punto final menos el inicial:
\begin{equation*}
    \int_C f(z) \, \dif z = F \big( c(t_2) \big) - F \big( c(t_1) \big)
\end{equation*}


\section{Teorema de Cauchy-Goursat}
\label{sec:CauchyGoursat}

Sea $A$ un conjunto abierto simplemente conexo y $C$ una curva cerrada y simple contenida por $A$:

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-int-curv-1.pdf_tex}
\end{center}

Aplicando el Teorema de Green en cada uno de los campos vectoriales de la definición (\ref{defn:integral}) de integral curvilínea, teniendo en cuenta las ecuaciones de Cauchy-Riemann, se puede observar que los integrandos se anulan.
\begin{align*}
    \oint f(z) \, \dif z &= \oint \Vec{G} \cdot \dif \Vec{s} + \iu \oint \Vec{H} \cdot \dif \Vec{s}
    \\
    &=
    \scale{0.93}{
    \iint_C \left( -v'_x - u'_y \right) \partial x \, \partial y + \iu \iint_C \left( u'_x-v'_y \right) \partial x \, \partial y
    }
    \\
    &= 0
\end{align*}

En conclusión:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si una función compleja es derivable en $A$ entonces la integral sobre cualquier curva $C$ cerrada y simple contenida por $A$ es nula:
    \begin{equation*}
        \oint f(z) \, \dif z = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si una función verifica el teorema de Cauchy-Goursat entonces existe su primitiva, ya que al tener integral cerrada nula cumple el teorema de la primitiva.
\end{mdframed}


\section{Fórmula integral de Cauchy}
\label{sec:CauchyFormula}

Sea $A$ un conjunto abierto simplemente conexo, sea $C$ una curva cerrada y simple contenida por $A$, sea $z_0$ un punto interior de $C$:

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-int-curv-2.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:CauchyFormula}
    \end{prop}
    Si $f(z)$ es derivable en todo el interior de $A$, salvo en $z_0$, se tiene que:
    \begin{equation*}
        \oint \frac{f(z)}{z - z_0} \dif z = f(z_0) \, 2 \pi \iu
    \end{equation*}
\end{mdframed}

Nótese que si $z_0$ estuviese afuera de $C$ o si $f(z)$ fuese derivable en $z_0$, la integral sobre $C$ se anularía por el Teorema de Cauchy-Goursat (Sec. \ref{sec:CauchyGoursat}).

La propiedad anterior puede ser generalizada.
\begin{equation*}
    \textrm{Sea} \enspace g(z) = \oint \frac{f(w)}{w - z} \dif w
\end{equation*}

Si $f(w)$ es contínua sobre la curva, se tiene que:
\begin{align*}
    \frac{\dif}{\dif z} g(z) &= \frac{\dif}{\dif z} \oint \frac{f(w)}{w-z}
    \\[1ex]
    &= \oint \left[ \frac{\dif}{\dif z} \frac{f(w)}{w-z} \right] \dif w
    \\[1ex]
    &= \oint \dfrac{f(w)}{(w-z)^2} dw
\end{align*}

Por extrapolación, se tiene:
\begin{equation*}
    \frac{\dif^\nth}{\dif z^\nth} g(z) = \nth! \oint \frac{f(w)}{(w-z)^{\nth+1}} \dif w
\end{equation*}

Por lo tanto $g(z) \in \class[\infty]$ y reemplazando $g(z)$ en la fórmula de Cauchy (Prop. \ref{prop:CauchyFormula}) se tiene:
\begin{equation*}
    g(z) = 2 \pi \, f(z)
\end{equation*}

Finalmente, derivando la ecuación anterior $\nth$ veces, se tiene la generalización del teorema.
Además, esto implica que $f(z) \in \mathcal{C} ^ \infty \big( \operatorname{int}(C) \big)$.
Nótese que si $\nth=0$ queda la Fórmula de Cauchy dada en la propiedad \ref{prop:CauchyFormula}.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \frac{\dif^\nth}{\dif z^\nth} f(z_0) = \frac{\nth!}{2 \pi \iu} \oint \frac{f(z)}{(z-z_0)^{\nth+1}} \dif z
    \end{equation*}
\end{mdframed}


\chapter{Series}


\section{Sucesiones}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Sucesión}
    \cusTe{Una sucesión es una función que toma valores del conjunto de los números naturales y devuelve números complejos.}
    \begin{equation*}
        a_\nth: \setN \longrightarrow \setC \tq a_\nth = h_\nth + \iu \, k_\nth
    \end{equation*}
\end{mdframed}

Planteando el siguiente límite y resolviendo por regla de l'Hôpital-Bernoulli se obtiene una importante propiedad.
\begin{align*}
    \lim_{\nth\to\infty} \sqrt[\nth]{\nth^\kth}
    &= \lim_{\nth\to\infty} \nth^{\kth/\nth}
    \\
    &= \lim_{\nth\to\infty} e^{\left[ \ln(\nth)^{\kth/\nth} \right]}
    \\
    &= \lim_{n\to\infty} e^{\left[ \tfrac{\kth}{\nth} \ln(n) \right]}
    \\
    &= e^{\kth \, \lim_{\nth\to\infty} \left[ \tfrac{\ln(\nth)}{\nth} \right]}
    \\
    &= e^{\kth \, \lim_{\nth\to\infty} \left[ \frac{1}{\nth} \right]}
\end{align*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \lim_{n\to\infty} \sqrt[n]{n^k} = 1 \quad \textrm{con} \enspace \nth, \kth \in \setN
    \end{equation*}
\end{mdframed}

Así como puede ser de utilidad la siguiente propiedad, por ejemplo para identificar alternancia de signo.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \lim_{\nth\to\infty} \kth^\nth =
        \left\{
        \begin{aligned}
            \infty \enspace & \textrm{si} \enspace k>1
            \\
            1 \enspace & \textrm{si} \enspace k=1
            \\
            0 \enspace & \textrm{si} \enspace -1<k<1
            \\
            \pm 1 \enspace & \textrm{si} \enspace k = -1
            \\
            \pm \infty \enspace & \textrm{si} \enspace k<-1
        \end{aligned}
        \right.
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Suma parcial}
    \cusTe{Una suma parcial es una sumatoria de $\Nth$ elementos de una sucesión.}
    \begin{gather*}
        S_\Nth: \setN \longrightarrow \setC \tq
        \\
        S_\Nth = \sum_{\nth=n_0}^\Nth \braces{a_\nth} = a_0 + a_1 + a_2 + \dots + a_\Nth
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Serie}
    \cusTe{Una serie es una sumatoria de los infinitos elementos de una sucesión.}
    \begin{equation*}
        S_\nth = \lim_{\Nth \to \infty} \braces{S_\Nth} = \sum_{\nth=n_0}^ \infty \braces{a_\nth}
    \end{equation*}
\end{mdframed}

Si la serie tiende a un número $w_0 \in \setC$, se dice convergente, verificando:
\begin{equation*}
    \sum_{\nth=n_0}^ \infty \braces{a_\nth} = w_0
\end{equation*}


\section{Convergencia de series}

La fórmula de la sucesión asociada a una serie es determinante para la convergencia de esta, ya que hay criterios que se basan solo en el análisis de la sucesión y no en la sumatoria en sí.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Condición necesaria}
    \begin{equation*}
        S_\nth \enspace \textrm{es convergente} \Rightarrow \lim_{\nth\to\infty} \braces{a_\nth} = 0
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Convergencia absoluta}
    \begin{equation*}
        \sum_{\nth=n_0}^\infty \braces{a_\nth} \enspace \textrm{converge} \iff \sum_{\nth=n_0}^\infty \norm{a_\nth} \enspace \textrm{converge}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:DAlembert}
    \end{prop}
    \cusTi{Criterio de D'Alembert}
    \cusTe{Planteando:}
    \begin{equation*}
        \lim_{\nth\to\infty} \norm{\frac{a_{\nth+1}}{a_\nth}} = L \neq 1
    \end{equation*}
    \cusTe{Se tiene que:}
    \begin{itemize}
        \item Si $L<1$ entonces $S_\nth$ converge
        \item Si $L>1$ entonces $S_\nth$ no converge
    \end{itemize}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:CauchyCriterion}
    \end{prop}
    \cusTi{Criterio de Cauchy}
    \cusTe{Planteando:}
    \begin{equation*}
        \lim_{\nth\to\infty} \norm{\sqrt[n]{a_\nth}} = L \neq 1
    \end{equation*}
    \cusTe{Se tiene que:}
    \begin{itemize}
        \item Si $L<1$ entonces $S_\nth$ converge
        \item Si $L>1$ entonces $S_\nth$ no converge
    \end{itemize}
\end{mdframed}


\section{Serie aritmética}

Es también conocida como la \emph{serie de Gauss} por haber este deducido su fórmula para el caso donde $\Nth=100$ a los once años de edad.
\begin{align*}
    S_\nth &= \sum_{\nth=1}^N \nth
    \\[1ex]
    &= 1 + 2 + 3 + \dots + \Nth
    \\[1em]
    &= \frac{\Nth(\Nth+\nth)}{2}
\end{align*}

Esta serie es divergente ya que si evaluamos $\nth\to\infty$ en la sucesion asociada o en la fórmula, el límite tiende a infinito.


\section{Serie geométrica}

Esta serie mantiene una razón constante, ya que cada término sumado se obtiene multiplicando el anterior por la razón $\kth$.
\begin{align*}
    S_\nth &= a_1 \sum_{\nth=0}^\Nth \kth^\nth
    \\[1ex]
    &= a_1 + a_1 \, \kth + a_1 \, \kth^2 + \dots + a_1 \, \kth^\Nth
    \\[1em]
    &= \frac{1-\kth^{\Nth+1}}{1-\kth}
\end{align*}

La convergencia depende de la razón $\kth$:
\begin{equation*}
    \left\{
    \begin{aligned}
        \lim_{\nth\to\infty} S_\nth &= \frac{a_1}{1-\kth} \iff \norm{\kth} < 1
        \\[1ex]
        \lim_{\nth\to\infty} S_\nth &= \infty \iff \norm{\kth} \geq 1
    \end{aligned}
    \right.
\end{equation*}


\section{Serie p}

\begin{align*}
    S_\nth &= \sum_{\nth=1}^\Nth \frac{1}{\nth^p}
    \\[1ex]
    &= \frac{1}{1^p} + \frac{1}{2^p} + \frac{1}{3^p} + \dots + \frac{1}{\Nth^p}
\end{align*}

La convergencia depende del parámetro $p$:
\begin{equation*}
    \left\{
    \begin{aligned}
        & \textrm{Si} \enspace 0 < p \leq 1 \Rightarrow S_\nth \enspace \textrm{es divergente.}
        \\[1ex]
        & \textrm{Si} \enspace 1 < p \Rightarrow S_\nth \enspace \textrm{es convergente.}
    \end{aligned}
    \right.
\end{equation*}

El caso particular en que $p=1$ se lo conoce como \emph{serie armónica}.


\section{Serie telescópica}

Esta serie es siempre convergente:
\begin{align*}
    S_\nth &= \sum_{\nth=1}^\Nth \left( a_\nth - a_{\nth+1} \right)
    \\[1ex]
    &= (a_1-a_2) + (a_2 - a_3) + \dots + (a_\Nth - a_{\Nth+1})
    \\[1em]
    &= a_1 - a_{\Nth+1}
    \\[1em]
    &= a_1 - \lim_{\nth\to\infty} a_{\nth+1}
\end{align*}


\section{Serie alternada}

Los términos sumados se intercalan entre positivos y negativos.
\begin{align*}
    S_\nth &= \sum_{\nth=1}^\Nth (-1)^\nth \, a_\nth
    \\
    &= -a_1 + a_2 - a_3 + \dots + (-1)^\Nth \, a_\Nth
\end{align*}

Si la sucesión $(a_\nth)$ asociada a una serie alternada es estrictamente decreciente, entonces la serie es convergente.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Criterio de Leibniz}
    \begin{equation*}
        S_\nth \enspace \textrm{es convergente} \iff \frac{\dif a_\nth}{\dif \nth} \leq 0
    \end{equation*}
\end{mdframed}


\section{Series de potencias}

Las Series de Potencias son series definidas en función de una variable independiente.
Es decir, que van a quedar definidas distintas series dependiendo en función del valor que se evalúe.
\begin{gather*}
    S_\nth(z) = \sum_{\nth=n_0}^\Nth a_\nth \, w^{\pm \nth}
    \\[1ex]
    \textrm{Donde} \enspace w = f(z) = z-z_0
\end{gather*}

Para determinar el radio de convergencia se suele usar el Criterio de D'Alembert (Prop. \ref{prop:DAlembert}) o el Criterio de Cauchy (Prop. \ref{prop:CauchyCriterion}).

Según el signo del exponente $\pm \nth$, podemos clasificar las series de potencias en positivas y negativas.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
        \label{defn:posPowerSeries}
    \end{defn}
    \cusTi{Series de potencias positivas}
    \begin{align*}
        S_\nth &= \sum_{\nth=0}^\Nth a_\nth \left( z-z_0 \right)^\nth
        \\[1ex]
        &= a_0 + a_1 \left( z-z_0 \right) + \dots + a_\Nth \left( z-z_0 \right)^\Nth
    \end{align*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Series de potencias negativas}
    \begin{align*}
        S_\nth &= \sum_{\nth=1}^\Nth b_\nth \left( z-z_0 \right)^{-\nth}
        \\[1ex]
        &= \frac{b_1}{(z-z_0)} + \frac{b_2}{(z-z_0)^2} + \dots + \frac{b_\Nth}{(z-z_0)^\Nth}
    \end{align*}
\end{mdframed}


\section{Derivación de series de potencias positivas}

Como las series de potencia están definidas a partir de funciones, podemos derivarlas e integrarlas.
Dada una serie $S_\nth$ podemos definir la serie de sus derivadas $D \, S_\nth$ y la serie de sus primitivas $P \, S_\nth$.
Estas van a tener la misma región de convergencia.
\begin{gather*}
    \textrm{Dada} \enspace S_\nth(z) = \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth
    \\[1ex]
    \frac{\dif}{\dif z} S_\nth(z) = \sum_{\nth=1}^\infty \nth \, a_\nth \left(z-z_0\right)^{\nth-1}
    \\[1ex]
    \int S_\nth(z) \, \dif z = \sum_{\nth=0}^\infty \frac{a_\nth \left(z-z_0\right)^{\nth+1}}{\nth+1}
\end{gather*}

Notar que para que la derivada de una serie exista, hay que verificar que al incrementar $z$ en $h$ la imagen esté dentro del radio de convergencia.


\section{Coeficientes de series de potencias positivas}

Evaluando $z_0$ en una serie de potencias (Def. \ref{defn:posPowerSeries}), se observa que el único término sumado va a ser el que verifique $\nth=0$, ya que si $\nth>0$ los terminos sumados son nulos.
\begin{multline*}
    S_\nth(z_0) = a_0 + a_1 \left(z_0-z_0\right) + a_2 \left(z_0-z_0\right)^2 +
    \\
    + a_3 \left(z_0-z_0\right)^3 + \dots + a_\Nth \left(z_0-z_0\right)^\Nth = a_0
\end{multline*}

Es posible calcular cualquiera de los $\Nth$ coeficientes $a_\nth$ de una serie, al que llamaremos $a_\mth$.
Siguiendo el mismo razonamiento anterior, al derivar $\mth$ veces una serie de potencias, se puede deducir por inducción el coeficiente $a_\mth$ a partir de dichas derivadas sucesivas.
\begin{multline*}
    \frac{\dif}{\dif z} S_\nth(z) = \sum_{\nth=1}^\infty \nth \, a_\nth \left(z-z_0\right)^{\nth-1}
    \\
    \Rightarrow \frac{\dif}{\dif z} S_\nth(z_0) = 1 \cdot a_1
\end{multline*}
\begin{multline*}
    \frac{\dif^2}{\dif z^2} S_\nth(z) = \sum_{\nth=2}^\infty \nth \left(\nth-1\right) a_\nth \left(z-z_0\right)^{\nth-2}
    \\
    \Rightarrow \frac{\dif^2}{\dif z^2} S_\nth(z_0) = 2 \cdot 1 \cdot a_2
\end{multline*}
\begin{multline*}
    \frac{\dif^3}{\dif z^3} S_\nth(z) = \sum_{\nth=3}^\infty \nth \left(\nth-1\right) \left(\nth-2\right) a_\nth \left(z-z_0\right)^{\nth-3}
    \\
    \Rightarrow \frac{\dif^3}{\dif z^3} S_\nth(z_0) = 3 \cdot 2 \cdot 1 \cdot a_3
\end{multline*}
\begin{equation*}
    \vdots
\end{equation*}
\begin{multline*}
    \frac{\dif^\mth}{\dif z^\mth} S_\nth (z) =
    \\
    \scale{0.96}{
    = \sum_{\nth=\mth}^\infty \nth \left(\nth-1\right) \left(\nth-2\right) \dots \big( \nth-\left(\mth-1\right) \big) \, a_\nth \left(z-z_0\right)^{\nth-\mth}
    }
    \\
    \Rightarrow \frac{\dif^\mth}{\dif z^\mth} S_\nth(z_0) = \mth! \, a_\mth
\end{multline*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Coeficiente de serie de potencias positivas}
    \begin{equation*}
        a_\mth = \left( \frac{1}{\mth!} \right) \frac{\dif^\mth}{\dif z^\mth} S_\nth(z_0)
    \end{equation*}
\end{mdframed}


\section{Teorema de Taylor}

Sea $A$ un conjunto abierto tal que:
\begin{equation*}
    A = \braces{z \in \setC \tq \norm{z-z_0}<R}
\end{equation*}

Sea $C$ una curva cerrada y simple contenida por $A$.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-series-1.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Desarrollo por Taylor}
    \cusTe{Si una función es derivable en $A$ entonces se puede desarrollar en series de Taylor:}
    \begin{equation*}
        f(z) = \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth \quad \forall \enspace z \in A
    \end{equation*}
    \cusTe{Donde:}
    \begin{align*}
        a_\nth &= \left( \frac{1}{\nth!} \right) \frac{\dif^\nth}{\dif z^\nth} f(z_0)
        \\[1ex]
        &= \left( \frac{1}{2 \pi \iu} \right) \oint \frac{f(z)}{(z-z_0)^{\nth+1}} \dif z
    \end{align*}
\end{mdframed}


\section{Teorema de Laurent}
\label{sec:Laurent}

Sea $A$ un conjunto abierto tal que:
\begin{equation*}
    A = \braces{z \in \setC \tq 0 \leq r< \norm{z-z_0}<R \leq \infty}
\end{equation*}

Sea $C$ una curva cerrada y simple contenida por $A$.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-series-2.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Desarrollo por Laurent}
    \cusTe{Si una función es derivable en $A$ entonces se puede desarrollar en series de Laurent:}
    \begin{equation*}
        f(z) = \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth + \sum_{\nth=1}^\infty b_\nth \left(z-z_0\right)^{-\nth}
    \end{equation*}
    \cusTe{Donde:}
    \begin{align*}
        a_\nth &= \left( \frac{1}{2\pi \iu} \right) \displaystyle\oint \frac{f(z)}{\left(z-z_0\right)^{\nth+1}} \dif z
        \\[1ex]
        b_\nth &= \left( \frac{1}{2\pi \iu} \right) \displaystyle\oint f(z) \left(z-z_0\right)^\nth \dif z
    \end{align*}
\end{mdframed}


\chapter{Polos}


\section{Singularidades aisladas}

Una singularidad es un punto donde una función o bien no está definida o bien no es derivable.
Si $f(z)$ tiene una singularidad en $z_0$ pero es derivable en un entorno $0<\norm{z-z_0}<R$ entonces se dice que es una singularidad aislada y $f(z)$ admite desarrollo en Series de Laurent (Sec. \ref{sec:Laurent}).
Analizando los coeficientes $b_\nth$ del desarrollo de Laurent, se pueden clasificar las singularidades aisladas de la siguiente manera:

\begin{itemize}
    \item Singularidad evitable:
    \begin{equation*}
        b_\nth = 0 \enspace \forall \enspace n>0
    \end{equation*}
    \item Polo de orden $\kth$:
    \begin{equation*}
        b_\nth = 0 \enspace \forall \enspace n>k
    \end{equation*}
    \item Singularidad esencial:
    \begin{equation*}
        b_\nth \neq 0 \enspace \forall \enspace n>0
    \end{equation*}
\end{itemize}


\section{Clasificación de polos}

A veces es posible clasificar la singularidad aislada sin hacer el desarrollo en serie de la función.
En particular, se puede detectar si una singularidad aislada es un polo de la siguiente forma.

Si $z_0$ es un Polo de orden $\kth$, la función se va a desarrollar de la siguiente manera:
\begin{multline*}
    f(z) = \frac{b_\kth}{\left(z-z_0\right)^\kth} + \frac{b_{\kth-1}}{\left(z-z_0\right)^{\kth-1}} + \frac{b_{\kth-2}}{\left(z-z_0\right)^{\kth-2}} +
    \\
    + \dots + \frac{b_1}{\left(z-z_0\right)} + \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth
\end{multline*}

\begin{multline*}
    f(z) = \frac{1}{\left(z-z_0\right)^\kth} \Big( b_\kth + b_{\kth-1} \left(z-z_0\right) + b_{\kth-2} \left(z-z_0\right)^2 +
    \\
    + \dots + b_1 \left(z-z_0\right)^{\kth-1} + \sum_{n=0}^\infty a_\nth \left(z-z_0\right)^{\nth+\kth} \Big)
\end{multline*}

Si llamamos $\Phi(z)$ a todo lo que está entre paréntesis, se tiene:
\begin{equation*}
    f(z) = \frac{\Phi(z)}{\left(z-z_0\right)^\kth}
\end{equation*}

Donde:
\begin{equation*}
    \Phi (z_0) = b_\kth \neq 0 \quad \land \quad \Phi (z_0) \in \class
\end{equation*}


\section{Ceros de una función analítica}

Se dice que $z_0$ es un cero de una $f(z)$ si verifica:
\begin{equation*}
    f(z_0)=0
\end{equation*}

Si la función es analítica en $z_0$ y su entorno, tiene desarrollo de Taylor.
\begin{equation*}
    f(z) = \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth
\end{equation*}

Cuando $z=z_0$, la función toma el primer término $a_0$ de la serie.
Por lo tanto, dado que $f(z_0) = a_0 = 0$, el desarrollo de Taylor comienza en 1.
\begin{equation*}
    f(z) = \underbrace{a_0}_{f(z_0)=0} + \sum_{\nth=1}^\infty a_\nth (z-z_0)^\nth
\end{equation*}

Pero además, la función puede tener derivadas sucesivas nulas.
Recordar que en un desarrollo de Taylor, los coeficientes $a_\nth$ están dados por las derivadas sucesivas evaluadas en $z_0$.
Si una derivada es nula, entonces también lo será este coeficiente.
\begin{align*}
    \frac{\dif}{\dif z} f(z_0) &= \frac{\dif^2}{\dif z^2} f(z_0)
    \\[1ex]
    &= \frac{\dif^{\kth-1}}{\dif z^{\kth-1}} f(z_0)
    \\
    &= 0
\end{align*}

Siguiendo el razonamiento anterior, si hay $\kth$ derivadas que se anulan entonces el desarrollo comienza en $\kth$, ya que los términos anteriores son nulos.
Tiene que haber al menos un valor de $\kth$ para el que la derivada no se anule, sino la función sería nula.
\begin{equation*}
    f(z) = \sum_{\nth=\kth}^\infty a_\nth \left(z-z_0\right)^\nth
\end{equation*}

Sacando factor común $\left(z-z_0\right)^\kth$ y definiendo la serie de potencias positivas como $\Phi(z)$, se tiene:
\begin{equation*}
    f(z) = \left(z-z_0\right)^\kth \underbrace{\sum_{\nth=\kth}^\infty a_\nth \left(z-z_0\right)^{\nth-\kth}}_{\Phi (z)}
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    Si $z_0$ es un cero de orden $k$, entonces $f$ se puede expresar como:
    \begin{equation*}
        f(z) = \left(z-z_0\right)^\kth \Phi (z)
    \end{equation*}
    \noTi{Donde:}
    \begin{equation*}
        \Phi (z_0) = a_\kth \neq 0
    \end{equation*}
\end{mdframed}


\section{Clasificación de singularidades}

De manera general, analizando los ceros de un cociente se pueden clasificar todas las singularidades, incluyendo los polos.

Si $f(z) = h(z)/g(z)$ con $g(z_0)=0$ entonces $f(z)$ tiene una singularidad aislada en $z_0$.
Y como $g(z)$ es analítica, se puede expresar como $g(z)=\left(z-z_0\right)^\kth \Phi_g (z)$.

\begin{itemize}
    \item \concept{Caso 1}
    
    Si $h(z_0) \neq 0$ entonces $z_0$ es un polo de orden $k$.
    \begin{align*}
        f(z) &= \frac{h(z)}{g(z)}
        \\[1ex]
        &= \underbrace{\frac{h(z)}{\Phi_g (z)}}_{\Phi_f (z)} \frac{1}{\left(z-z_0\right)^\kth}
        \\[1ex]
        &= \frac{\Phi_f (z)}{\left(z-z_0\right)^\kth}
    \end{align*}

    \item \concept{Caso 2}
    
    Si $h(z_0) = 0$ se tiene:
    \begin{align*}
        f(z) &= \frac{\left(z-z_0\right)^\mth}{\left(z-z_0\right)^\kth} \underbrace{\frac{\Phi_h (z)}{\Phi_g (z)}}_{\Phi_f (z)}
        \\[1ex]
        &= \left(z-z_0\right)^{\mth-\kth} \Phi_f (z)
    \end{align*}
    
    \begin{itemize}
        \item \concept{Caso 2.1}
        
        Si $m \geq k$ entonces $z_0$ es una singularidad aislada evitable.
        \begin{equation*}
            f(z) = \left(z-z_0\right)^{\mth-\kth} \Phi(z)
        \end{equation*}

        \item \concept{Caso 2.2}
        
        Si $m<k$ entonces $z_0$ es un polo de orden $k-m$.
        \begin{equation*}
            f(z) = \frac{\Phi (z)}{\left(z-z_0\right)^{\kth-\mth}}
        \end{equation*}
    \end{itemize}
\end{itemize}


\chapter{Residuos}


\section{Teorema de los residuos}
\label{sec:residue}

El residuo $(b_1)$ es el coeficiente del primer término de las series de potencias negativas:
\begin{equation*}
    f(z) = \frac{b_1}{z-z_0} + \sum_{\nth=2}^\infty \frac{b_\nth}{\left(z-z_0\right)^\nth} + \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth
\end{equation*}

Las siguientes, son las primitivas de las series de la ecuación anterior:
\begin{align*}
    \int \sum_{\nth=2}^\infty \frac{b_\nth}{\left(z-z_0\right)^\nth} \dif z
    &= \sum_{\nth=2}^\infty b_\nth \frac{-1}{\nth-1} \frac{1}{\left(z-z_0\right)^{\nth-1}}
    \\
    \int \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth \dif z
    &= \sum_{\nth=0}^\infty \frac{a_\nth}{\nth+1} \left(z-z_0\right)^{\nth+1}
\end{align*}

Calculando la integral de $f(z)$ sobre una curva cerrada $C$ contenida en un conjunto abierto simplemente conexo $A$, se tiene:
\begin{multline*}
    \oint_C f(z) \, \dif z =
    \\
    \scale{0.96}{
    = \oint_C \frac{b_1}{z-z_0} \dif z + \oint \sum_{\nth=2}^\infty \frac{b_\nth}{\left(z-z_0\right)^n} + \oint \sum_{\nth=0}^\infty a_\nth \left(z-z_0\right)^\nth
    }
\end{multline*}

Pero las integrales sobre una curva cerrada de las series que tienen primitiva se anulan por el teorema de la primitiva (Sec. \ref{sec:primitive}), quedando de la ecuación anterior solamente:
\begin{equation*}
    \oint_C f(z) \, \dif z = \oint_C \frac{b_1}{z-z_0} \dif z
\end{equation*}

Esto significa que la integral sobre la curva $C$ está dada por la integral del término $(z-z_0)^{-1}$ y su coeficiente $b_1$.
Por lo cual, es posible tomar cualquier curva $C_i$ que contenga a la singularidad $z_0$.
Por la fórmula de Cauchy (Sec. \ref{sec:CauchyFormula}), la integral anterior es:
\begin{equation*}
    \oint_C f(z) \, \dif z = 2 \pi \, \iu \, b_1
\end{equation*}

El teorema de los residuos permite calcular la integral sobre una curva cerrada que encierre una cantidad $\Nth$ finita de singularidades aisladas:

\begin{center}
    \def\svgwidth{\linewidth}
    \input{./images/calc-polos-2.pdf_tex}
\end{center}

\begin{center}
    \def\svgwidth{0.5\linewidth}
    \input{./images/calc-polos-1.pdf_tex}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \oint_C f(z) \, \dif z = \sum_{\nth=0}^\Nth 2 \pi \, \iu \, b_1(z_\nth)
    \end{equation*}
    \noTi{Donde, el residuo de cada polo está dado por:}
    \begin{equation*}
        b_1(z_0) = \frac{\dif^{\kth-1}}{\dif z^{\kth-1}} \frac{\Phi(z_0)}{(\kth-1)!}
    \end{equation*}
\end{mdframed}


\section{Integrales reales periódicas}

Esta es una aplicación del teorema de los residuos (Sec. \ref{sec:residue}) que permite calcular integrales de funciones periódicas de una variable real:
\begin{equation*}
    \int_{t_0}^{t_0+T} f(t) \, \dif t
\end{equation*}

Dada una circunferencia unitaria $C$ parametrizada:
\begin{equation*}
    z(t) = e^{\iu \omega t} = \cos(\omega t) + \iu \sin(\omega t)
\end{equation*}

Donde:
\begin{gather*}
    t_0 \leq t < t_0+T
    \\
    T = \frac{1}{\omega}
\end{gather*}

Y su diferencial de arco está dado por:
\begin{equation*}
    \dif z = \iu \, \omega \, e^{\iu \omega t} \, \dif t
\end{equation*}

A partir de la propiedad \ref{prop:ReIm} y la parametrización de $C$, se aplica el siguiente cambio de variable:
\begin{gather*}
    \cos(\omega t) = \frac{1}{2} \left( z + \frac{1}{z} \right)
    \\[1ex]
    \sin(\omega t) = \frac{1}{2\iu} \left( z - \frac{1}{z} \right)
\end{gather*}

El método consiste en suponer que $f(t)$ es una función real que surge de evaluar la curva $z(t)$ en una función compleja $f(z)$ de manera que su parte imaginaria es nula:
\begin{equation*}
    \int_{t_0}^{t_0+T} f \left( e^{\iu \omega t} \right) \dif t  = \oint_C f(z) \, \dif z
\end{equation*}


\section{Integrales impropias}

Sea $f(z)$ un cociente de polinomios.
Se tiene un segmento de la recta real y un semicírculo $(C)$ formando entre ambos una curva cerrada $(C_0)$.

Este semicírculo cerrado contiene las $\Nth$ singularidades que $f(z) \, e^{\iu \omega z}$ presente en el semiplano superior.

\begin{center}
    \def\svgwidth{0.6\linewidth}
    \input{./images/calc-polos-3.pdf_tex}
\end{center}

\begin{equation*}
    f(z) = \frac{P(z)}{Q(z)}
\end{equation*}

Donde:
\begin{equation*}
    Q(z_0) \neq 0 \quad \forall \enspace z_0 \in C_o
\end{equation*}

Además, el grado de $P(z)$ y de $Q(z)$ verifican:
\begin{gather*}
    \textrm{Si} \enspace \omega = 0 \Rightarrow \operatorname{grad}(P) > \operatorname{grad}(Q) + 1
    \\[1ex]
    \textrm{Si} \enspace \omega \neq 0 \Rightarrow \operatorname{grad}(P) > \operatorname{grad}(Q)
\end{gather*}

Se define entonces la siguiente integral curvilínea sobre el total de la curva cerrada:
\begin{equation*}
    \oint f(z) \, e^{\iu \omega z} \, \dif z = \int_C f(z) \, e^{\iu \omega z} \, \dif z + \int_{-R}^{R} f(t) \, e^{\iu \omega t} \, \dif t
\end{equation*}

La parte izquierda de la ecuación se resuelve independientemente del radio $(R)$ del semicírculo, por el teorema de los residuos (Sec. \ref{sec:residue}).

Tomando el límite cuando $R\to\infty$ queda definida una integral real impropia.
\begin{equation*}
    \sum_{\nth=0}^\Nth 2 \pi \, \iu \, b_1(z_\nth) = \int_C f(z) \, e^{\iu \omega z} \, \dif z + \int_{-\infty}^{\infty} f(z) \, e^{\iu \omega t} \, \dif t
\end{equation*}

Si $\int_C f(z) \, e^{\iu \omega z} \, \dif z = 0$ cuando $R \to \infty$ entonces la integral impropia queda definida como sigue:
\begin{equation*}
    \int_{-\infty}^{\infty} f(z) \, e^{\iu \omega t} \, \dif t = \sum_{\nth=0}^\Nth 2 \pi \, \iu \, b_1(z_\nth)
\end{equation*}

La siguiente propiedad es de importancia a la hora de computar integrales impropias.
Vemos que:
\begin{equation*}
    \norm{e^{\iu \omega z}} = \norm{e^{\iu \omega \left(x + \iu y\right)}} = \underbrace{\norm{e^{\iu \omega x}}}_{=1} e^{-\omega y}
\end{equation*}

Obteniendo:

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{gather*}
        \norm{e^{\iu \omega z}} = e^{-\omega y}
        \\[1em]
        \left\{
        \begin{aligned}
            \norm{e^{\iu \omega z}} &< 1 \iff \omega \, y > 0
            \\
            \norm{e^{\iu \omega z}} &> 1 \iff \omega \, y < 0
        \end{aligned}
        \right.
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Desigualdad de Jordan}
    \begin{equation*}
        \int_0^\pi e^{-R \sin(\theta)} \, \dif \theta < \frac{\pi}{R}
    \end{equation*}
\end{mdframed}


\chapter{Transformaciones complejas}


\section{Transformada de Laplace}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Transformada de Laplace}
    \begin{equation*}
        \mathcal{L}(f) = F(s) = \int_0^\infty e^{-st} f(t) \, \dif t
    \end{equation*}
\end{mdframed}

\begin{center}
    \renewcommand{\arraystretch}{2.5}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{Tabla de transformadas}
        \\ \hline \hline
        $f(t)$ & $F(s)$
        \\ \hline \hline
        $t^n$ & $\dfrac{n!}{s^{n+1}}$
        \\ \hline
        $e^{at}$ & $\dfrac{1}{s-a} $
        \\ \hline
        $\cos(bt)$ & $\dfrac{s}{s^2+b^2}$
        \\ \hline
        $\sin(bt)$ & $\dfrac{b}{s^2+b^2}$
        \\ \hline
        $\cosh(at)$ & $\dfrac{s}{s^2-a^2}$
        \\ \hline
        $\sinh(at)$ & $\dfrac{a}{s^2-a^2}$
        \\ \hline
    \end{tabular}
\end{center}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Linealidad}
    \begin{equation*}
        \mathcal{L}(af + bg) = a \, \mathcal{L}(f) + b \, \mathcal{L}(g)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Transformada de la derivada}
    \begin{equation*}
        \mathcal{L}(f') = s \, \mathcal{L}(f) - f(0)
    \end{equation*}
\end{mdframed}

Análogamente, de manera iterada, se pueden obtener expresiones para las transformadas de las derivadas sucesivas:
\begin{gather*}
    \mathcal{L}(f'') = s^2 \, \mathcal{L}(f) - s \, f(0) - f'(0)
    \\[1ex]
    \mathcal{L}(f''') = s^3 \, \mathcal{L}(f) - s^2 \, f(0) - s \, f'(0) - f''(0)
    \\
    \vdots
    \\
    \scale{0.95}{
    \mathcal{L}(f^n) = s^\nth \,\mathcal{L}(f) - s^{\nth-1} \, f(0) - s^{\nth-2} \, f'(0) \dots - s^0 \, f^{\nth-1}(0)
    }
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Traslación en $s$}
    \begin{equation*}
        G(s) = F(s-a)
    \end{equation*}
    \noTi{Donde:}
    \begin{equation*}
        g(t) = e^{at} \, f(t)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Traslación en $t$}
    \begin{equation*}
        \mathcal{L}(g)=e^{-sa}\mathcal{L}(f)
    \end{equation*}
    \noTi{Donde:}
    \begin{equation*}
        g(t) = f(t-a)
    \end{equation*}
\end{mdframed}


\section{Transformada de Fourier}

Las funciones que verifican $f(t)=f(t+T)$ para un período $T$ se llaman periódicas y se pueden desarrollar en series de Fourier.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Serie de Fourier}
    \begin{equation*}
        f(t) = \sum_{\kth=-\infty}^{\infty} \hat{f}_\kth \, e^{\iu \omega t}
    \end{equation*}
\end{mdframed}

Donde tanto $f(t)$ como $e^{\iu \omega t}$ son periódicas.
Por lo tanto, se tiene:
\begin{gather*}
    e^{\iu \omega t} = e^{\iu \omega (t+T)} = e^{\iu \omega t} e^{\iu \omega T}
    \\
    1 = e^{\iu \omega T} = \cos(\omega T) + \iu \sin(\omega T)
    \\
    \omega T = 2 \pi k
    \\
    \omega = 2 \pi k f
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        \hat{f}_k = \frac{1}{T} \int_0^T f(t) \, e^{\iu \omega \kth t} \, \dif t
    \end{equation*}
\end{mdframed}

La transformada de Fourier determina el espectro de frecuencias $\hat{f}(\omega)$ de la forma de onda $f(t)$.
Esto es, la relevancia que cada frecuencia $e^{-\iu \omega t}$ tenga.
La operación inversa determina la forma de onda $\hat{f}(t)$ que tiene cierto espectro de frecuencias $f(\omega)$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Transformada de Fourier}
    \begin{equation*}
        \hat{f}(\omega) = \int_{-\infty}^{\infty} f(t) \, e^{-\iu \omega t} \, \dif t
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Transformada de Fourier inversa}
    \begin{equation*}
        \hat{f}(t) = \int_{-\infty}^{\infty} f(\omega) \, e^{-\iu \omega t} \, \dif \omega
    \end{equation*}
\end{mdframed}


\begin{thebibliography}{2}
    \bibitem{1} \textsc{Astelarra, M. \& Dalvarade, M. (2022). Álgebra II. Guía de problemas.}
    
    \bibitem{2} \textsc{Martini, Ricardo. (2018). Notas de Análisis 2.}
\end{thebibliography}


\chapter{Probabilidad y estadística}


\section{Topología}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Propiedad distributiva}
    \begin{gather*}
        A \cup (B \cap C) = (A \cup B) \cap (A \cup C)
        \\
        A \cap (B \cup C) = (A \cap B) \cup (A \cap C)
        \\[1ex]
        (A \cap B) \cap (A \cap C) = A \cap B \cap C
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Propiedad asociativa}
    \begin{gather*}
        A \cup B \cup C = A \cup (B \cup C) = (A \cup B) \cup C
        \\
        A \cap B \cap C = A \cap (B \cap C) = (A \cap B) \cap C
    \end{gather*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Leyes de Morgan}
    \begin{equation*}
        (A \cap B)^c = A^c \cup B^c
        \\
        (A \cup B)^c = A^c \cap B^c
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para $\nth$ conjuntos disjuntos}
    \begin{equation*}
        P \left( \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n P(A_i)
        \iff A_i \cap A_j = \setO
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para dos conjuntos}
    \begin{equation*}
        P(A \cup B) = P(A) + P(B) - P(A \cap B)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Unión de probabilidades para tres conjuntos}
    \begin{gather*}
        P (A \cup B \cup C) =
        \\
        =P(A) + P(B) + P(C) - P(A \cap B) +
        \\
        - P(B \cap C) - P(A \cap C) + P(A \cap B \cap C)
    \end{gather*}
\end{mdframed}


\section{Métodos de conteo}


\subsection{Factorial}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Factorial}
    \cusTe{Multiplicación de todos los números naturales desde 1 hasta $n$.}
    \begin{equation*}
        n! = n(n-1)! = n(n-1)(n-2) \dots 3 \cdot 2 \cdot 1
    \end{equation*}
\end{mdframed}


\subsection{Combinatorio}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Combinatorio}
    \cusTe{Cantidad de formas de elegir $m$ entre un grupo de $n$ elementos siendo $n>m$.}
    \begin{equation*}
        \comb{n}{m} = \frac{n!}{(n-m)! \, m!}
    \end{equation*}
\end{mdframed}


\subsection{Permutaciones}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Permutaciones}
    \cusTe{Cantidad de formas de ordenar una secuancia de $n$ elementos pudiendo cada una de las $k$ tareas repetirse $n_i$ veces.}
    \begin{equation*}
        \#A = \frac{n!}{n_1!n_2! \dots n_k!}
    \end{equation*}
\end{mdframed}


\section{Probabilidad condicional}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \noTi{Probabilidad de que ocurra el evento $A$ con la condición de que $B$ haya ocurrido.}
    \begin{equation*}
        P(A|B) = \frac{P(A \cap B)}{P(B)}
    \end{equation*}
\end{mdframed}


\subsection{Teorema de multiplicación}

Un individuo tiene una probabilidad $P(B)$ de pertenecer a una partición $B$.
Además, hay una probabilidad $P(A|B)$ de que el individuo cumpla cierta característica $A$ además de pertenecer a $B$.
La probabilidad de que un individuo tenga la característica $A$ y sea parte de la partición $B$ simultaneamente está dada por el teorema de multiplicación.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de multiplicación}
    \begin{equation*}
        P(A \cap B) = P(A|B) \, P(B)
    \end{equation*}
\end{mdframed}


\subsection{Teorema de probabilidad total}

Calcula la probabilidad de que un individuo tenga cierta característica $A$ sobre el total de las particiones.
Esto es la suma de las partes de cada partición $B_i$ que cumplen la característica.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de probabilidad total}
    \begin{equation*}
        P(A) = \sum_{i=1}^n P(A|B_i) \, P(B_i)
    \end{equation*}
\end{mdframed}


\subsection{Teorema de Bayes}

La probabilidad de que un individuo pertenezca a cierta partición $(B_i)$ dado que tiene cierta característica $(A)$ es
\begin{equation*}
    P(B_i|A) = \frac{P(A \cap B_i)}{P(A)}
\end{equation*}

Usando la definición de Probabilidad Condicional y el Teorema de Multiplicación, se tiene

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema de Bayes}
    \begin{equation*}
        P(B_i|A) = \frac{P(A|B_i) \, P(B_i)}{P(A)}
    \end{equation*}
\end{mdframed}


\section{Independencia}

Si dos eventos $A$ y $B$ son independientes uno del otro, el hecho de que ocurra uno no dice nada sobre la ocurrencia del otro.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Independencia de dos eventos}
    \begin{equation*}
        P(A|B) = P(A)
    \end{equation*}
\end{mdframed}

De manera que, para no contradecir la definición de probabilidad condicional, se tiene

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \noTi{Los eventos $A$ y $B$ son independientes $\iff$}
    \begin{equation*}
        P(A \cap B) = P(A) \, P(B)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \noTi{Si $A_1, A_2 \dots A_n$ son independientes $\implies$}
    \begin{equation*}
        P(A_1 \cap A_2 \cap \dots \cap A_n) = P(A_1) \, P(A_2) \dots P(A_n)
    \end{equation*}
\end{mdframed}


\section{Variables aleatorias discretas}

Sea la variable aleatoria $X$ una función que toma valores del espacio muestral $\Omega$ y entrega una cantidad numerable de valores.
\begin{equation*}
    X: \Omega \longrightarrow \setR \tq X(\omega) = x
\end{equation*}

Se dice que $X$ es una variable aleatoria discreta (o VAD), ya que no tiene intervalos contínuos en su imagen.
\begin{equation*}
    \ran(X) = \im(X) \subseteq \setN_0
\end{equation*}


\section{Función de probabilidad puntual}

La función de probabilidad puntual calcula qué probabilidad hay de que una VAD devuelva un valor $k$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función de probabilidad puntual}
    \begin{equation*}
        p_X: \setR \longrightarrow \setR \tq p_X(x) = P(X=k)
    \end{equation*}
\end{mdframed}


\section{Función de distribución}

La función de distribución acumulada es una función partida que calcula qué probabilidad hay de que una VAD devuelva un valor menor o igual a $k$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Función de distribución}
    \begin{equation*}
        F_X: \setR \longrightarrow \setR \tq F_X(x) = P(X \leq k)
    \end{equation*}
\end{mdframed}


\section{Esperanza, varianza, covarianza y desvío}

La esperanza, media, o valor esperado es un promedio entre los valores de $\ran(X)$ pesado por su probabilidad.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Esperanza}
    \begin{equation*}
        E(X) = \sum_{i=1}^n x_i \, \fx[p_X]{x_i}
    \end{equation*}
\end{mdframed}

La varianza calcula qué tan cerca está el valor de la imagen a su esperanza.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza}
    \begin{equation*}
        V(X) = \sum_{i=1}^n \bb{x_i - E(X)}^2 \, p_X(x_i)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        V(X) = \sum_{i=1}^n x_i^2 \, p_X(x_i) - \bb{E(X)}^2
    \end{equation*}
\end{mdframed}

La covarianza es un valor que indica el grado de variación conjunta de dos variables aleatorias respecto a sus medias.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Covarianza}
    \begin{equation*}
        \fx[C]{X\,Y} = \fx[E]{\bb{X-\fx[E]{X}} \bb{Y-\fx[E]{Y}}}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Desvío estandar}
    \begin{equation*}
        \sigma = \sqrt{V(X)}
    \end{equation*}
\end{mdframed}


\section{VAD famosas}


\subsection{Variable aleatoria binomial}

\begin{itemize}
\item Se realizan $n$ pruebas para verificar si se da o no cierto evento $A_i$ en cada una de las muestras.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item La probabilidad $(\rho)$ de que cualquier $A_i$ resulte en éxito es constante ya que las $n$ pruebas son con reposición.
\end{itemize}

Sea la variable aleatoria $X$ la cantidad de éxitos $(S)$ entre las $n$ pruebas.
Se denota $X=\bi(n,\rho)$ y su probabilidad puntual es:
\begin{equation*}
p_X(x) = \comb{n}{x} \rho^x \bb{1-\rho}^{n-x}
\end{equation*}

Notar que el combinatorio es la cantidad de $x$ éxitos entre $n$ pruebas.
El factor $\rho^x$ es la proabilidad de cada éxito multiplicada $x$ veces.
Y el factor $\bb{1-\rho}^{n-x}$ es la probabilidad de cada fracaso multiplicada por la cantidad de fracasos.

La esperanza y la varianza de $\bi(n,\rho)$ están dadas por:
\begin{gather*}
    E \bb{\bi(n,\rho)} = n \, \rho
    \\
    V \bb{\bi(n,\rho)} = n \, \rho \bb{1-\rho}
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Variable Aleatoria de Bernoulli}
    \begin{equation*}
        \be(\rho) = \bi(1,\rho)
    \end{equation*}
\end{mdframed}


\subsection{Variable aleatoria geométrica}

\begin{itemize}
\item Se realiza una cantidad incierta de pruebas para verificar si se da o no cierto evento $A_i$ en cada muestra.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item La probabilidad $(\rho)$ de que cualquier $A_i$ resulte en éxito es constante ya que las $n$ pruebas son con reposición.
\end{itemize}

Sea la variable aleatoria $X$ la primer prueba que resultó en éxito.
Se denota $X=\geo(\rho)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \rho \bb{1-\rho}^{x-1}
\end{equation*}

La esperanza y la varianza de $\geo(\rho)$ están dadas por:
\begin{gather*}
    E \bb{\geo(\rho)} = \frac{1}{\rho}
    \\[1ex]
    V \bb{\geo(\rho)} = \frac{1-\rho}{\rho^2}
\end{gather*}


\subsection{Variable aleatoria hipergeométrica}

\begin{itemize}
\item Se realizan pruebas de $n$ muestras para verificar si se da o no cierto evento $A_i$ en cada una de las pruebas de un lote de $N$ muestras en total.

\item Los eventos $A_i$ son independientes y pueden resultar o bien en un éxito $(S)$ o bien en fracaso $(F)$.

\item Las pruebas son sin reposición.
\end{itemize}

Sea la variable aleatoria $X$ la cantidad de éxitos ($S$) entre las $(n)$ muestras.
Se denota $X=\hip(n,N,S)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \comb{S}{x} \comb{N-S}{n-x} \comb{N}{n}^{-1}
\end{equation*}

La esperanza y la varianza de $\hip(n,N,S)$ están dadas por:
\begin{gather*}
    E \bb{\hip(n,N,S)} = \frac{n \, S}{N}
    \\[1ex]
    V \bb{\hip(n,N,S)} = \frac{\frac{n \, S}{N} \bb{1- \frac{S}{n}} (N-n)}{N-1}
\end{gather*}


\subsection{Variable aleatoria Poisson}

Sea la variable aleatoria $X$ la cantidad $x \in [0;n]$ de muestras que verifiquen un evento $A$.
Se denota $X=\po(\lambda)$ y su probabilidad puntual es:
\begin{equation*}
    p_X(x) = \frac{\lambda^x}{e^\lambda x!}
\end{equation*}

La esperanza y la varianza de $\po(\lambda)$ son
\begin{equation*}
    E \bb{\po(\lambda)} = V \bb{\po(\lambda)} = \lambda
\end{equation*}


\section{Variables aleatorias contínuas}

Sea la variable aleatoria $X$ una función que toma valores del espacio muestral $\Omega$ y entrega valores reales.
\begin{equation*}
    X: \Omega \longrightarrow \setR \tq X(\omega) = x
\end{equation*}

Se dice que $X$ es una variable aleatoria contínua, ya que puede tener intervalos contínuos en su imagen.
\begin{equation*}
    \ran(X) = \im(X) \subseteq \setR
\end{equation*}


\section{Función de densidad}

La función de densidad $f_X(x)$ es una aproximación de la altura $h$ de cada rectángulo cuando el ancho $\Delta x \to 0$ en el histograma de una VAC.
\begin{equation*}
    f_X: \setR \longrightarrow \setR \tq 0 < f_X(x) \approx h(x)
\end{equation*}

En el histograma de un experimento, la altura de cada rectángulo depende de la frecuencia relativa $fr$ que tengan los valores $x$ de la VAC y de qué tantos valores $x \in \Delta x$ contemple dicha frecuencia.
Luego
\begin{equation*}
    h(x) = \frac{f_r}{\Delta x}
\end{equation*}

Además, el área de cada rectángulo se calcula multiplicando la altura $h(x)$ por el ancho $\Delta x$:
\begin{equation*}
    A = h(x) \, \Delta x
\end{equation*}

Si el ancho $\Delta x \to 0$, dejando constante la altura $h(x)$, se obtiene un diferencial de área:
\begin{equation*}
    \dif A = h(x) \, \dif x
\end{equation*}

Pero como $A=f_r$, se tiene que $\dif A$ es la mejor aproximación a la probabilidad de que $x \in \Delta x$ por lo tanto $f(x)$ aproxima a $h(x)$:
\begin{equation*}
    \dif F = f(x) \, \dif x
\end{equation*}


\section{Función de distribución}

El área bajo la gráfica de $f_X(x)$ representa la probabilidad de que una variable aleatoria contínua $x$ tome un valor entre $a$ y $b$:
\begin{gather*}
    A_i = h(x_i) \, \Delta x_i = f_r
    \\
    A_n = \sum_{i=1}^n h(x_i) \, \Delta x_i \approx A
    \\
    A = \lim_{n \to \infty} \sum_{i=1}^n f(x_i) \, \Delta x_i
    \\
    A = \int_a^b f(x) \, \dif x
    \\
    A = F(b) - F(a) = P(x \in [a,b])
\end{gather*}

La Función de Distribución Acumulada $F_X(x)$ es una función partida, contínua y no decreciente:
\begin{equation*}
    F_X: \setR \longrightarrow \setR \tq F_X(x) = \int_{-\infty}^k f(x) \, \dif x = P(x \leq k)
\end{equation*}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \noTi{El área total bajo la curva de la gráfica de $f_X(x)$ es  $P(\Omega)=1$, luego}
    \begin{equation*}
        \lim_{k \to \infty} P(x \leq k)=1
    \end{equation*}
\end{mdframed}


\section{Esperanza, varianza y mediana}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Esperanza}
    \begin{equation*}
        E(X) = \int_{-\infty}^{\infty} x \, \fx{x} \, \dif x
    \end{equation*}
\end{mdframed}

Las siguientes son propiedades de la esperanza:
\begin{gather*}
    E(a \, X + b) = a \, E(X) + b
    \\
    E(X+Y) = E(X) + E(Y)
    \\
    E \big( \fx[g]{x} \big) = \int_{-\infty}^{\infty} \fx[g]{x} \, \fx{x} \, \dif x
    \\
    E(X \, Y) = E(X) - E(Y) \quad \text{con $X,Y$ independientes}
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza}
    \begin{equation*}
        V(X) = \int_{-\infty}^{\infty} \bb{x - E(X)}^2 \fx{x} \, \dif x
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        V(X) = \int_{-\infty}^{\infty} x^2 \, f(x) \, \dif x - \bb{E(X)}^2
    \end{equation*}
\end{mdframed}

Las siguientes son propiedades de la varianza:
\begin{gather*}
    V(a \, X+b) = a^2 \, V(X)
    \\
    V(X+Y) = V(X) + V(Y) \quad \text{con $X, Y$ independientes}
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Mediana}
    \begin{equation*}
        M(X) = \braces{x \in \ran(X) \tq F_X(x) = 0.5}
    \end{equation*}
\end{mdframed}


\section{VAC famosas}


\subsection{Distribución uniforme}

Sea $X$ una VAC con distribución Uniforme:
\begin{equation*}
    X = \uni(a,b)
\end{equation*}

Su función de densidad está dada por:
\begin{equation*}
    f_X(x) =
    \left\{
    \begin{aligned}
        & \frac{1}{b-a} \quad \text{si } a \leq x < b
        \\[1ex]
        & 0 \quad \text{si } x \notin [a,b)
    \end{aligned}
    \right.
\end{equation*}

Y al integrar la rama $a \leq x < b$:
\begin{equation*}
    \int_a^k f(x) \, \dif x = \barrow{\dfrac{x}{b-a}}{a}{k}
\end{equation*}

Se tiene que la función de distribución $F_X(x)$ es:
\begin{equation*}
    F_X(x) =
    \left\{
    \begin{aligned}
        & \frac{x-a}{b-a} \quad \text{si } a \leq x < b
        \\
        & 0 \quad \text{si } x \notin [a,b)
    \end{aligned}
    \right.
\end{equation*}


\subsection{Distribución exponencial}

Sea $X$ una VAC con distribución exponencial:
\begin{equation*}
    X = \ex(\lambda)
\end{equation*}

Su función de densidad está dada por:
\begin{equation*}
    f_X(x) =
    \left\{
    \begin{aligned}
        & \frac{\lambda}{e^{\lambda x}} \quad \text{si } 0 \leq x
        \\[1ex]
        & 0 \quad \text{si } x < 0
    \end{aligned}
    \right.
\end{equation*}

Y al integrar la rama $a \leq x < b$:
\begin{equation*}
    \int_0^k f(x) \, \dif x = -\barrow{e^{-\lambda x}}{0}{k}
\end{equation*}

Se tiene que la función de distribución $F_X(x)$ es:
\begin{equation*}
    F_X(x) =
    \left\{
    \begin{aligned}
        & 1-e^{-\lambda x} \quad \text{si } 0 \leq x
        \\
        & 0 \quad \text{si } x < 0
    \end{aligned}
    \right.
\end{equation*}


\subsection{Distribución normal}

Sea $X$ una VAC con distribución normal:
\begin{equation}
    X = \nor(\mu,\sigma^2)
\end{equation}

Su la función de densidad está dada por:
\begin{equation*}
    f_X(x) = \frac{e^{\tfrac{(x-\mu)^2}{2 \, \sigma^2}}}{\sqrt{2 \, \pi} \, \sigma}
\end{equation*}

Con lo cual no es posible obtener una fórmula para la distribución mediante el cálculo de la densidad.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Estandarización}
    \begin{equation*}
        \frac{X-\mu}{\sigma} = \nor(0,1)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \begin{equation*}
        c \, \nor(\mu,\sigma^2) = \nor(c \, \mu, c^2 \, \sigma^2) \quad \text{con } c \in \setR
    \end{equation*}
\end{mdframed}


\section{Teorema central del límite}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
        \label{prop:TCL}
    \end{prop}
    \noTi{La suma de $n$ variables $X_i = \nor(\mu_i,\sigma_i^2)$ independientes está dada por}
    \begin{equation*}
        \sum_i^n X_i = \fx[\nor]{\sub{\mu}{tot},\sub{\sigma^2}{tot}}
    \end{equation*}
    \noTi{Donde}
    \begin{align*}
        \sub{\mu}{tot} &= \sum_i^n \mu_i
        \\[1ex]
        \sub{\sigma^2}{tot} &= \sum_i^n \sigma_i^2
    \end{align*}
\end{mdframed}

Dos o más variables se dicen idénticamente distribuidas si tienen igual distribución.
Esto es
\begin{equation*}
    \braces{X_1;X_2 \dots X_n} \text{ son IID} \iff F_{X_i}(x)=F_{X_j}(x) \quad \forall \, i,j \in [1,n]
\end{equation*}

De manera que se cumple que:
\begin{itemize}
    \item $E(X_i)$ es igual para toda $X_i$
    \item $V(X_i)$ es igual para toda $X_i$
    \item $p_{X_i}(x)$ es igual para toda VAD $X_i$
    \item $f_{X_i}(x)$ es igual para toda VAC $X_i$
\end{itemize}

Una muestra aleatoria es un conjunto de variables aleatorias independientes e idénticamente distribuidas (IID).

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Teorema central del límite}
    \cusTe{Dadas $X_i$ VAC IID (no necesariamente normales) se cumple}
    \begin{equation*}
        \sum_i^n X_i \approx \nor(n \, \mu , n \, \sigma^2)
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Corolario del TCL}
    \begin{equation*}
        \ave{X} = \dfrac{\sum_i^n X_i}{n} \approx \fx[\nor]{\mu, \tfrac{\sigma^2}{n}}
    \end{equation*}
    \noTi{Donde}
    \begin{align*}
        \mu &= E(\mu_i) 
        \\
        \sigma^2 &= V(\sigma_i^2)
    \end{align*}
\end{mdframed}


\section{Estimación puntual}

Sean $X_1,X_2 \dots X_n$ muestras aleatorias con esperanza $\mu$ y varianza $\sigma^2$.
Para $n>30$, es posible estimar los \emph{momentos} $\hat{\mu}$ y $\hat{\sigma}^2$ que aproximan a la esperanza y la varianza, respectivamente.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Promedio muestral}
    \begin{equation*}
        \ave{X} = \frac{\sum_{i=1}^n X_i}{n} = \hat{\mu} \approx E(X_i) = \mu
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Varianza muestral}
    \begin{equation*}
        s^2 = \frac{\sum_{i=1}^n \bb{X_i - \ave{X}}^2}{n-1} = \hat{\sigma}^2 \approx V(X_i) = \sigma^2 
    \end{equation*}
\end{mdframed}

Cualquier parámetro $\theta$ de una muestra aleatoria ya sea este $\mu, \sigma, \lambda, \rho$ se puede estimar como aquel que verifique la definición de esperanza:
\begin{equation*}
    E_{\hat{\theta}} (X_i) = \ave{X} \implies \theta \approx \hat{\theta}
\end{equation*}

Al estimar un parámetro, se comete un error, ya que $n$ es finito.
El error cuadrático medio ($\ecm$) está definido como:
\begin{equation*}
    \ecm = E \big( (\hat{\theta} - \theta)^2 \big)
    = \big( E(\hat{\theta})-\theta \big)^2 + V(\hat{\theta})
\end{equation*}

Donde $E(\hat{\theta})-\theta$ se denomina \emph{sesgo}.
Se dice que un estimador es insesgado si el sesgo es nulo.


\section{Intervalos de confianza}

Además de estimar un parámetro $\theta$ de manera puntual, se puede determinar un \emph{intervalo de confianza} $I=[a;b]$ de manera que los valores entre $a$ y $b$ sean los que probablemente tome el parámetro real.
Esto es
\begin{equation*}
    P(a<\theta<b) = 1 - \alpha
\end{equation*}

Donde $\alpha \in [0,001 ; 0,05]$ es el nivel de significación.

Como el intervalo $I$ está centrado en $\hat{\theta}$, el error máximo cometido es
\begin{equation*}
    \varepsilon = \frac{b-a}{2}
\end{equation*}


\subsection{Intervalo para la esperanza}

Según la propiedad \ref{prop:TCL}, se tiene para $X_1, X_2 \dots X_n$ VAC con distribución normal:
\begin{equation*}
    \ave{X} = \fx[\nor]{\mu, \tfrac{\sigma^2}{n}}
\end{equation*}

Obteniendo, al estandarizar:
\begin{equation*}
    \frac{\ave{X}-\mu}{\sqrt{\dfrac{\sigma^2}{n}}} = \fx[\nor]{0,1} = Z
\end{equation*}

Por otro lado, se define el percentil $Z_\alpha$ tal que
\begin{equation*}
    P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha
\end{equation*}

Que es $Z_\alpha=1.96$ si el nivel de confianza fuese $95\%$ o, lo que es lo mismo, el nivel de significación fuese $\alpha=0.5$ pues
\begin{equation*}
    P(-1.96<Z<1.96) = 0.95
\end{equation*}

Reemplazando $Z$ en la probabilidad definida, queda
\begin{gather*}
    P \bb{ -Z_\alpha < \frac{\ave{X}-\mu}{\sqrt{\dfrac{\sigma^2}{n}}} < Z_\alpha } = 1 - \alpha
    \\[1ex]
    P \bb{ - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < \ave{X} - \mu < Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
    \\[1ex]
    P \bb{ - \ave{X} - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < -\mu < - \ave{X} + Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
    \\[1ex]
    P \bb{ \ave{X} - Z_\alpha \sqrt{\frac{\sigma^2}{n}} < \mu < \ave{X} + Z_\alpha \sqrt{\frac{\sigma^2}{n}} } = 1 - \alpha
\end{gather*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo para la esperanza dado $\sigma^2$}
    \begin{equation*}
        \mu = \ave{X} \pm Z_\alpha \sqrt{\frac{\sigma^2}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}

En caso de no conocer la varianza real se realiza el mismo análisis pero para $\sigma^2 \approx s^2$ considerando que la distribución no va a ser normal estandar sino T-student de grado $n-1$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo para la esperanza dado $s^2$}
    \begin{equation*}
        \mu = \ave{X} \pm T_\alpha \sqrt{\frac{s^2}{n}}
    \end{equation*}
    \noTi{Donde $T_\alpha$ es el percentil cuya distribución T-student verifica $P(T_{n-1}>T_\alpha) = \dfrac{\alpha}{2}$.}
\end{mdframed}

En caso de no conocer la distribución de las VAC $X_1, X_2 \dots X_n$ se realiza una aproximación.
Sea $\ave{X}$ el promedio de una muestra aleatoria de $n>30$ variables, con cualquier distribución.
Por TCL $s^2 \to \sigma^2$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo de nivel asintótico para la esperanza}
    \begin{equation*}
        \mu = \ave{X} \pm Z_\alpha \sqrt{\frac{s^2}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}


\subsection{Intervalo para la proporción}

Sea $\ave{X}$ el promedio de una muestra aleatoria de $n>30$ variables de Bernoulli.
Por TCL se trata de una suma de variables con esperanza $\rho \approx \ave{X}$ y varianza $\rho \bb{1-\rho}$.
\begin{align*}
    \ave{X} & \approx \fx[\nor]{\rho , \frac{\rho \bb{1-\rho}}{n}}
    \\
    \frac{\ave{X}-\rho}{\sqrt{\frac{\ave{X} \bb{1-\ave{X}}}{n}}} & \approx \nor(0,1)
\end{align*}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Intervalo de nivel asintótico para la proporción}
    \begin{equation*}
        \rho = \ave{X} \pm Z_\alpha \sqrt{\frac{\ave{X} \bb{1-\ave{X}}}{n}}
    \end{equation*}
    \noTi{Donde $Z_\alpha$ es el percentil cuya distribución normal estandar verifica $P(-Z_\alpha<Z<Z_\alpha) = 1 - \alpha$.}
\end{mdframed}


\subsection{Intervalo para la varianza}

Dada una muestra aleatoria con distribución normal, se tiene:
\begin{equation*}
    P \bb{\frac{(n-1)s^2}{\chi_{n-1,\tfrac{\alpha}{2}}^2} < \sigma^2 < \frac{(n-1)s^2}{\chi_{n-1,1-\tfrac{\alpha}{2}}^2} } = 1 - \alpha
\end{equation*}


\section{Test de hipótesis}

Un test de hipótesis sirve para determinar si una modificación en un experimento es significativa o no.

Sea $\mu$ la esperanza luego de haber aplicado la modificación.
Se definen dos posibles hipótesis.
La hipótesis $h_0$ o ``conservadora'' sugiere que no hay evidencia de que la modificación haya sido eficiente.
La hipótesis $h_1$ o ``progresista'' sugiere que la modificación implicaría un cambio efectivo.
\begin{equation*}
    \left\{
    \begin{aligned}
        h_0: \mu = \mu_0
        \\
        h_1: \mu \neq \mu_0
    \end{aligned}
    \right.
\end{equation*}

Podemos definir los posibles errores cometidos al determinar qué hipótesis es correcta.
El primer error es suponer que la modificación es significativa cuando no lo es.
El segundo es descartarla cuando es significativa.
El primero es más grave ya que implica una inversión en un cambio que no representa una mejora.
\begin{equation*}
    \left\{
    \begin{aligned}
        \text{Error de tipo 1: Rechazar $h_0$ siendo verdadera.}
        \\
        \text{Error de tipo 2: No rechazar $h_0$ siendo falsa.}
    \end{aligned}
    \right.
\end{equation*}

El test de hipótesis consiste en determinar el riesgo que se quiere correr al optar por la modificación.
El riesgo está dado por $\alpha$ o $\beta$ según determine el experimento el tipo de error por el cual guiarse.
Se rechaza o no $h_0$ si y solo si
\begin{equation*}
    \left\{
    \begin{aligned}
        P \bb{\text{Error de tipo 1}} = \alpha
        \\
        P \bb{\text{Error de tipo 2}} = \beta
    \end{aligned}
    \right.
\end{equation*}


\subsection{Test para la esperanza}

Según la propiedad \ref{prop:TCL}, se tiene para $X_1, X_2 \dots X_n$ VAC con distribución normal:
\begin{equation*}
    \ave{X} = \fx[\nor]{\mu_0, \tfrac{\sigma^2}{n}}
\end{equation*}

Obteniendo, estandarizar:
\begin{gather*}
    \frac{\ave{X}-\mu_0}{\sqrt{\dfrac{\sigma^2}{n}}} = \fx[\nor]{0,1} = Z
    \\[1ex]
    \frac{\norm{\ave{X}-\mu_0}}{\sqrt{\dfrac{\sigma^2}{n}}} = \norm{Z}
\end{gather*}

Por otro lado, se define el percentil $Z_\alpha$ tal que:
\begin{equation*}
    P(\norm{Z}>\norm{Z_\alpha}) = \alpha
\end{equation*}

Esta ecuación indica que la probabilidad de que la ``distancia'' entre el promedio muestral ($\ave{X}$) y la media real ($\mu_0$) sea mayor que el percentil $Z_\alpha$ es tan baja como $\alpha$ lo sea.
Por lo tanto, se optará por rechazar $h_0$ si se verifica la siguiente inecuación.
\begin{equation*}
    \norm{\ave{X}-\mu_0} > \norm{Z_\alpha} \sqrt{\dfrac{\sigma^2}{n}}
\end{equation*}


\subsection{P-valor}


\section{Regresión lineal}



\appendix


\chapter{Aplicaciones en ingeniería}


\section{Decibeles}

Las escalas logarítmicas son aquellas que tienen un factor de escala no lineal.
En un gráfico con escala logarítmica, los intervalos del eje aumentan de manera no simétrica, como se muestra en la siguiente imagen:

\begin{center}
    \def\svgwidth{\linewidth}
    \input{./images/misc-escala-log.pdf_tex}
\end{center}

En la imagen anterior, el eje $m$ está representado en escala logarítmica con base 10, mientras que el eje $n$ en escala lineal.
Una escala logarítmica permite representar fenómenos que requieran más resolución a medida que la variable disminuye.
Por ejemplo, el intervalo $[1;10]$ representa la mitad del intervalo total, mientras que el intervalo $[10;19]$ representa menos de la cuarta parte, y aún así ambos tienen el mismo módulo.

A partir de la definición del logarítmo, es posible asignar a las magnitudes $m$ un valor del eje $n$ y viceversa:
\begin{equation*}
    \log{m}=n
\end{equation*}

Se suelen estudiar relaciones entre una magnitud $(x)$ y un valor de referencia $(x_0)$ constante:
\begin{equation*}
    m=\frac{x}{x_0}
\end{equation*}

Cuando se expresa una relacion $m$ con escala logarítmica en base 10, se dice que su respectivo valor $n$ está en bels.
El bel no es una unidad del sistema internacional, sino un indicador de la escala usada.
Una magnitud dada en bels indica qué tantos múltiplos de 10 mayor que la referencia es.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Bel}
    \cusTe{Una relación $n$ dada en bels es aquella que toma una relación $m$ y le aplica una escala logarítmica.}
    \begin{equation*}
        f_{\si{\bel}}(x)=\log \Big( \frac{x}{x_0} \Big) \, \si{\bel}
    \end{equation*}
\end{mdframed}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Decibel}
    \cusTe{Una relación dada en decibeles es la décima parte de una dada en bels.}
    \begin{equation*}
        f_{\si{\deci\bel}}(x) = 10 \log \Big( \frac{x}{x_0} \Big) \si{\deci\bel}
    \end{equation*}
\end{mdframed}

De esta forma, podemos expresar diferentes magnitudes ($x$) en bels ($\si{\bel}$) o decibeles ($\si{\deci\bel}$) para expresar la relación ($m$) entre la magnitud y el valor de referencia ($x_0$) dado:

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline 
        \multicolumn{4}{|c|}{Escalas logarítmicas} \\ \hline \hline
        $x$ & $m$ & $f_{\, \si{\bel}}(x)$ & $f_{\si{\deci\bel}}(x)$ \\
        \hline \hline
        $x_0$ & 1 & $0 \, \mathrm{bel}$ & $0 \, \si{\deci\bel}$ \\ \hline
        $2 \, x_0$ & 2 & $0.3 \, \mathrm{bel}$ & $3 \, \si{\deci\bel}$ \\ \hline
        $10 \, x_0$ & 10 & $1 \, \mathrm{bel}$ & $10 \, \si{\deci\bel}$ \\ \hline
        $100 \, x_0$ & 100 & $2 \, \mathrm{bel}$ & $20 \, \si{\deci\bel}$ \\ \hline
        $1000 \, x_0$ & 1000 & $3 \, \mathrm{bel}$ & $30 \, \si{\deci\bel}$ \\ \hline
        $10^n \, x_0$ & $10^n$ & $n \, \mathrm{bel}$ & $10 n \, \si{\deci\bel}$ \\ \hline
    \end{tabular}
\end{center}

Al igual que $100\,\si{\centi\metre} = 1\,\si{\metre}$, por más que el decibel no sea una unidad, el factor de conversión $\si{\deci\bel} = 10^{-1} \,\si{\bel}$ se puede verificar analizando cualquier fila de la tabla anterior.


\section{Promedios y valor eficaz}

La media, valor medio, o simplemente promedio, es un la cantidad de veces que se repite el elemento de un conjunto que más se repite.
Se define como un caso particular de la media generalizada tomando $k=1$.
Así mismo se definen el valor cuadrático medio (abreviado como rms por las siglas en inglés root mean square) y valor cúbico medio tomando $k=2$ y $k=3$ respectivamente.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Media generalizada}
    \begin{equation*}
        \sub{x}{med} = \sqrt[\uproot{20} \scriptstyle k]{ \frac{1}{N} \sum_{n=0}^{N} x_\ith^k }
    \end{equation*}
\end{mdframed}

Los valores $x_\ith$ podrían tratarse, por ejemplo, de la temperatura hora a hora registrada a lo largo de un día.
En tal caso, habría 24 valores de temperatura asociados al tiempo en que se dieron.
Pero el cálculo del promedio no distinguiría diferencia entre dos horas consecutivas en las que hubo la misma temperatura porque el tiempo sería independiente.
Los valores $x_\ith$ no necesariamente tienen relación entre si, y si la tienen, son variables discretas.

Si en vez de hacer mediciones regularmente cada una hora como en el caso anterior, se registrase la temperatura durante intervalos de tiempo en que esta no varía, no se podría definir la cantidad total de $N$ mediciones porque habría infinitos valores (continuos pero diferentes entre sí) de temperatura registrados durante intervalos continuos.
Para funciones $f(x)$ que tomen un valor $f_\ith$ constante durante intervalos $\Delta x_\ith$ se define el valor medio ponderado.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Media ponderada}
    \cusTe{Sea $f(x)$ una función continua por tramos y constante por tramos.}
    \begin{equation*}
        \sub{f}{med} = \sqrt[\uproot{36} \scriptstyle k]{ \frac{\sum\limits_{n=0}^{N} f_\ith^k(x) \, \Delta x_\ith}{\sum\limits_{n=0}^{N} \Delta x_\ith }}
    \end{equation*}
\end{mdframed}

Si quisiéramos tomar mediciones instantáneas de la temperatura en función del tiempo, no podríamos definir intervalos $\Delta x$ finitos.
Estaríamos considerando que los intervalos son infinitesimales.
Para funciones continuas, pero que no necesariamente sean constantes por tramos, se toma el límite $\Delta x \to 0$ en el valor medio ponderado.
Pero en tal caso, quedaría una integral impropia con $N\to\infty$ que no sería computable.
Ahora bien, para funciones periódicas podemos evaluar $N \to x_0+T$ sabiendo que el valor medio es el mismo para cualquier intervalo $[x_0;x_0+T]$.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Media ponderada}
    \cusTe{Sea $f(x)$ una función periódica.}
    \begin{equation*}
        \sub{f}{med} = \sqrt[\uproot{15} \scriptstyle k]{\frac{1}{T} \int_{x_0}^{x_0+T} f^k(x) \, \dif x}
    \end{equation*}
\end{mdframed}

De esta forma cabe destacar el caso particular para el valor cuadrático medio con $k=2$ para funciones periódicas, llamado valor eficaz.

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Valor eficaz}
    \begin{equation*}
        \rms{f}=\sqrt{\dfrac{1}{T} \int_{x_0}^{x_0+T} f^2(x)\,\dif x}
    \end{equation*}
\end{mdframed}

De manera que, por ejemplo, si la función está dada por una expresión senoidal $f(x)=\peak{A} \sin(\omega x + \varphi)$ acotada por $-\peak{A}<f(x)<\peak{A}$, su valor eficaz es:
\begin{equation*}
    \rms{f}=\frac{\peak{A}}{\sqrt{2}}
\end{equation*}

\begin{center}
    \def\svgwidth{0.8\linewidth}
    \input{./images/misc-rms.pdf_tex}
\end{center}


% FALTA TERMINAR \section{Crecimiento y decaimiento exponencial}


\section{Coordenadas esféricas}

Para ondas tridimensionales que se propagan en frentes de onda esféricos, como lo son el caso de las ondas en campo libre, se define la segunda derivada con respecto del espacio según el laplaciano de presión.

\begin{mdframed}[style=MyFrame1]
    \begin{prop}
    \end{prop}
    \cusTi{Laplaciano de presión}
    \begin{equation*}
        \grad^2 p(r) = \frac{1}{r} \frac{\partial^2 (rp)}{\partial r^2}
    \end{equation*}
\end{mdframed}


\section{Fasores}

\begin{mdframed}[style=MyFrame1]
    \begin{defn}
    \end{defn}
    \cusTi{Fasor}
    \cusTe{Un fasor es la representación de una oscilación $x(t)$ a partir de un número complejo $z(\theta)=\norm{z} e^{\iu\theta}$ cuyo ángulo $\theta(t)=\omega t + \varphi$ varía uniformemente de manera que:}
    \begin{equation*}
        x(t) = \Re (z)
    \end{equation*}
\end{mdframed}

\subsection{Suma de fasores}
Se tienen $\nth$ valores $x_\ith$ de una magnitud que puede ser expresada por la parte real de un número complejo.
\begin{gather*}
    x_1 = \Re(z_1)
    \\
    x_2 = \Re(z_2)
    \\
    \vdots
    \\
    x_\ith = \Re(z_\ith)
\end{gather*}

Se pretende calcular la suma $x_T$ de dichos valores:
\begin{align*}
    x_T &= x_1 + x_2 + \ldots + x_\ith
    \\
    x_T &=  \sum_{\ith=1}^\nth x_\ith
\end{align*}

Cada número complejo $z_\ith$ representa una magnitud $x_\ith$ y expresándolo en su forma polar $z_\ith=\norm{z_\ith}\,e^{\theta_\ith}$ donde $\theta_\ith=\omega_\ith t + \varphi_\ith$ se tiene:
\begin{gather*}
    z_1 = \norm{z_1} \, e^{\iu (\omega_1 t + \varphi_1)} = \norm{z_1} \, e^{\iu \omega_1 t} \, e^{\iu \varphi_1}
    \\
    z_2 = \norm{z_2} \, e^{\iu (\omega_2 t + \varphi_2)} = \norm{z_2} \, e^{\iu \omega_2 t} \, e^{\iu \varphi_2}
    \\
    \vdots
    \\
    z_\ith = \norm{z_\ith} \, e^{\iu (\omega_\ith t + \varphi_\ith)} = \norm{z_\ith} \, e^{\iu \omega_\ith t} \, e^{\iu \varphi_\ith}
\end{gather*}

Se define $w_\ith=\norm{z_\ith}\,e^{\varphi_\ith}$ simplemente para simplificar notación.
No confundir $w$ con $\omega$.
Luego, cada $z_\ith$ queda expresado por:
\begin{gather*}
    z_1 = w_1 \, e^{\iu \omega_1 t}
    \\
    z_2 = w_2 \, e^{\iu \omega_2 t}
    \\
    \vdots
    \\
    z_\ith = w_\ith \, e^{\iu \omega_\ith t}
\end{gather*}

La suma $z_T$ va a estar dada por:
\begin{align*}
    z_T &= w_1 \, e^{\iu \omega_1 t} + w_2 \, e^{\iu \omega_2 t} + \ldots + w_\ith \, e^{\iu \omega_\ith t}
    \\
    z_T &= \sum_{\ith=1}^\nth w_\ith \, e^{\iu \omega_\ith t}
\end{align*}

Si todos los valores de dicha magnitud tienen la misma frecuencia angular $\omega_1=\omega_2=\ldots=\omega_\ith=\omega_0$ la suma queda dada por:

\begin{align*}
    z_T &= e^{\iu \omega_0 t} \left( w_1 + w_2 + \ldots + w_\ith \right)
    \\
    &=  e^{\iu \omega_0 t} \sum_{\ith=1}^\nth w_\ith
    \\
    &= e^{\iu \omega_0 t} \sum_{\ith=1}^\nth \norm{z_\ith} e^{\varphi_\ith}
    \\
    &= e^{\iu \omega_0 t} \sum_{\ith=1}^\nth \norm{z_\ith} \big[ \cos(\varphi_\ith)+\iu\sin(\varphi_\ith) \big]
    \\
    &= e^{\iu \omega_0 t} \left[ \sum_{\ith=1}^\nth \norm{z_\ith} \cos(\varphi_\ith) + \iu \sum_{\ith=1}^\nth \norm{z_\ith} \sin(\varphi_\ith) \right]
    \\
    &= e^{\iu \omega_0 t} \, w_T
\end{align*}

Donde $w_T=\sum_{\ith=1}^\nth w_\ith$ fue definido nuevamente para simplificar notación.

El módulo de $w_T$ es:
\begin{equation*}
    \norm{w_T} = \sqrt{\left[ \sum_{\ith=1}^\nth \norm{z_\ith} \cos(\varphi_\ith) \right]^2 + \left[ \sum_{\ith=1}^\nth \norm{z_\ith} \sin(\varphi_\ith) \right]^2}
\end{equation*}

La fase $\varphi_T$ de $w_T$ es:
\begin{equation*}
    \varphi_T = \arctan \left( \frac{\sum_{\ith=1}^\nth \norm{z_\ith} \sin(\varphi_\ith)}{\sum_{\ith=1}^\nth \norm{z_\ith} \cos(\varphi_\ith)} \right)
\end{equation*}

Pudiendo expresar, finalmente, la suma $z_T$ como:
\begin{align*}
    z_T  &= e^{\iu \omega_0 t} \, w_T
    \\
    &= e^{\iu \omega_0 t} \, \norm{w_T} e^{\iu \varphi_T}
    \\
    &= \norm{w_T} \, e^{\iu(\omega_0 t + \varphi_T)}
\end{align*}

Luego, la magnitud $x_T$ es por definición:
\begin{equation*}
    x_T(t) = \Re(z_T) = \norm{w_T} \cos(\omega_0 t + \varphi_T)
\end{equation*}

Observar que la frecuencia angular $\omega_0$ de la suma $z_T$ es la misma que la de los sumandos, de modo que la suma tiene el período de los fasores que se sumen.


% FALTA TERMINAR \subsection{Multiplicación de fasores}


\end{document}